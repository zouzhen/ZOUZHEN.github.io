<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



















  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






  

<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '6.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="基本概念  张量     张量类似于NumPy的ndarray，另外还有Tensors也可用于GPU以加速计算。   from __future__ import print_function import torch   构造一个未初始化的5x3矩阵： x = torch.empty(5, 3) print(x)     tensor([[ 3.2401e+18,  0.0000e+00,  1">
<meta name="keywords" content="pytorch">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch入门">
<meta property="og:url" content="http://yoursite.com/2018/08/01/PyTorch入门/index.html">
<meta property="og:site_name" content="ZOUZHEN_BLOG">
<meta property="og:description" content="基本概念  张量     张量类似于NumPy的ndarray，另外还有Tensors也可用于GPU以加速计算。   from __future__ import print_function import torch   构造一个未初始化的5x3矩阵： x = torch.empty(5, 3) print(x)     tensor([[ 3.2401e+18,  0.0000e+00,  1">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2018-08-05T09:10:09.400Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PyTorch入门">
<meta name="twitter:description" content="基本概念  张量     张量类似于NumPy的ndarray，另外还有Tensors也可用于GPU以加速计算。   from __future__ import print_function import torch   构造一个未初始化的5x3矩阵： x = torch.empty(5, 3) print(x)     tensor([[ 3.2401e+18,  0.0000e+00,  1">



  <link rel="alternate" href="/atom.xml" title="ZOUZHEN_BLOG" type="application/atom+xml" />




  <link rel="canonical" href="http://yoursite.com/2018/08/01/PyTorch入门/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>PyTorch入门 | ZOUZHEN_BLOG</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    <a href="https://github.com/zouzhen"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_gray_6d6d6d.png" alt="Fork me on GitHub"></a>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta custom-logo">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">ZOUZHEN_BLOG</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />关于</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />分类</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/01/PyTorch入门/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZOUZHEN">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZOUZHEN_BLOG">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">PyTorch入门
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-08-01 14:59:07" itemprop="dateCreated datePublished" datetime="2018-08-01T14:59:07+08:00">2018-08-01</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-08-05 17:10:09" itemprop="dateModified" datetime="2018-08-05T17:10:09+08:00">2018-08-05</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/框架/" itemprop="url" rel="index"><span itemprop="name">框架</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/01/PyTorch入门/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">Comments：</span> <span class="post-comments-count valine-comment-count" data-xid="/2018/08/01/PyTorch入门/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/08/01/PyTorch入门/" class="leancloud_visitors" data-flag-title="PyTorch入门">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数：</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          


          

          

        </div>
      </header>
    

    
    
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><hr>
<ul>
<li><font color="#00dddd" size="4">张量</font><br>  </li>
</ul>
<hr>
<p>张量类似于NumPy的ndarray，另外还有Tensors也可用于GPU以加速计算。  </p>
<pre><code>from __future__ import print_function
import torch  
</code></pre><p>构造一个未初始化的5x3矩阵：</p>
<pre><code>x = torch.empty(5, 3)
print(x)    
tensor([[ 3.2401e+18,  0.0000e+00,  1.3474e-08],
    [ 4.5586e-41,  1.3476e-08,  4.5586e-41],
    [ 1.3476e-08,  4.5586e-41,  1.3474e-08],
    [ 4.5586e-41,  1.3475e-08,  4.5586e-41],
    [ 1.3476e-08,  4.5586e-41,  1.3476e-08]])  
</code></pre><p>构造一个矩阵填充的零和dtype long：</p>
<pre><code>x = torch.zeros(5, 3, dtype=torch.long)
print(x)  
tensor([[ 0,  0,  0],
        [ 0,  0,  0],
        [ 0,  0,  0],
        [ 0,  0,  0],
        [ 0,  0,  0]])  
</code></pre><p>直接从数据构造张量：</p>
<pre><code>x = torch.tensor([5.5, 3])
print(x)
tensor([ 5.5000,  3.0000])
</code></pre><p>或者根据现有的张量创建张量。除非用户提供新值，否则这些方法将重用输入张量的属性，例如dtype</p>
<pre><code>x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes
print(x)
x = torch.randn_like(x, dtype=torch.float)    # override dtype!
print(x)                                      # result has the same size

tensor([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.]], dtype=torch.float64)
tensor([[ 0.2641,  0.0149,  0.7355],
        [ 0.6106, -1.2480,  1.0592],
        [ 2.6305,  0.5582,  0.3042],
        [-1.4410,  2.4951, -0.0818],
        [ 0.8605,  0.0001, -0.7220]])
</code></pre><p>得到它的大小：  </p>
<pre><code>print(x.size())
torch.Size([5, 3])  
</code></pre><p><strong>注意</strong>  </p>
<p><strong>torch.Size</strong> 实际上是一个元组，因此它支持所有元组操作。  </p>
<hr>
<ul>
<li><font color="#00dddd" size="4">操作</font><br>   </li>
</ul>
<hr>
<p>操作有多种语法。在下面的示例中，我们将查看添加操作。</p>
<p>增加：语法1  </p>
<pre><code>y = torch.rand(5, 3)
print(x + y)

tensor([[ 0.7355,  0.2798,  0.9392],
        [ 1.0300, -0.6085,  1.7991],
        [ 2.8120,  1.2438,  1.2999],
        [-1.0534,  2.8053,  0.0163],
        [ 1.4088,  0.9000, -0.1172]])
</code></pre><p>增加：语法2</p>
<pre><code>print(torch.add(x, y))

tensor([[ 0.7355,  0.2798,  0.9392],
        [ 1.0300, -0.6085,  1.7991],
        [ 2.8120,  1.2438,  1.2999],
        [-1.0534,  2.8053,  0.0163],
        [ 1.4088,  0.9000, -0.1172]])
</code></pre><p>增加：提供输出张量作为参数</p>
<pre><code>result = torch.empty(5, 3)
torch.add(x, y, out=result)
print(result)

tensor([[ 0.7355,  0.2798,  0.9392],
        [ 1.0300, -0.6085,  1.7991],
        [ 2.8120,  1.2438,  1.2999],
        [-1.0534,  2.8053,  0.0163],
        [ 1.4088,  0.9000, -0.1172]])
</code></pre><p>增加：就地</p>
<pre><code># adds x to y
y.add_(x)
print(y)

tensor([[ 0.7355,  0.2798,  0.9392],
        [ 1.0300, -0.6085,  1.7991],
        [ 2.8120,  1.2438,  1.2999],
        [-1.0534,  2.8053,  0.0163],
        [ 1.4088,  0.9000, -0.1172]])
</code></pre><p><strong>注意</strong></p>
<p>任何使原位张量变形的操作都是用<em>。后固定的。例如：x.copy</em>(y)，x.t_()，将改变x。</p>
<p>你可以使用标准的NumPy索引与所有的铃声和​​口哨！</p>
<pre><code>print(x[:, 1])

tensor([ 0.0149, -1.2480,  0.5582,  2.4951,  0.0001])
</code></pre><p>调整大小：如果要调整大小/重塑张量，可以使用torch.view：</p>
<pre><code>x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)  # the size -1 is inferred from other dimensions
print(x.size(), y.size(), z.size())

torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
</code></pre><p>如果你有一个元素张量，用于.item()获取值作为Python数字</p>
<pre><code>x = torch.randn(1)
print(x)
print(x.item())

tensor([ 1.3159])
1.3159412145614624
</code></pre><h3 id="NumPy-Bridge"><a href="#NumPy-Bridge" class="headerlink" title="NumPy Bridge"></a>NumPy Bridge</h3><p>将Torch Tensor转换为NumPy阵列（反之亦然）是一件轻而易举的事。<br>Torch Tensor和NumPy阵列将共享其底层内存位置，更改一个将改变另一个。</p>
<hr>
<ul>
<li><font color="#00dddd" size="4">将Torch Tensor转换为NumPy数组</font><br>    </li>
</ul>
<hr>
<pre><code>a = torch.ones(5)
print(a)

tensor([ 1.,  1.,  1.,  1.,  1.])
b = a.numpy()
print(b)

[1. 1. 1. 1. 1.]
</code></pre><p>了解numpy数组的值如何变化。</p>
<pre><code>a.add_(1)
print(a)
print(b)

tensor([ 2.,  2.,  2.,  2.,  2.])
[2. 2. 2. 2. 2.]
</code></pre><hr>
<ul>
<li><font color="#00dddd" size="4">将NumPy数组转换为Torch Tensor</font><br></li>
</ul>
<hr>
<p>了解更改np阵列如何自动更改Torch Tensor</p>
<pre><code>import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)
np.add(a, 1, out=a)
print(a)
print(b)

[2. 2. 2. 2. 2.]
tensor([ 2.,  2.,  2.,  2.,  2.], dtype=torch.float64)
</code></pre><p>除了CharTensor之外，CPU上的所有Tensors都支持转换为NumPy并返回。</p>
<hr>
<ul>
<li><font color="#00dddd" size="4">CUDA Tensors</font><br></li>
</ul>
<hr>
<p>可以使用该.to方法将张量移动到任何设备上。</p>
<pre><code># let us run this cell only if CUDA is available
# We will use ``torch.device`` objects to move tensors in and out of GPU
if torch.cuda.is_available():
    device = torch.device(&quot;cuda&quot;)          # a CUDA device object
    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU
    x = x.to(device)                       # or just use strings ``.to(&quot;cuda&quot;)``
    z = x + y
    print(z)
    print(z.to(&quot;cpu&quot;, torch.double))       # ``.to`` can also change dtype together!

tensor([ 2.3159], device=&apos;cuda:0&apos;)
tensor([ 2.3159], dtype=torch.float64)
</code></pre><hr>
<ul>
<li><font color="#00dddd" size="6">torch</font><br></li>
</ul>
<hr>
<blockquote>
<p>torch.eye</p>
</blockquote>
<p>torch.eye(n, m=None, out=None)<br>返回一个2维张量，对角线位置全1，其它位置全0</p>
<p>参数:</p>
<ul>
<li>n (int ) – 行数</li>
<li>m (int, optional) – 列数.如果为None,则默认为n</li>
<li>out (Tensor, optinal) - Output tensor</li>
</ul>
<p>返回值: 对角线位置全1，其它位置全0的2维张量</p>
<p>返回值类型: <font color="#00099ff" size="4">Tensor</font><br></p>
<p>例子:</p>
<pre><code>&gt;&gt;&gt; torch.eye(3)
1  0  0
0  1  0
0  0  1
[torch.FloatTensor of size 3x3]
</code></pre><blockquote>
<p>from_numpy</p>
</blockquote>
<p>torch.from_numpy(ndarray) → Tensor<br>Numpy桥，将numpy.ndarray 转换为pytorch的 Tensor。 返回的张量tensor和numpy的ndarray共享同一内存空间。修改一个会导致另外一个也被修改。返回的张量不能改变大小。</p>
<p>例子:</p>
<pre><code>&gt;&gt;&gt; a = numpy.array([1, 2, 3])
&gt;&gt;&gt; t = torch.from_numpy(a)
&gt;&gt;&gt; t
torch.LongTensor([1, 2, 3])
&gt;&gt;&gt; t[0] = -1
&gt;&gt;&gt; a
array([-1,  2,  3])
</code></pre><blockquote>
<p>torch.linspace</p>
</blockquote>
<p>torch.linspace(start, end, steps=100, out=None) → Tensor<br>返回一个1维张量，包含在区间start 和 end 上均匀间隔的steps个点。 输出1维张量的长度为steps。</p>
<p>参数:</p>
<ul>
<li>start (float) – 序列的起始点</li>
<li>end (float) – 序列的最终值</li>
<li>steps (int) – 在start 和 end间生成的样本数  </li>
<li>out (Tensor, optional) – 结果张量  </li>
</ul>
<p>例子:</p>
<pre><code>&gt;&gt;&gt; torch.linspace(3, 10, steps=5)

3.0000
4.7500
6.5000
8.2500
10.0000
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.linspace(-10, 10, steps=5)

-10
-5
0
5
10
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5)

-10
-5
0
5
10
[torch.FloatTensor of size 5]
</code></pre><blockquote>
<p>torch.logspace</p>
</blockquote>
<p>torch.logspace(start, end, steps=100, out=None) → Tensor<br>返回一个1维张量，包含在区间 10start 和 10end上以对数刻度均匀间隔的steps个点。 输出1维张量的长度为steps。</p>
<p>参数:</p>
<ul>
<li>start (float) – 序列的起始点</li>
<li>end (float) – 序列的最终值</li>
<li>steps (int) – 在start 和 end间生成的样本数  </li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子:</p>
<pre><code>&gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5)

1.0000e-10
1.0000e-05
1.0000e+00
1.0000e+05
1.0000e+10
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5)

1.2589
2.1135
3.5481
5.9566
10.0000
[torch.FloatTensor of size 5]
</code></pre><blockquote>
<p>torch.ones</p>
</blockquote>
<p>torch.ones(*sizes, out=None) → Tensor<br>返回一个全为1 的张量，形状由可变参数sizes定义。</p>
<p>参数:</p>
<ul>
<li>sizes (int…) – 整数序列，定义了输出形状  </li>
<li>out (Tensor, optional) – 结果张量  </li>
</ul>
<p>例子:  </p>
<pre><code>&gt;&gt;&gt; torch.ones(2, 3)

1  1  1
1  1  1
[torch.FloatTensor of size 2x3]

&gt;&gt;&gt; torch.ones(5)

1
1
1
1
1
[torch.FloatTensor of size 5]
</code></pre><blockquote>
<p>torch.rand</p>
</blockquote>
<p>torch.rand(*sizes, out=None) → Tensor<br>返回一个张量，包含了从区间[0,1)的均匀分布中抽取的一组随机数，形状由可变参数sizes 定义。</p>
<p>参数:</p>
<ul>
<li>sizes (int…) – 整数序列，定义了输出形状  </li>
<li>out (Tensor, optinal) - 结果张量  </li>
</ul>
<p>例子：  </p>
<pre><code>&gt;&gt;&gt; torch.rand(4)

0.9193
0.3347
0.3232
0.7715
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.rand(2, 3)

0.5010  0.5140  0.0719
0.1435  0.5636  0.0538
[torch.FloatTensor of size 2x3]
</code></pre><blockquote>
<p>torch.randn</p>
</blockquote>
<p>torch.randn(*sizes, out=None) → Tensor<br>返回一个张量，包含了从标准正态分布(均值为0，方差为 1，即高斯白噪声)中抽取一组随机数，形状由可变参数sizes定义。  </p>
<p>参数:</p>
<ul>
<li>sizes (int…) – 整数序列，定义了输出形状</li>
<li>out (Tensor, optinal) - 结果张量  </li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; torch.randn(4)

-0.1145
0.0094
-1.1717
0.9846
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.randn(2, 3)

1.4339  0.3351 -1.0999
1.5458 -0.9643 -0.3558
[torch.FloatTensor of size 2x3]
</code></pre><blockquote>
<p>torch.randperm</p>
</blockquote>
<p>torch.randperm(n, out=None) → LongTensor<br>给定参数n，返回一个从0 到n -1 的随机整数排列。</p>
<p>参数:</p>
<ul>
<li>n (int) – 上边界(不包含)  </li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; torch.randperm(4)

2
1
3
0
[torch.LongTensor of size 4]
</code></pre><blockquote>
<p>torch.arange</p>
</blockquote>
<p>torch.arange(start, end, step=1, out=None) → Tensor<br>返回一个1维张量，长度为 floor((end−start)/step)。包含从start到end，以step为步长的一组序列值(默认步长为1)。</p>
<p>参数:</p>
<ul>
<li>start (float) – 序列的起始点</li>
<li>end (float) – 序列的终止点</li>
<li>step (float) – 相邻点的间隔大小</li>
</ul>
<p>out (Tensor, optional) – 结果张量<br>例子：</p>
<pre><code>&gt;&gt;&gt; torch.arange(1, 4)

1
2
3
[torch.FloatTensor of size 3]

&gt;&gt;&gt; torch.arange(1, 2.5, 0.5)

1.0000
1.5000
2.0000
[torch.FloatTensor of size 3]
</code></pre><blockquote>
<p>torch.range</p>
</blockquote>
<p>torch.range(start, end, step=1, out=None) → Tensor<br>返回一个1维张量，有 floor((end−start)/step)+1 个元素。包含在半开区间[start, end）从start开始，以step为步长的一组值。 step 是两个值之间的间隔，即 xi+1=xi+step</p>
<font color="#ff0000" face="黑体">警告：建议使用函数 torch.arange()<br></font><br>参数:<br><br><em> start (float) – 序列的起始点
</em> end (float) – 序列的最终值<br><em> step (int) – 相邻点的间隔大小<br><br>out (Tensor, optional) – 结果张量<br>例子：<br><br>    &gt;&gt;&gt; torch.range(1, 4)<br><br>    1<br>    2<br>    3<br>    4<br>    [torch.FloatTensor of size 4]<br><br>    &gt;&gt;&gt; torch.range(1, 4, 0.5)<br><br>    1.0000<br>    1.5000<br>    2.0000<br>    2.5000<br>    3.0000<br>    3.5000<br>    4.0000<br>    [torch.FloatTensor of size 7]<br><br>&gt; torch.zeros<br><br>torch.zeros(</em>sizes, out=None) → Tensor<br>返回一个全为标量 0 的张量，形状由可变参数sizes 定义。<br><br>参数:<br><br><em> sizes (int…) – 整数序列，定义了输出形状<br><br>( out (Tensor, optional) – 结果张量<br>例子：<br><br>    &gt;&gt;&gt; torch.zeros(2, 3)<br><br>    0  0  0<br>    0  0  0<br>    [torch.FloatTensor of size 2x3]<br><br>    &gt;&gt;&gt; torch.zeros(5)<br><br>    0<br>    0<br>    0<br>    0<br>    0<br>    [torch.FloatTensor of size 5]<br><br>#### 索引,切片,连接,换位Indexing, Slicing, Joining, Mutating Ops<br>—<br>&gt; torch.cat<br><br>torch.cat(inputs, dimension=0) → Tensor<br>在给定维度上对输入的张量序列seq 进行连接操作。<br><br>torch.cat()可以看做 torch.split() 和 torch.chunk()的反操作。 cat() 函数可以通过下面例子更好的理解。<br><br>参数:

</em> inputs (sequence of Tensors) – 可以是任意相同Tensor 类型的python 序列<br><em> dimension (int, optional) – 沿着此维连接张量序列。<br><br>例子：<br><br>    &gt;&gt;&gt; x = torch.randn(2, 3)<br>    &gt;&gt;&gt; x<br><br>    0.5983 -0.0341  2.4918<br>    1.5981 -0.5265 -0.8735<br>    [torch.FloatTensor of size 2x3]<br><br>    &gt;&gt;&gt; torch.cat((x, x, x), 0)<br><br>    0.5983 -0.0341  2.4918<br>    1.5981 -0.5265 -0.8735<br>    0.5983 -0.0341  2.4918<br>    1.5981 -0.5265 -0.8735<br>    0.5983 -0.0341  2.4918<br>    1.5981 -0.5265 -0.8735<br>    [torch.FloatTensor of size 6x3]<br><br>    &gt;&gt;&gt; torch.cat((x, x, x), 1)<br><br>    0.5983 -0.0341  2.4918  0.5983 -0.0341  2.4918  0.5983 -0.0341  2.4918<br>    1.5981 -0.5265 -0.8735  1.5981 -0.5265 -0.8735  1.5981 -0.5265 -0.8735<br>    [torch.FloatTensor of size 2x9]<br><br>&gt;torch.chunk<br><br>torch.chunk(tensor, chunks, dim=0)<br>在给定维度(轴)上将输入张量进行分块儿。<br><br>参数:

</em> tensor (Tensor) – 待分块的输入张量<br><em> chunks (int) – 分块的个数
</em> dim (int) – 沿着此维度进行分块<br><br>&gt; torch.gather<br><br>torch.gather(input, dim, index, out=None) → Tensor<br>沿给定轴dim，将输入索引张量index指定位置的值进行聚合。<br><br>对一个3维张量，输出可以定义为：<br><br>    out[i][j][k] = tensor[index[i][j][k]][j][k]  # dim=0<br>    out[i][j][k] = tensor[i][index[i][j][k]][k]  # dim=1<br>    out[i][j][k] = tensor[i][j][index[i][j][k]]  # dim=3<br>例子：<br><br>    &gt;&gt;&gt; t = torch.Tensor([[1,2],[3,4]])<br>    &gt;&gt;&gt; torch.gather(t, 1, torch.LongTensor([[0,0],[1,0]]))<br>    1  1<br>    4  3<br>    [torch.FloatTensor of size 2x2]<br>参数:<br><br><em> input (Tensor) – 源张量
</em> dim (int) – 索引的轴<br><em> index (LongTensor) – 聚合元素的下标
</em> out (Tensor, optional) – 目标张量<br><br>&gt; torch.index_select<br><br>torch.index_select(input, dim, index, out=None) → Tensor<br>沿着指定维度对输入进行切片，取index中指定的相应项(index为一个LongTensor)，然后返回到一个新的张量， 返回的张量与原始张量<em>Tensor</em>有相同的维度(在指定轴上)。<br><br><font color="#ff0000" face="黑体">注意： 返回的张量不与原始张量共享内存空间。<br></font>  

<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int) – 索引的轴</li>
<li>index (LongTensor) – 包含索引下标的一维张量</li>
<li>out (Tensor, optional) – 目标张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)
&gt;&gt;&gt; x

1.2045  2.4084  0.4001  1.1372
0.5596  1.5677  0.6219 -0.7954
1.3635 -1.2313 -0.5414 -1.8478
[torch.FloatTensor of size 3x4]

&gt;&gt;&gt; indices = torch.LongTensor([0, 2])
&gt;&gt;&gt; torch.index_select(x, 0, indices)

1.2045  2.4084  0.4001  1.1372
1.3635 -1.2313 -0.5414 -1.8478
[torch.FloatTensor of size 2x4]

&gt;&gt;&gt; torch.index_select(x, 1, indices)

1.2045  0.4001
0.5596  0.6219
1.3635 -0.5414
[torch.FloatTensor of size 3x2]
</code></pre><blockquote>
<p>torch.masked_select</p>
</blockquote>
<p>torch.masked_select(input, mask, out=None) → Tensor<br>根据掩码张量mask中的二元值，取输入张量中的指定项( mask为一个 ByteTensor)，将取值返回到一个新的1D张量，</p>
<p>张量 mask须跟input张量有相同数量的元素数目，但形状或维度不需要相同。 注意： 返回的张量不与原始张量共享内存空间。</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>mask (ByteTensor) – 掩码张量，包含了二元索引值</li>
<li>out (Tensor, optional) – 目标张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)
&gt;&gt;&gt; x

1.2045  2.4084  0.4001  1.1372
0.5596  1.5677  0.6219 -0.7954
1.3635 -1.2313 -0.5414 -1.8478
[torch.FloatTensor of size 3x4]

&gt;&gt;&gt; indices = torch.LongTensor([0, 2])
&gt;&gt;&gt; torch.index_select(x, 0, indices)

1.2045  2.4084  0.4001  1.1372
1.3635 -1.2313 -0.5414 -1.8478
[torch.FloatTensor of size 2x4]

&gt;&gt;&gt; torch.index_select(x, 1, indices)

1.2045  0.4001
0.5596  0.6219
1.3635 -0.5414
[torch.FloatTensor of size 3x2]
</code></pre><blockquote>
<p>torch.nonzero</p>
</blockquote>
<p>torch.nonzero(input, out=None) → LongTensor<br>返回一个包含输入input中非零元素索引的张量。输出张量中的每行包含输入中非零元素的索引。</p>
<p>如果输入input有n维，则输出的索引张量output的形状为 z x n, 这里 z 是输入张量input中所有非零元素的个数。</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 源张量</li>
<li>out (LongTensor, optional) – 包含索引值的结果张量  </li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; torch.nonzero(torch.Tensor([1, 1, 1, 0, 1]))

0
1
2
4
[torch.LongTensor of size 4x1]

&gt;&gt;&gt; torch.nonzero(torch.Tensor([[0.6, 0.0, 0.0, 0.0],
...                             [0.0, 0.4, 0.0, 0.0],
...                             [0.0, 0.0, 1.2, 0.0],
...                             [0.0, 0.0, 0.0,-0.4]]))

0  0
1  1
2  2
3  3
[torch.LongTensor of size 4x2]
</code></pre><blockquote>
<p>torch.split</p>
</blockquote>
<p>torch.split(tensor, split_size, dim=0)<br>将输入张量分割成相等形状的chunks（如果可分）。 如果沿指定维的张量形状大小不能被split_size 整分， 则最后一个分块会小于其它分块。</p>
<p>参数:</p>
<ul>
<li>tensor (Tensor) – 待分割张量</li>
<li>split_size (int) – 单个分块的形状大小</li>
<li>dim (int) – 沿着此维进行分割</li>
</ul>
<blockquote>
<p>torch.squeeze</p>
</blockquote>
<p>torch.squeeze(input, dim=None, out=None)<br>将输入张量形状中的1 去除并返回。 如果输入是形如(A×1×B×1×C×1×D)，那么输出形状就为： (A×B×C×D)<br>当给定dim时，那么挤压操作只在给定维度上。例如，输入形状为: (A×1×B), squeeze(input, 0) 将会保持张量不变，只有用 squeeze(input, 1)，形状会变成 (A×B)。</p>
<font color="#ff0000" face="黑体">注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。<br></font>  

<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int, optional) – 如果给定，则input只会在给定维度挤压</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; x = torch.zeros(2,1,2,1,2)
&gt;&gt;&gt; x.size()
(2L, 1L, 2L, 1L, 2L)
&gt;&gt;&gt; y = torch.squeeze(x)
&gt;&gt;&gt; y.size()
(2L, 2L, 2L)
&gt;&gt;&gt; y = torch.squeeze(x, 0)
&gt;&gt;&gt; y.size()
(2L, 1L, 2L, 1L, 2L)
&gt;&gt;&gt; y = torch.squeeze(x, 1)
&gt;&gt;&gt; y.size()
(2L, 2L, 1L, 2L)
</code></pre><blockquote>
<p>torch.stack[source]</p>
</blockquote>
<p>torch.stack(sequence, dim=0)<br>沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。</p>
<p>参数:</p>
<ul>
<li>sqequence (Sequence) – 待连接的张量序列</li>
<li>dim (int) – 插入的维度。必须介于 0 与 待连接的张量序列数之间。</li>
</ul>
<blockquote>
<p>torch.t</p>
</blockquote>
<p>torch.t(input, out=None) → Tensor<br>输入一个矩阵（2维张量），并转置0, 1维。 可以被视为函数transpose(input, 0, 1)的简写函数。</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)
&gt;&gt;&gt; x

0.4834  0.6907  1.3417
-0.1300  0.5295  0.2321
[torch.FloatTensor of size 2x3]

&gt;&gt;&gt; torch.t(x)

0.4834 -0.1300
0.6907  0.5295
1.3417  0.2321
[torch.FloatTensor of size 3x2]
</code></pre><blockquote>
<p>torch.transpose</p>
</blockquote>
<p>torch.transpose(input, dim0, dim1, out=None) → Tensor<br>返回输入矩阵input的转置。交换维度dim0和dim1。 输出张量与输入张量共享内存，所以改变其中一个会导致另外一个也被修改。</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim0 (int) – 转置的第一维</li>
<li><p>dim1 (int) – 转置的第二维</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)
&gt;&gt;&gt; x

0.5983 -0.0341  2.4918
1.5981 -0.5265 -0.8735
[torch.FloatTensor of size 2x3]

&gt;&gt;&gt; torch.transpose(x, 0, 1)

0.5983  1.5981
-0.0341 -0.5265
2.4918 -0.8735
[torch.FloatTensor of size 3x2]
</code></pre></li>
</ul>
<blockquote>
<p>torch.unbind</p>
</blockquote>
<p>torch.unbind(tensor, dim=0)[source]<br>移除指定维后，返回一个元组，包含了沿着指定维切片后的各个切片</p>
<p>参数:</p>
<ul>
<li>tensor (Tensor) – 输入张量</li>
<li>dim (int) – 删除的维度</li>
</ul>
<blockquote>
<p>torch.unsqueeze</p>
</blockquote>
<p>torch.unsqueeze(input, dim, out=None)<br>返回一个新的张量，对输入的制定位置插入维度 1</p>
<font color="#ff0000" face="黑体">注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。<br></font>  

<p>如果dim为负，则将会被转化dim+input.dim()+1</p>
<p>参数:  </p>
<ul>
<li>tensor (Tensor) – 输入张量</li>
<li>dim (int) – 插入维度的索引</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; x = torch.Tensor([1, 2, 3, 4])
&gt;&gt;&gt; torch.unsqueeze(x, 0)
1  2  3  4
[torch.FloatTensor of size 1x4]
&gt;&gt;&gt; torch.unsqueeze(x, 1)
1
2
3
4
[torch.FloatTensor of size 4x1]
</code></pre><h4 id="随机抽样-Random-sampling"><a href="#随机抽样-Random-sampling" class="headerlink" title="随机抽样 Random sampling"></a>随机抽样 Random sampling</h4><hr>
<blockquote>
<p>torch.manual_seed</p>
</blockquote>
<p>torch.manual_seed(seed)<br>设定生成随机数的种子，并返回一个 torch._C.Generator 对象.</p>
<p>参数: </p>
<ul>
<li>seed (int or long) – 种子.</li>
</ul>
<blockquote>
<p>torch.initial_seed</p>
</blockquote>
<p>torch.initial_seed()<br>返回生成随机数的原始种子值（python long）。</p>
<blockquote>
<p>torch.get_rng_state</p>
</blockquote>
<p>torch.get_rng_state()[source]<br>返回随机生成器状态(ByteTensor)</p>
<blockquote>
<p>torch.set_rng_state</p>
</blockquote>
<p>torch.set_rng_state(new_state)[source]<br>设定随机生成器状态 参数: new_state (torch.ByteTensor) – 期望的状态</p>
<blockquote>
<p>torch.default_generator</p>
</blockquote>
<p>torch.default_generator = &lt;torch._C.Generator object&gt;</p>
<blockquote>
<p>torch.bernoulli</p>
</blockquote>
<p>torch.bernoulli(input, out=None) → Tensor<br>从伯努利分布中抽取二元随机数(0 或者 1)。</p>
<p>输入张量须包含用于抽取上述二元随机值的概率。 因此，输入中的所有值都必须在［0,1］区间，即 0&lt;=inputi&lt;=1<br>输出张量的第i个元素值， 将会以输入张量的第i个概率值等于1。</p>
<p>返回值将会是与输入相同大小的张量，每个值为0或者1 </p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入为伯努利分布的概率值</li>
<li>out (Tensor, optional) – 输出张量(可选)  </li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.Tensor(3, 3).uniform_(0, 1) # generate a uniform random matrix with range [0, 1]
&gt;&gt;&gt; a

0.7544  0.8140  0.9842
0.5282  0.0595  0.6445
0.1925  0.9553  0.9732
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.bernoulli(a)

1  1  1
0  0  1
0  1  1
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing &quot;1&quot; is 1
&gt;&gt;&gt; torch.bernoulli(a)

1  1  1
1  1  1
1  1  1
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing &quot;1&quot; is 0
&gt;&gt;&gt; torch.bernoulli(a)

0  0  0
0  0  0
0  0  0
[torch.FloatTensor of size 3x3]
</code></pre><blockquote>
<p>torch.multinomial</p>
</blockquote>
<p>torch.multinomial(input, num_samples,replacement=False, out=None) → LongTensor<br>返回一个张量，每行包含从input相应行中定义的多项分布中抽取的num_samples个样本。</p>
<font color="#ff0000" face="黑体"> [注意]:输入input每行的值不需要总和为1 (这里我们用来做权重)，但是必须非负且总和不能为0。<br></font>

<p>当抽取样本时，依次从左到右排列(第一个样本对应第一列)。</p>
<p>如果输入input是一个向量，输出out也是一个相同长度num_samples的向量。如果输入input是有 m行的矩阵，输出out是形如m×n的矩阵。</p>
<p>如果参数replacement 为 True, 则样本抽取可以重复。否则，一个样本在每行不能被重复抽取。</p>
<p>参数num_samples必须小于input长度(即，input的列数，如果是input是一个矩阵)。</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 包含概率值的张量</li>
<li>num_samples (int) – 抽取的样本数</li>
<li>replacement (bool, optional) – 布尔值，决定是否能重复抽取</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; weights = torch.Tensor([0, 10, 3, 0]) # create a Tensor of weights
&gt;&gt;&gt; torch.multinomial(weights, 4)

1
2
0
0
[torch.LongTensor of size 4]

&gt;&gt;&gt; torch.multinomial(weights, 4, replacement=True)

1
2
1
2
[torch.LongTensor of size 4]
</code></pre><blockquote>
<p>torch.normal()</p>
</blockquote>
<p>torch.normal(means, std, out=None)<br>返回一个张量，包含从给定参数means,std的离散正态分布中抽取随机数。 均值means是一个张量，包含每个输出元素相关的正态分布的均值。 std是一个张量，包含每个输出元素相关的正态分布的标准差。 均值和标准差的形状不须匹配，但每个张量的元素个数须相同。</p>
<p>参数:</p>
<ul>
<li>means (Tensor) – 均值</li>
<li>std (Tensor) – 标准差</li>
<li>out (Tensor) – 可选的输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; torch.normal(means=torch.arange(1, 11), std=torch.arange(1, 0, -0.1))

1.5104
1.6955
2.4895
4.9185
4.9895
6.9155
7.3683
8.1836
8.7164
9.8916
[torch.FloatTensor of size 10]
torch.normal(mean=0.0, std, out=None)
</code></pre><p>与上面函数类似，所有抽取的样本共享均值。</p>
<p>参数:</p>
<ul>
<li>means (Tensor,optional) – 所有分布均值</li>
<li>std (Tensor) – 每个元素的标准差</li>
<li>out (Tensor) – 可选的输出张量</li>
</ul>
<p>例子:</p>
<pre><code>&gt;&gt;&gt; torch.normal(mean=0.5, std=torch.arange(1, 6))

0.5723
0.0871
-0.3783
-2.5689
10.7893
[torch.FloatTensor of size 5]
</code></pre><p>torch.normal(means, std=1.0, out=None)<br>与上面函数类似，所有抽取的样本共享标准差。</p>
<p>参数:</p>
<ul>
<li>means (Tensor) – 每个元素的均值</li>
<li>std (float, optional) – 所有分布的标准差</li>
<li>out (Tensor) – 可选的输出张量</li>
</ul>
<p>例子:</p>
<pre><code>&gt;&gt;&gt; torch.normal(means=torch.arange(1, 6))

1.1681
2.8884
3.7718
2.5616
4.2500
[torch.FloatTensor of size 5]
</code></pre><h4 id="序列化-Serialization"><a href="#序列化-Serialization" class="headerlink" title="序列化 Serialization"></a>序列化 Serialization</h4><hr>
<blockquote>
<p>torch.saves[source]</p>
</blockquote>
<p>torch.save(obj, f, pickle_module=<module 'pickle'="" from="" '="" home="" jenkins="" miniconda="" lib="" python3.5="" pickle.py'="">, pickle_protocol=2)<br>保存一个对象到一个硬盘文件上 参考: Recommended approach for saving a model </module></p>
<p>参数：</p>
<ul>
<li>obj – 保存对象</li>
<li>f － 类文件对象 (返回文件描述符)或一个保存文件名的字符串</li>
<li>pickle_module – 用于pickling元数据和对象的模块</li>
<li>pickle_protocol – 指定pickle protocal 可以覆盖默认参数</li>
</ul>
<blockquote>
<p>torch.load[source]</p>
</blockquote>
<p>torch.load(f, map_location=None, pickle_module=<module 'pickle'="" from="" '="" home="" jenkins="" miniconda="" lib="" python3.5="" pickle.py'="">)<br>从磁盘文件中读取一个通过torch.save()保存的对象。<br>torch.load() 可通过参数map_location 动态地进行内存重映射，使其能从不动设备中读取文件。一般调用时，需两个参数: storage 和 location tag. 返回不同地址中的storage，或着返回None (此时地址可以通过默认方法进行解析). 如果这个参数是字典的话，意味着其是从文件的地址标记到当前系统的地址标记的映射。 默认情况下， location tags中 “cpu”对应host tensors，‘cuda:device_id’ (e.g. ‘cuda:2’) 对应cuda tensors。 用户可以通过register_package进行扩展，使用自己定义的标记和反序列化方法。</module></p>
<p>参数:</p>
<ul>
<li>f – 类文件对象 (返回文件描述符)或一个保存文件名的字符串</li>
<li>map_location – 一个函数或字典规定如何remap存储位置</li>
<li>pickle_module – 用于unpickling元数据和对象的模块 (必须匹配序列化文件时的pickle_module )</li>
</ul>
<p>例子:</p>
<pre><code>&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;)
# Load all tensors onto the CPU
&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location=lambda storage, loc: storage)
# Map tensors from GPU 1 to GPU 0
&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location={&apos;cuda:1&apos;:&apos;cuda:0&apos;})
</code></pre><h4 id="并行化-Parallelism"><a href="#并行化-Parallelism" class="headerlink" title="并行化 Parallelism"></a>并行化 Parallelism</h4><hr>
<blockquote>
<p>torch.get_num_threads</p>
</blockquote>
<p>torch.get_num_threads() → int<br>获得用于并行化CPU操作的OpenMP线程数</p>
<blockquote>
<p>torch.set_num_threads</p>
</blockquote>
<p>torch.set_num_threads(int)<br>设定用于并行化CPU操作的OpenMP线程数</p>
<h4 id="数学操作Math-operations"><a href="#数学操作Math-operations" class="headerlink" title="数学操作Math operations"></a>数学操作Math operations</h4><hr>
<h5 id="Pointwise-Ops"><a href="#Pointwise-Ops" class="headerlink" title="Pointwise Ops"></a>Pointwise Ops</h5><blockquote>
<p>torch.abs</p>
</blockquote>
<p>torch.abs(input, out=None) → Tensor<br>计算输入张量的每个元素绝对值</p>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; torch.abs(torch.FloatTensor([-1, -2, 3]))
FloatTensor([1, 2, 3])
</code></pre><p>torch.acos(input, out=None) → Tensor<br>torch.acos(input, out=None) → Tensor</p>
<p>返回一个新张量，包含输入张量每个元素的反余弦。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

-0.6366
0.2718
0.4469
1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.acos(a)
2.2608
1.2956
1.1075
    nan
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.add()</p>
</blockquote>
<p>torch.add(input, value, out=None)<br>对输入张量input逐元素加上标量值value，并返回结果到一个新的张量out，即 out=tensor+value。</p>
<p>如果输入input是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>value (Number) – 添加到输入每个元素的数</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

0.4050
-1.2227
1.8688
-0.4185
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.add(a, 20)

20.4050
18.7773
21.8688
19.5815
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<blockquote>
<p>torch.add(input, value=1, other, out=None)</p>
</blockquote>
</blockquote>
<p>other 张量的每个元素乘以一个标量值value，并加到iput 张量上。返回结果到输出张量out。即，out=input+(other∗value)</p>
<p>两个张量 input and other的尺寸不需要匹配，但元素总数必须一样。</p>
<p>注意 :当两个张量形状不匹配时，输入张量的形状会作为输出张量的尺寸。</p>
<p>如果other是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 第一个输入张量</li>
<li>value (Number) – 用于第二个张量的尺寸因子</li>
<li>other (Tensor) – 第二个输入张量</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

-0.9310
2.0330
0.0852
-0.2941
[torch.FloatTensor of size 4]

&gt;&gt;&gt; b = torch.randn(2, 2)
&gt;&gt;&gt; b

1.0663  0.2544
-0.1513  0.0749
[torch.FloatTensor of size 2x2]

&gt;&gt;&gt; torch.add(a, 10, b)
9.7322
4.5770
-1.4279
0.4552
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.addcdiv</p>
</blockquote>
<p>torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None) → Tensor<br>用tensor2对tensor1逐元素相除，然后乘以标量值value 并加到tensor。</p>
<p>张量的形状不需要匹配，但元素数量必须一致。</p>
<p>如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。</p>
<p>参数：</p>
<ul>
<li>tensor (Tensor) – 张量，对 tensor1 ./ tensor 进行相加</li>
<li>value (Number, optional) – 标量，对 tensor1 ./ tensor2 进行相乘</li>
<li>tensor1 (Tensor) – 张量，作为被除数(分子)</li>
<li>tensor2 (Tensor) –张量，作为除数(分母)</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; t = torch.randn(2, 3)
&gt;&gt;&gt; t1 = torch.randn(1, 6)
&gt;&gt;&gt; t2 = torch.randn(6, 1)
&gt;&gt;&gt; torch.addcdiv(t, 0.1, t1, t2)

0.0122 -0.0188 -0.2354
0.7396 -1.5721  1.2878
[torch.FloatTensor of size 2x3]
</code></pre><blockquote>
<p>torch.addcmul</p>
</blockquote>
<p>torch.addcmul(tensor, value=1, tensor1, tensor2, out=None) → Tensor<br>用tensor2对tensor1逐元素相乘，并对结果乘以标量值value然后加到tensor。 张量的形状不需要匹配，但元素数量必须一致。 如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。</p>
<p>参数：</p>
<ul>
<li>tensor (Tensor) – 张量，对tensor1 ./ tensor 进行相加</li>
<li>value (Number, optional) – 标量，对 tensor1 . tensor2 进行相乘</li>
<li>tensor1 (Tensor) – 张量，作为乘子1</li>
<li>tensor2 (Tensor) –张量，作为乘子2</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; t = torch.randn(2, 3)
&gt;&gt;&gt; t1 = torch.randn(1, 6)
&gt;&gt;&gt; t2 = torch.randn(6, 1)
&gt;&gt;&gt; torch.addcmul(t, 0.1, t1, t2)

0.0122 -0.0188 -0.2354
0.7396 -1.5721  1.2878
[torch.FloatTensor of size 2x3]
</code></pre><blockquote>
<p>torch.asin</p>
</blockquote>
<p>torch.asin(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的反正弦函数</p>
<p>参数：</p>
<ul>
<li>tensor (Tensor) – 输入张量</li>
<li>nout (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
0.2718
0.4469
1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.asin(a)
-0.6900
0.2752
0.4633
    nan
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.atan</p>
</blockquote>
<p>torch.atan(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的反正切函数</p>
<p>参数：</p>
<ul>
<li>tensor (Tensor) – 输入张量</li>
</ul>
<p>out (Tensor, optional) – 输出张量</p>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
0.2718
0.4469
1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.atan(a)
-0.5669
0.2653
0.4203
0.9196
[torch.FloatTensor of size 4]
</code></pre><ul>
<li>torch.atan2</li>
</ul>
<p>torch.atan2(input1, input2, out=None) → Tensor<br>返回一个新张量，包含两个输入张量input1和input2的反正切函数</p>
<p>参数：</p>
<ul>
<li>input1 (Tensor) – 第一个输入张量</li>
<li>input2 (Tensor) – 第二个输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
0.2718
0.4469
1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.atan2(a, torch.randn(4))
-2.4167
2.9755
0.9363
1.6613
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.ceil</p>
</blockquote>
<p>torch.ceil(input, out=None) → Tensor<br>天井函数，对输入input张量每个元素向上取整, 即取不小于每个元素的最小整数，并返回结果到输出。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

1.3869
0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.ceil(a)

2
1
-0
-0
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.clamp</p>
</blockquote>
<p>torch.clamp(input, min, max, out=None) → Tensor<br>将输入input张量每个元素的夹紧到区间 $[min,max]$，并返回结果到一个新张量。</p>
<p>操作定义如下：</p>
<pre><code>      | min, if x_i &lt; min
y_i = | x_i, if min &lt;= x_i &lt;= max
      | max, if x_i &gt; max
</code></pre><p>如果输入是FloatTensor or DoubleTensor类型，则参数min max 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，min， max取整数、实数皆可。】</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>min (Number) – 限制范围下限</li>
<li>max (Number) – 限制范围上限</li>
<li>Nout (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

1.3869
0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.clamp(a, min=-0.5, max=0.5)

0.5000
0.3912
-0.5000
-0.5000
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<blockquote>
<p>torch.clamp(input, *, min, out=None) → Tensor</p>
</blockquote>
</blockquote>
<p>将输入input张量每个元素的限制到不小于min ，并返回结果到一个新张量。</p>
<p>如果输入是FloatTensor or DoubleTensor类型，则参数 min 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，min取整数、实数皆可。】</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>value (Number) – 限制范围下限</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

1.3869
0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.clamp(a, min=0.5)

1.3869
0.5000
0.5000
0.5000
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<blockquote>
<p>torch.clamp(input, *, max, out=None) → Tensor</p>
</blockquote>
</blockquote>
<p>将输入input张量每个元素的限制到不大于max ，并返回结果到一个新张量。</p>
<p>如果输入是FloatTensor or DoubleTensor类型，则参数 max 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，max取整数、实数皆可。】</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>value (Number) – 限制范围上限</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

1.3869
0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.clamp(a, max=0.5)

0.5000
0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.cos</p>
</blockquote>
<p>torch.cos(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的余弦。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
0.2718
0.4469
1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.cos(a)
0.8041
0.9633
0.9018
0.2557
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.cosh</p>
</blockquote>
<p>torch.cosh(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的双曲余弦。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
0.2718
0.4469
1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.cosh(a)
1.2095
1.0372
1.1015
1.9917
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.div()</p>
</blockquote>
<p>torch.div(input, value, out=None)<br>将input逐元素除以标量值value，并返回结果到输出张量out。 即 out=tensor/value<br>如果输入是FloatTensor or DoubleTensor类型，则参数 value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>value (Number) – 除数</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a

-0.6147
-1.1237
-0.1604
-0.6853
0.1063
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.div(a, 0.5)

-1.2294
-2.2474
-0.3208
-1.3706
0.2126
[torch.FloatTensor of size 5]
</code></pre><blockquote>
<blockquote>
<p>torch.div(input, other, out=None)</p>
</blockquote>
</blockquote>
<p>两张量input和other逐元素相除，并将结果返回到输出。即， outi=inputi/otheri<br>两张量形状不须匹配，但元素数须一致。</p>
<font color="#ff0000" face="黑体">注意：当形状不匹配时，input的形状作为输出张量的形状。<br></font>

<p>参数：</p>
<ul>
<li>input (Tensor) – 张量(分子)</li>
<li>other (Tensor) – 张量(分母)</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4,4)
&gt;&gt;&gt; a

-0.1810  0.4017  0.2863 -0.1013
0.6183  2.0696  0.9012 -1.5933
0.5679  0.4743 -0.0117 -0.1266
-0.1213  0.9629  0.2682  1.5968
[torch.FloatTensor of size 4x4]

&gt;&gt;&gt; b = torch.randn(8, 2)
&gt;&gt;&gt; b

0.8774  0.7650
0.8866  1.4805
-0.6490  1.1172
1.4259 -0.8146
1.4633 -0.1228
0.4643 -0.6029
0.3492  1.5270
1.6103 -0.6291
[torch.FloatTensor of size 8x2]

&gt;&gt;&gt; torch.div(a, b)

-0.2062  0.5251  0.3229 -0.0684
-0.9528  1.8525  0.6320  1.9559
0.3881 -3.8625 -0.0253  0.2099
-0.3473  0.6306  0.1666 -2.5381
[torch.FloatTensor of size 4x4]
</code></pre><blockquote>
<p>torch.exp</p>
</blockquote>
<p>torch.exp(tensor, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的指数。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
</ul>
<p>out (Tensor, optional) – 输出张量</p>
<pre><code>&gt;&gt;&gt; torch.exp(torch.Tensor([0, math.log(2)]))
torch.FloatTensor([1, 2])
</code></pre><blockquote>
<p>torch.floor</p>
</blockquote>
<p>torch.floor(input, out=None) → Tensor<br>床函数: 返回一个新张量，包含输入input张量每个元素的floor，即不小于元素的最大整数。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

1.3869
0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.floor(a)

1
0
-1
-1
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.fmod</p>
</blockquote>
<p>torch.fmod(input, divisor, out=None) → Tensor<br>计算除法余数。 除数与被除数可能同时含有整数和浮点数。此时，余数的正负与被除数相同。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 被除数</li>
<li>divisor (Tensor or float) – 除数，一个数或与被除数相同类型的张量  </li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; torch.fmod(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2)
torch.FloatTensor([-1, -0, -1, 1, 0, 1])
&gt;&gt;&gt; torch.fmod(torch.Tensor([1, 2, 3, 4, 5]), 1.5)
torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5])
</code></pre><p>参考: torch.remainder(), 计算逐元素余数， 相当于python 中的 % 操作符。</p>
<blockquote>
<p>torch.frac</p>
</blockquote>
<p>torch.frac(tensor, out=None) → Tensor<br>返回每个元素的分数部分。</p>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; torch.frac(torch.Tensor([1, 2.5, -3.2])
torch.FloatTensor([0, 0.5, -0.2])
</code></pre><blockquote>
<p>torch.lerp</p>
</blockquote>
<p>torch.lerp(start, end, weight, out=None)<br>对两个张量以start，end做线性插值， 将结果返回到输出张量。<br>即，outi=starti+weight∗(endi−starti)</p>
<p>参数：</p>
<ul>
<li>start (Tensor) – 起始点张量</li>
<li>end (Tensor) – 终止点张量</li>
<li>weight (float) – 插值公式的weight</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; start = torch.arange(1, 5)
&gt;&gt;&gt; end = torch.Tensor(4).fill_(10)
&gt;&gt;&gt; start

1
2
3
4
[torch.FloatTensor of size 4]

&gt;&gt;&gt; end

10
10
10
10
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.lerp(start, end, 0.5)

5.5000
6.0000
6.5000
7.0000
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.log</p>
</blockquote>
<p>torch.log(input, out=None) → Tensor<br>计算input 的自然对数</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a

-0.4183
0.3722
-0.3091
0.4149
0.5857
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.log(a)

    nan
-0.9883
    nan
-0.8797
-0.5349
[torch.FloatTensor of size 5]
</code></pre><blockquote>
<p>torch.log1p</p>
</blockquote>
<p>torch.log1p(input, out=None) → Tensor<br>计算 input+1的自然对数 yi=log(xi+1)  </p>
<font color="#ff0000" face="黑体">注意：对值比较小的输入，此函数比torch.log()更准确。<br></font>

<p>如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a

-0.4183
0.3722
-0.3091
0.4149
0.5857
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.log1p(a)

-0.5418
0.3164
-0.3697
0.3471
0.4611
[torch.FloatTensor of size 5]
</code></pre><blockquote>
<p>torch.mul</p>
</blockquote>
<p>torch.mul(input, value, out=None)<br>用标量值value乘以输入input的每个元素，并返回一个新的结果张量。 out=tensor∗value<br>如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>value (Number) – 乘到每个元素的数</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3)
&gt;&gt;&gt; a

-0.9374
-0.5254
-0.6069
[torch.FloatTensor of size 3]

&gt;&gt;&gt; torch.mul(a, 100)

-93.7411
-52.5374
-60.6908
[torch.FloatTensor of size 3]
</code></pre><blockquote>
<blockquote>
<p>torch.mul(input, other, out=None)</p>
</blockquote>
</blockquote>
<p>两个张量input,other按元素进行相乘，并返回到输出张量。即计算outi=inputi∗otheri<br>两计算张量形状不须匹配，但总元素数须一致。 注意：当形状不匹配时，input的形状作为输入张量的形状。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 第一个相乘张量</li>
<li>other (Tensor) – 第二个相乘张量</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4,4)
&gt;&gt;&gt; a

-0.7280  0.0598 -1.4327 -0.5825
-0.1427 -0.0690  0.0821 -0.3270
-0.9241  0.5110  0.4070 -1.1188
-0.8308  0.7426 -0.6240 -1.1582
[torch.FloatTensor of size 4x4]

&gt;&gt;&gt; b = torch.randn(2, 8)
&gt;&gt;&gt; b

0.0430 -1.0775  0.6015  1.1647 -0.6549  0.0308 -0.1670  1.0742
-1.2593  0.0292 -0.0849  0.4530  1.2404 -0.4659 -0.1840  0.5974
[torch.FloatTensor of size 2x8]

&gt;&gt;&gt; torch.mul(a, b)

-0.0313 -0.0645 -0.8618 -0.6784
0.0934 -0.0021 -0.0137 -0.3513
1.1638  0.0149 -0.0346 -0.5068
-1.0304 -0.3460  0.1148 -0.6919
[torch.FloatTensor of size 4x4]
</code></pre><blockquote>
<p>torch.neg</p>
</blockquote>
<p>torch.neg(input, out=None) → Tensor<br>返回一个新张量，包含输入input 张量按元素取负。 即， out=−1∗input</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a

-0.4430
1.1690
-0.8836
-0.4565
0.2968
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.neg(a)

0.4430
-1.1690
0.8836
0.4565
-0.2968
[torch.FloatTensor of size 5]
</code></pre><blockquote>
<p>torch.pow</p>
</blockquote>
<p>torch.pow(input, exponent, out=None)<br>对输入input的按元素求exponent次幂值，并返回结果张量。 幂值exponent 可以为单一 float 数或者与input相同元素数的张量。</p>
<p>当幂值为标量时，执行操作：<br>outi=xexponent  </p>
<p>当幂值为张量时，执行操作：<br>outi=xexponenti</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>exponent (float or Tensor) – 幂值</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

-0.5274
-0.8232
-2.1128
1.7558
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.pow(a, 2)

0.2781
0.6776
4.4640
3.0829
[torch.FloatTensor of size 4]

&gt;&gt;&gt; exp = torch.arange(1, 5)
&gt;&gt;&gt; a = torch.arange(1, 5)
&gt;&gt;&gt; a

1
2
3
4
[torch.FloatTensor of size 4]

&gt;&gt;&gt; exp

1
2
3
4
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.pow(a, exp)

1
4
27
256
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<blockquote>
<p>torch.pow(base, input, out=None)</p>
</blockquote>
</blockquote>
<p>base 为标量浮点值,input为张量， 返回的输出张量 out 与输入张量相同形状。</p>
<p>执行操作为:<br>outi=baseinputi</p>
<p>参数：</p>
<ul>
<li>base (float) – 标量值，指数的底</li>
<li>input ( Tensor) – 幂值</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; exp = torch.arange(1, 5)
&gt;&gt;&gt; base = 2
&gt;&gt;&gt; torch.pow(base, exp)

2
4
8
16
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.reciprocal</p>
</blockquote>
<p>torch.reciprocal(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的倒数，即 1.0/x。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

1.3869
0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.reciprocal(a)

0.7210
2.5565
-1.1583
-1.8289
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.remainder</p>
</blockquote>
<p>torch.remainder(input, divisor, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的除法余数。 除数与被除数可能同时包含整数或浮点数。余数与除数有相同的符号。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 被除数</li>
<li>divisor (Tensor or float) – 除数，一个数或者与除数相同大小的张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; torch.remainder(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2)
torch.FloatTensor([1, 0, 1, 1, 0, 1])
&gt;&gt;&gt; torch.remainder(torch.Tensor([1, 2, 3, 4, 5]), 1.5)
torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5])
</code></pre><p>参考: 函数torch.fmod() 同样可以计算除法余数，相当于 C 的 库函数fmod()</p>
<ul>
<li>torch.round</li>
</ul>
<p>torch.round(input, out=None) → Tensor<br>返回一个新张量，将输入input张量每个元素舍入到最近的整数。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

1.2290
1.3409
-0.5662
-0.0899
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.round(a)

1
1
-1
-0
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.rsqrt</p>
</blockquote>
<p>torch.rsqrt(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的平方根倒数。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

1.2290
1.3409
-0.5662
-0.0899
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.rsqrt(a)

0.9020
0.8636
    nan
    nan
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.sigmoid</p>
</blockquote>
<p>torch.sigmoid(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的sigmoid值。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

-0.4972
1.3512
0.1056
-0.2650
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.sigmoid(a)

0.3782
0.7943
0.5264
0.4341
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.sign</p>
</blockquote>
<p>torch.sign(input, out=None) → Tensor<br>符号函数：返回一个新张量，包含输入input张量每个元素的正负。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
0.2718
0.4469
1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.sign(a)

-1
1
1
1
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.sin</p>
</blockquote>
<p>torch.sin(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的正弦。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
0.2718
0.4469
1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.sin(a)
-0.5944
0.2684
0.4322
0.9667
[torch.FloatTensor of size 4]
</code></pre><ul>
<li>torch.sinh</li>
</ul>
<p>torch.sinh(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的双曲正弦。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
0.2718
0.4469
1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.sinh(a)
-0.6804
0.2751
0.4619
1.7225
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.sqrt</p>
</blockquote>
<p>torch.sqrt(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的平方根。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

1.2290
1.3409
-0.5662
-0.0899
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.sqrt(a)

1.1086
1.1580
    nan
    nan
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.tan</p>
</blockquote>
<p>torch.tan(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的正切。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
0.2718
0.4469
1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.tan(a)
-0.7392
0.2786
0.4792
3.7801
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.tanh</p>
</blockquote>
<p>torch.tanh(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的双曲正切。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
-0.6366
0.2718
0.4469
1.3122
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.tanh(a)
-0.5625
0.2653
0.4193
0.8648
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.trunc</p>
</blockquote>
<p>torch.trunc(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的截断值(标量x的截断值是最接近其的整数，其比x更接近零。简而言之，有符号数的小数部分被舍弃)。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

-0.4972
1.3512
0.1056
-0.2650
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.trunc(a)

-0
1
0
-0
[torch.FloatTensor of size 4]
</code></pre><h5 id="Reduction-Ops"><a href="#Reduction-Ops" class="headerlink" title="Reduction Ops"></a>Reduction Ops</h5><blockquote>
<p>torch.cumprod</p>
</blockquote>
<p>torch.cumprod(input, dim, out=None) → Tensor<br>返回输入沿指定维度的累积积。例如，如果输入是一个N 元向量，则结果也是一个N 元向量，第i 个输出元素值为$ yi=x1∗x2∗x3∗…∗xi $</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int) – 累积积操作的维度</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(10)
&gt;&gt;&gt; a

1.1148
1.8423
1.4143
-0.4403
1.2859
-1.2514
-0.4748
1.1735
-1.6332
-0.4272
[torch.FloatTensor of size 10]

&gt;&gt;&gt; torch.cumprod(a, dim=0)

1.1148
2.0537
2.9045
-1.2788
-1.6444
2.0578
-0.9770
-1.1466
1.8726
-0.8000
[torch.FloatTensor of size 10]

&gt;&gt;&gt; a[5] = 0.0
&gt;&gt;&gt; torch.cumprod(a, dim=0)

1.1148
2.0537
2.9045
-1.2788
-1.6444
-0.0000
0.0000
0.0000
-0.0000
0.0000
[torch.FloatTensor of size 10]
</code></pre><blockquote>
<p>torch.cumsum</p>
</blockquote>
<p>torch.cumsum(input, dim, out=None) → Tensor<br>返回输入沿指定维度的累积和。例如，如果输入是一个N元向量，则结果也是一个N元向量，第i 个输出元素值为 yi=x1+x2+x3+…+xi</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int) – 累积和操作的维度</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(10)
&gt;&gt;&gt; a

-0.6039
-0.2214
-0.3705
-0.0169
1.3415
-0.1230
0.9719
0.6081
-0.1286
1.0947
[torch.FloatTensor of size 10]

&gt;&gt;&gt; torch.cumsum(a, dim=0)

-0.6039
-0.8253
-1.1958
-1.2127
0.1288
0.0058
0.9777
1.5858
1.4572
2.5519
[torch.FloatTensor of size 10]
</code></pre><blockquote>
<p>torch.dist</p>
</blockquote>
<p>torch.dist(input, other, p=2, out=None) → Tensor<br>返回 (input - other) 的 p范数 。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>other (Tensor) – 右侧输入张量</li>
<li>p (float, optional) – 所计算的范数</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(4)
&gt;&gt;&gt; x

0.2505
-0.4571
-0.3733
0.7807
[torch.FloatTensor of size 4]

&gt;&gt;&gt; y = torch.randn(4)
&gt;&gt;&gt; y

0.7782
-0.5185
1.4106
-2.4063
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.dist(x, y, 3.5)
3.302832063224223
&gt;&gt;&gt; torch.dist(x, y, 3)
3.3677282206393286
&gt;&gt;&gt; torch.dist(x, y, 0)
inf
&gt;&gt;&gt; torch.dist(x, y, 1)
5.560028076171875
</code></pre><blockquote>
<p>torch.mean</p>
</blockquote>
<p>torch.mean(input) → float<br>返回输入张量所有元素的均值。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a

-0.2946 -0.9143  2.1809
[torch.FloatTensor of size 1x3]

&gt;&gt;&gt; torch.mean(a)
0.32398951053619385
</code></pre><blockquote>
<blockquote>
<p>torch.mean(input, dim, out=None) → Tensor</p>
</blockquote>
</blockquote>
<p>返回输入张量给定维度dim上每行的均值。</p>
<p>输出形状与输入相同，除了给定维度上为1.</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int) – the dimension to reduce</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a

-1.2738 -0.3058  0.1230 -1.9615
0.8771 -0.5430 -0.9233  0.9879
1.4107  0.0317 -0.6823  0.2255
-1.3854  0.4953 -0.2160  0.2435
[torch.FloatTensor of size 4x4]

&gt;&gt;&gt; torch.mean(a, 1)

-0.8545
0.0997
0.2464
-0.2157
[torch.FloatTensor of size 4x1]
</code></pre><blockquote>
<p>torch.median</p>
</blockquote>
<p>torch.median(input, dim=-1, values=None, indices=None) -&gt; (Tensor, LongTensor)<br>返回输入张量给定维度每行的中位数，同时返回一个包含中位数的索引的LongTensor。</p>
<p>dim值默认为输入张量的最后一维。 输出形状与输入相同，除了给定维度上为1.</p>
<font color="#ff0000" face="黑体">注意: 这个函数还没有在torch.cuda.Tensor中定义<br></font>

<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int) – 缩减的维度</li>
<li>values (Tensor, optional) – 结果张量</li>
<li>indices (Tensor, optional) – 返回的索引结果张量  </li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a

-0.6891 -0.6662
0.2697  0.7412
0.5254 -0.7402
0.5528 -0.2399
[torch.FloatTensor of size 4x2]

&gt;&gt;&gt; a = torch.randn(4, 5)
&gt;&gt;&gt; a

0.4056 -0.3372  1.0973 -2.4884  0.4334
2.1336  0.3841  0.1404 -0.1821 -0.7646
-0.2403  1.3975 -2.0068  0.1298  0.0212
-1.5371 -0.7257 -0.4871 -0.2359 -1.1724
[torch.FloatTensor of size 4x5]

&gt;&gt;&gt; torch.median(a, 1)
(
0.4056
0.1404
0.0212
-0.7257
[torch.FloatTensor of size 4x1]
,
0
2
4
1
[torch.LongTensor of size 4x1]
)
</code></pre><blockquote>
<p>torch.mode</p>
</blockquote>
<p>torch.mode(input, dim=-1, values=None, indices=None) -&gt; (Tensor, LongTensor)<br>返回给定维dim上，每行的众数值。 同时返回一个LongTensor，包含众数职的索引。dim值默认为输入张量的最后一维。</p>
<p>输出形状与输入相同，除了给定维度上为1.</p>
<font color="#ff0000" face="黑体">注意: 这个函数还没有在torch.cuda.Tensor中定义<br></font>

<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int) – 缩减的维度</li>
<li>values (Tensor, optional) – 结果张量</li>
<li>indices (Tensor, optional) – 返回的索引张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a

-0.6891 -0.6662
0.2697  0.7412
0.5254 -0.7402
0.5528 -0.2399
[torch.FloatTensor of size 4x2]

&gt;&gt;&gt; a = torch.randn(4, 5)
&gt;&gt;&gt; a

0.4056 -0.3372  1.0973 -2.4884  0.4334
2.1336  0.3841  0.1404 -0.1821 -0.7646
-0.2403  1.3975 -2.0068  0.1298  0.0212
-1.5371 -0.7257 -0.4871 -0.2359 -1.1724
[torch.FloatTensor of size 4x5]

&gt;&gt;&gt; torch.mode(a, 1)
(
-2.4884
-0.7646
-2.0068
-1.5371
[torch.FloatTensor of size 4x1]
,
3
4
2
0
[torch.LongTensor of size 4x1]
)
</code></pre><blockquote>
<p>torch.norm</p>
</blockquote>
<p>torch.norm(input, p=2) → float<br>返回输入张量input 的p 范数。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>p (float,optional) – 范数计算中的幂指数值</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a

-0.4376 -0.5328  0.9547
[torch.FloatTensor of size 1x3]

&gt;&gt;&gt; torch.norm(a, 3)
1.0338925067372466
</code></pre><blockquote>
<blockquote>
<p>torch.norm(input, p, dim, out=None) → Tensor</p>
</blockquote>
</blockquote>
<p>返回输入张量给定维dim 上每行的p 范数。 输出形状与输入相同，除了给定维度上为1.</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>p (float) – 范数计算中的幂指数值</li>
<li>dim (int) – 缩减的维度</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 2)
&gt;&gt;&gt; a

-0.6891 -0.6662
0.2697  0.7412
0.5254 -0.7402
0.5528 -0.2399
[torch.FloatTensor of size 4x2]

&gt;&gt;&gt; torch.norm(a, 2, 1)

0.9585
0.7888
0.9077
0.6026
[torch.FloatTensor of size 4x1]

&gt;&gt;&gt; torch.norm(a, 0, 1)

2
2
2
2
[torch.FloatTensor of size 4x1]
</code></pre><blockquote>
<p>torch.prod</p>
</blockquote>
<p>torch.prod(input) → float<br>返回输入张量input 所有元素的积。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a

0.6170  0.3546  0.0253
[torch.FloatTensor of size 1x3]

&gt;&gt;&gt; torch.prod(a)
0.005537458061418483
</code></pre><blockquote>
<blockquote>
<p>orch.prod(input, dim, out=None) → Tensor</p>
</blockquote>
</blockquote>
<p>返回输入张量给定维度上每行的积。 输出形状与输入相同，除了给定维度上为1.</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int) – 缩减的维度</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 2)
&gt;&gt;&gt; a

0.1598 -0.6884
-0.1831 -0.4412
-0.9925 -0.6244
-0.2416 -0.8080
[torch.FloatTensor of size 4x2]

&gt;&gt;&gt; torch.prod(a, 1)

-0.1100
0.0808
0.6197
0.1952
[torch.FloatTensor of size 4x1]
</code></pre><blockquote>
<p>torch.std</p>
</blockquote>
<p>torch.std(input) → float<br>返回输入张量input 所有元素的标准差。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a

-1.3063  1.4182 -0.3061
[torch.FloatTensor of size 1x3]

&gt;&gt;&gt; torch.std(a)
1.3782334731508061
</code></pre><blockquote>
<blockquote>
<p>torch.std(input, dim, out=None) → Tensor</p>
</blockquote>
</blockquote>
<p>返回输入张量给定维度上每行的标准差。 输出形状与输入相同，除了给定维度上为1.</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int) – 缩减的维度</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a

0.1889 -2.4856  0.0043  1.8169
-0.7701 -0.4682 -2.2410  0.4098
0.1919 -1.1856 -1.0361  0.9085
0.0173  1.0662  0.2143 -0.5576
[torch.FloatTensor of size 4x4]

&gt;&gt;&gt; torch.std(a, dim=1)

1.7756
1.1025
1.0045
0.6725
[torch.FloatTensor of size 4x1]
</code></pre><blockquote>
<p>torch.sum</p>
</blockquote>
<p>torch.sum(input) → float<br>返回输入张量input 所有元素的和。</p>
<p>输出形状与输入相同，除了给定维度上为1.</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a

0.6170  0.3546  0.0253
[torch.FloatTensor of size 1x3]

&gt;&gt;&gt; torch.sum(a)
0.9969287421554327
</code></pre><blockquote>
<blockquote>
<p>torch.sum(input, dim, out=None) → Tensor</p>
</blockquote>
</blockquote>
<p>返回输入张量给定维度上每行的和。 输出形状与输入相同，除了给定维度上为1.</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int) – 缩减的维度</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a

-0.4640  0.0609  0.1122  0.4784
-1.3063  1.6443  0.4714 -0.7396
-1.3561 -0.1959  1.0609 -1.9855
2.6833  0.5746 -0.5709 -0.4430
[torch.FloatTensor of size 4x4]

&gt;&gt;&gt; torch.sum(a, 1)

0.1874
0.0698
-2.4767
2.2440
[torch.FloatTensor of size 4x1]
</code></pre><blockquote>
<p>torch.var</p>
</blockquote>
<p>torch.var(input) → float<br>返回输入张量所有元素的方差</p>
<p>输出形状与输入相同，除了给定维度上为1.</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a

-1.3063  1.4182 -0.3061
[torch.FloatTensor of size 1x3]

&gt;&gt;&gt; torch.var(a)
1.899527506513334
</code></pre><blockquote>
<blockquote>
<p>torch.var(input, dim, out=None) → Tensor</p>
</blockquote>
</blockquote>
<p>返回输入张量给定维度上每行的方差。 输出形状与输入相同，除了给定维度上为1.</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int) – the dimension to reduce</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a

-1.2738 -0.3058  0.1230 -1.9615
0.8771 -0.5430 -0.9233  0.9879
1.4107  0.0317 -0.6823  0.2255
-1.3854  0.4953 -0.2160  0.2435
[torch.FloatTensor of size 4x4]

&gt;&gt;&gt; torch.var(a, 1)

0.8859
0.9509
0.7548
0.6949
[torch.FloatTensor of size 4x1]
</code></pre><h4 id="比较操作-Comparison-Ops"><a href="#比较操作-Comparison-Ops" class="headerlink" title="比较操作 Comparison Ops"></a>比较操作 Comparison Ops</h4><hr>
<blockquote>
<p>torch.eq</p>
</blockquote>
<p>torch.eq(input, other, out=None) → Tensor<br>比较元素相等性。第二个参数可为一个数或与第一个参数同类型形状的张量。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 待比较张量</li>
<li>other (Tensor or float) – 比较张量或数</li>
<li>out (Tensor, optional) – 输出张量，须为 ByteTensor类型 or 与input同类型</li>
</ul>
<p>返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(相等为1，不等为0 )</p>
<p>返回类型： Tensor</p>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; torch.eq(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))
1  0
0  1
[torch.ByteTensor of size 2x2]
</code></pre><blockquote>
<p>torch.equal</p>
</blockquote>
<p>torch.equal(tensor1, tensor2) → bool<br>如果两个张量有相同的形状和元素值，则返回True ，否则 False。</p>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; torch.equal(torch.Tensor([1, 2]), torch.Tensor([1, 2]))
True
</code></pre><blockquote>
<p>torch.ge</p>
</blockquote>
<p>torch.ge(input, other, out=None) → Tensor<br>逐元素比较input和other，即是否 input&gt;=other。</p>
<p>如果两个张量有相同的形状和元素值，则返回True ，否则 False。 第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 待对比的张量</li>
<li>other (Tensor or float) – 对比的张量或float值</li>
<li>out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。<br>返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;= other )。  </li>
</ul>
<p>返回类型： Tensor</p>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; torch.ge(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))
1  1
0  1
[torch.ByteTensor of size 2x2]
</code></pre><blockquote>
<p>torch.gt</p>
</blockquote>
<p>torch.gt(input, other, out=None) → Tensor<br>逐元素比较input和other ， 即是否input&gt;other 如果两个张量有相同的形状和元素值，则返回True ，否则 False。 第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 要对比的张量</li>
<li>other (Tensor or float) – 要对比的张量或float值</li>
<li>out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。<br>返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;= other )。 返回类型： Tensor</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; torch.gt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))
0  1
0  0
[torch.ByteTensor of size 2x2]
</code></pre><blockquote>
<p>torch.kthvalue</p>
</blockquote>
<p>torch.kthvalue(input, k, dim=None, out=None) -&gt; (Tensor, LongTensor)<br>取输入张量input指定维上第k 个最小值。如果不指定dim，则默认为input的最后一维。</p>
<p>返回一个元组 (values,indices)，其中indices是原始输入张量input中沿dim维的第 k 个最小值下标。</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 要对比的张量</li>
<li>k (int) – 第 k 个最小值</li>
<li>dim (int, optional) – 沿着此维进行排序</li>
<li>out (tuple, optional) – 输出元组 (Tensor, LongTensor) 可选地给定作为 输出 buffers</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(1, 6)
&gt;&gt;&gt; x

1
2
3
4
5
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.kthvalue(x, 4)
(
4
[torch.FloatTensor of size 1]
,
3
[torch.LongTensor of size 1]
)
</code></pre><blockquote>
<p>torch.le</p>
</blockquote>
<p>torch.le(input, other, out=None) → Tensor<br>逐元素比较input和other ， 即是否input&lt;=other 第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 要对比的张量</li>
<li>other (Tensor or float ) – 对比的张量或float值</li>
<li>out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。<br>返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;= other )。</li>
</ul>
<p>返回类型： Tensor</p>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; torch.le(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))
1  0
1  1
[torch.ByteTensor of size 2x2]
</code></pre><blockquote>
<p>torch.lt</p>
</blockquote>
<p>torch.lt(input, other, out=None) → Tensor<br>逐元素比较input和other ， 即是否 input&lt;other</p>
<p>第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 要对比的张量</li>
<li>other (Tensor or float ) – 对比的张量或float值</li>
<li>out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。</li>
</ul>
<p>input： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 tensor &gt;= other )。 </p>
<p>返回类型： Tensor</p>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; torch.lt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))
0  0
1  0
[torch.ByteTensor of size 2x2]
</code></pre><blockquote>
<p>torch.max</p>
</blockquote>
<p>torch.max()<br>返回输入张量所有元素的最大值。</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a

0.4729 -0.2266 -0.2085
[torch.FloatTensor of size 1x3]

&gt;&gt;&gt; torch.max(a)
0.4729
</code></pre><blockquote>
<blockquote>
<p>torch.max(input, dim, max=None, max_indices=None) -&gt; (Tensor, LongTensor)</p>
</blockquote>
</blockquote>
<p>返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引。</p>
<p>输出形状中，将dim维设定为1，其它与输入形状保持一致。</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int) – 指定的维度</li>
<li>max (Tensor, optional) – 结果张量，包含给定维度上的最大值</li>
<li>max_indices (LongTensor, optional) – 结果张量，包含给定维度上每个最大值的位置索引</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt; a = torch.randn(4, 4)
&gt;&gt; a

0.0692  0.3142  1.2513 -0.5428
0.9288  0.8552 -0.2073  0.6409
1.0695 -0.0101 -2.4507 -1.2230
0.7426 -0.7666  0.4862 -0.6628
torch.FloatTensor of size 4x4]

&gt;&gt;&gt; torch.max(a, 1)
(
1.2513
0.9288
1.0695
0.7426
[torch.FloatTensor of size 4x1]
,
2
0
0
0
[torch.LongTensor of size 4x1]
)
</code></pre><blockquote>
<p>torch.max(input, other, out=None) → Tensor</p>
</blockquote>
<p>返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引。 即，outi=max(inputi,otheri)<br>输出形状中，将dim维设定为1，其它与输入形状保持一致。</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>other (Tensor) – 输出张量</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

1.3869
0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]

&gt;&gt;&gt; b = torch.randn(4)
&gt;&gt;&gt; b

1.0067
-0.8010
0.6258
0.3627
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.max(a, b)

1.3869
0.3912
0.6258
0.3627
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.min</p>
</blockquote>
<p>torch.min(input) → float<br>返回输入张量所有元素的最小值。</p>
<p>参数: </p>
<ul>
<li>input (Tensor) – 输入张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a

0.4729 -0.2266 -0.2085
[torch.FloatTensor of size 1x3]

&gt;&gt;&gt; torch.min(a)
-0.22663167119026184
</code></pre><blockquote>
<blockquote>
<p>torch.min(input, dim, min=None, min_indices=None) -&gt; (Tensor, LongTensor)</p>
</blockquote>
</blockquote>
<p>返回输入张量给定维度上每行的最小值，并同时返回每个最小值的位置索引。</p>
<p>输出形状中，将dim维设定为1，其它与输入形状保持一致。</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int) – 指定的维度</li>
<li>min (Tensor, optional) – 结果张量，包含给定维度上的最小值</li>
<li>min_indices (LongTensor, optional) – 结果张量，包含给定维度上每个最小值的位置索引</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt; a = torch.randn(4, 4)
&gt;&gt; a

0.0692  0.3142  1.2513 -0.5428
0.9288  0.8552 -0.2073  0.6409
1.0695 -0.0101 -2.4507 -1.2230
0.7426 -0.7666  0.4862 -0.6628
torch.FloatTensor of size 4x4]

&gt;&gt; torch.min(a, 1)

0.5428
0.2073
2.4507
0.7666
torch.FloatTensor of size 4x1]

3
2
2
1
torch.LongTensor of size 4x1]
</code></pre><blockquote>
<blockquote>
<p>torch.min(input, other, out=None) → Tensor</p>
</blockquote>
</blockquote>
<p>input中逐元素与other相应位置的元素对比，返回最小值到输出张量。即，outi=min(tensori,otheri)<br>两张量形状不需匹配，但元素数须相同。</p>
<font color="#ff0000" face="黑体">注意：当形状不匹配时，input的形状作为返回张量的形状。<br></font>

<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>other (Tensor) – 第二个输入张量</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a

1.3869
0.3912
-0.8634
-0.5468
[torch.FloatTensor of size 4]

&gt;&gt;&gt; b = torch.randn(4)
&gt;&gt;&gt; b

1.0067
-0.8010
0.6258
0.3627
[torch.FloatTensor of size 4]

&gt;&gt;&gt; torch.min(a, b)

1.0067
-0.8010
-0.8634
-0.5468
[torch.FloatTensor of size 4]
</code></pre><blockquote>
<p>torch.ne</p>
</blockquote>
<p>torch.ne(input, other, out=None) → Tensor<br>逐元素比较input和other ， 即是否 input!=other。 第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 待对比的张量</li>
<li>other (Tensor or float) – 对比的张量或float值</li>
<li>out (Tensor, optional) – 输出张量。必须为ByteTensor或者与input相同类型。</li>
</ul>
<p>返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果 (如果 tensor != other 为True ，返回1)。</p>
<p>返回类型： Tensor</p>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; torch.ne(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))
0  1
1  0
[torch.ByteTensor of size 2x2]
</code></pre><blockquote>
<p>torch.sort</p>
</blockquote>
<p>torch.sort(input, dim=None, descending=False, out=None) -&gt; (Tensor, LongTensor)<br>对输入张量input沿着指定维按升序排序。如果不给定dim，则默认为输入的最后一维。如果指定参数descending为True，则按降序排序</p>
<p>返回元组 (sorted_tensor, sorted_indices) ， sorted_indices 为原始输入中的下标。</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 要对比的张量</li>
<li>dim (int, optional) – 沿着此维排序</li>
<li>descending (bool, optional) – 布尔值，控制升降排序</li>
<li>out (tuple, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)
&gt;&gt;&gt; sorted, indices = torch.sort(x)
&gt;&gt;&gt; sorted

-1.6747  0.0610  0.1190  1.4137
-1.4782  0.7159  1.0341  1.3678
-0.3324 -0.0782  0.3518  0.4763
[torch.FloatTensor of size 3x4]

&gt;&gt;&gt; indices

0  1  3  2
2  1  0  3
3  1  0  2
[torch.LongTensor of size 3x4]

&gt;&gt;&gt; sorted, indices = torch.sort(x, 0)
&gt;&gt;&gt; sorted

-1.6747 -0.0782 -1.4782 -0.3324
0.3518  0.0610  0.4763  0.1190
1.0341  0.7159  1.4137  1.3678
[torch.FloatTensor of size 3x4]

&gt;&gt;&gt; indices

0  2  1  2
2  0  2  0
1  1  0  1
[torch.LongTensor of size 3x4]
</code></pre><blockquote>
<p>torch.topk</p>
</blockquote>
<p>torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor)<br>沿给定dim维度返回输入张量input中 k 个最大值。 如果不指定dim，则默认为input的最后一维。 如果为largest为 False ，则返回最小的 k 个值。</p>
<p>返回一个元组 (values,indices)，其中indices是原始输入张量input中测元素下标。 如果设定布尔值sorted 为<em>True</em>，将会确保返回的 k 个值被排序。</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>k (int) – “top-k”中的k</li>
<li>dim (int, optional) – 排序的维</li>
<li>largest (bool, optional) – 布尔值，控制返回最大或最小值</li>
<li>sorted (bool, optional) – 布尔值，控制返回值是否排序</li>
<li>out (tuple, optional) – 可选输出张量 (Tensor, LongTensor) output buffers</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(1, 6)
&gt;&gt;&gt; x

1
2
3
4
5
[torch.FloatTensor of size 5]

&gt;&gt;&gt; torch.topk(x, 3)
(
5
4
3
[torch.FloatTensor of size 3]
,
4
3
2
[torch.LongTensor of size 3]
)
&gt;&gt;&gt; torch.topk(x, 3, 0, largest=False)
(
1
2
3
[torch.FloatTensor of size 3]
,
0
1
2
[torch.LongTensor of size 3]
)
</code></pre><h4 id="其它操作-Other-Operations"><a href="#其它操作-Other-Operations" class="headerlink" title="其它操作 Other Operations"></a>其它操作 Other Operations</h4><hr>
<blockquote>
<p>torch.cross</p>
</blockquote>
<p>torch.cross(input, other, dim=-1, out=None) → Tensor<br>返回沿着维度dim上，两个张量input和other的向量积（叉积）。 input和other 必须有相同的形状，且指定的dim维上size必须为3。</p>
<p>如果不指定dim，则默认为第一个尺度为3的维。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>other (Tensor) – 第二个输入张量</li>
<li>dim (int, optional) – 沿着此维进行叉积操作</li>
<li>out (Tensor,optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 3)
&gt;&gt;&gt; a

-0.6652 -1.0116 -0.6857
0.2286  0.4446 -0.5272
0.0476  0.2321  1.9991
0.6199  1.1924 -0.9397
[torch.FloatTensor of size 4x3]

&gt;&gt;&gt; b = torch.randn(4, 3)
&gt;&gt;&gt; b

-0.1042 -1.1156  0.1947
0.9947  0.1149  0.4701
-1.0108  0.8319 -0.0750
0.9045 -1.3754  1.0976
[torch.FloatTensor of size 4x3]

&gt;&gt;&gt; torch.cross(a, b, dim=1)

-0.9619  0.2009  0.6367
0.2696 -0.6318 -0.4160
-1.6805 -2.0171  0.2741
0.0163 -1.5304 -1.9311
[torch.FloatTensor of size 4x3]

&gt;&gt;&gt; torch.cross(a, b)

-0.9619  0.2009  0.6367
0.2696 -0.6318 -0.4160
-1.6805 -2.0171  0.2741
0.0163 -1.5304 -1.9311
[torch.FloatTensor of size 4x3]
</code></pre><ul>
<li>torch.diag</li>
</ul>
<p>torch.diag(input, diagonal=0, out=None) → Tensor<br>如果输入是一个向量(1D 张量)，则返回一个以input为对角线元素的2D方阵<br>如果输入是一个矩阵(2D 张量)，则返回一个包含input对角线元素的1D张量<br>参数diagonal指定对角线:</p>
<p>diagonal = 0, 主对角线<br>diagonal &gt; 0, 主对角线之上<br>diagonal &lt; 0, 主对角线之下  </p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>diagonal (int, optional) – 指定对角线</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<p>取得以input为对角线的方阵：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3)
&gt;&gt;&gt; a

1.0480
-2.3405
-1.1138
[torch.FloatTensor of size 3]

&gt;&gt;&gt; torch.diag(a)

1.0480  0.0000  0.0000
0.0000 -2.3405  0.0000
0.0000  0.0000 -1.1138
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.diag(a, 1)

0.0000  1.0480  0.0000  0.0000
0.0000  0.0000 -2.3405  0.0000
0.0000  0.0000  0.0000 -1.1138
0.0000  0.0000  0.0000  0.0000
[torch.FloatTensor of size 4x4]
</code></pre><p>取得给定矩阵第k个对角线:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a

-1.5328 -1.3210 -1.5204
0.8596  0.0471 -0.2239
-0.6617  0.0146 -1.0817
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.diag(a, 0)

-1.5328
0.0471
-1.0817
[torch.FloatTensor of size 3]

&gt;&gt;&gt; torch.diag(a, 1)

-1.3210
-0.2239
[torch.FloatTensor of size 2]
</code></pre><blockquote>
<p>torch.histc</p>
</blockquote>
<p>torch.histc(input, bins=100, min=0, max=0, out=None) → Tensor<br>计算输入张量的直方图。以min和max为range边界，将其均分成bins个直条，然后将排序好的数据划分到各个直条(bins)中。如果min和max都为0, 则利用数据中的最大最小值作为边界。</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>bins (int) – 直方图 bins(直条)的个数(默认100个)</li>
<li>min (int) – range的下边界(包含)</li>
<li>max (int) – range的上边界(包含)</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>返回： 直方图 </p>
<p>返回类型：张量</p>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; torch.histc(torch.FloatTensor([1, 2, 1]), bins=4, min=0, max=3)
FloatTensor([0, 2, 1, 0])
</code></pre><blockquote>
<p>torch.renorm</p>
</blockquote>
<p>torch.renorm(input, p, dim, maxnorm, out=None) → Tensor<br>返回一个张量，包含规范化后的各个子张量，使得沿着dim维划分的各子张量的p范数小于maxnorm。</p>
<font color="#ff0000" face="黑体">注意 如果p范数的值小于maxnorm，则当前子张量不需要修改。<br></font>

<font color="#ff0000" face="黑体">注意: 更详细解释参考torch7 以及Hinton et al. 2012, p. 2<br></font>

<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>p (float) – 范数的p</li>
<li>dim (int) – 沿着此维切片，得到张量子集</li>
<li>maxnorm (float) – 每个子张量的范数的最大值</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; x = torch.ones(3, 3)
&gt;&gt;&gt; x[1].fill_(2)
&gt;&gt;&gt; x[2].fill_(3)
&gt;&gt;&gt; x

1  1  1
2  2  2
3  3  3
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.renorm(x, 1, 0, 5)

1.0000  1.0000  1.0000
1.6667  1.6667  1.6667
1.6667  1.6667  1.6667
[torch.FloatTensor of size 3x3]
</code></pre><blockquote>
<p>torch.trace</p>
</blockquote>
<p>torch.trace(input) → float<br>返回输入2维矩阵对角线元素的和(迹)</p>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(1, 10).view(3, 3)
&gt;&gt;&gt; x

1  2  3
4  5  6
7  8  9
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.trace(x)
15.0
</code></pre><blockquote>
<p>torch.tril</p>
</blockquote>
<p>torch.tril(input, k=0, out=None) → Tensor<br>返回一个张量out，包含输入矩阵(2D张量)的下三角部分，out其余部分被设为0。这里所说的下三角部分为矩阵指定对角线diagonal之上的元素。</p>
<p>参数k控制对角线:</p>
<ul>
<li>k = 0, 主对角线</li>
<li>k &gt; 0, 主对角线之上</li>
<li>k &lt; 0, 主对角线之下</li>
</ul>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>k (int, optional) – 指定对角线</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3,3)
&gt;&gt;&gt; a

1.3225  1.7304  1.4573
-0.3052 -0.3111 -0.1809
1.2469  0.0064 -1.6250
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.tril(a)

1.3225  0.0000  0.0000
-0.3052 -0.3111  0.0000
1.2469  0.0064 -1.6250
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.tril(a, k=1)

1.3225  1.7304  0.0000
-0.3052 -0.3111 -0.1809
1.2469  0.0064 -1.6250
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.tril(a, k=-1)

0.0000  0.0000  0.0000
-0.3052  0.0000  0.0000
1.2469  0.0064  0.0000
[torch.FloatTensor of size 3x3]
</code></pre><blockquote>
<p>torch.triu</p>
</blockquote>
<p>torch.triu(input, k=0, out=None) → Tensor<br>返回一个张量，包含输入矩阵(2D张量)的上三角部分，其余部分被设为0。这里所说的上三角部分为矩阵指定对角线diagonal之上的元素。</p>
<p>参数k控制对角线:</p>
<ul>
<li>k = 0, 主对角线</li>
<li>k &gt; 0, 主对角线之上</li>
<li>k &lt; 0, 主对角线之下</li>
</ul>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>k (int, optional) – 指定对角线</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3,3)
&gt;&gt;&gt; a

1.3225  1.7304  1.4573
-0.3052 -0.3111 -0.1809
1.2469  0.0064 -1.6250
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.triu(a)

1.3225  1.7304  1.4573
0.0000 -0.3111 -0.1809
0.0000  0.0000 -1.6250
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.triu(a, k=1)

0.0000  1.7304  1.4573
0.0000  0.0000 -0.1809
0.0000  0.0000  0.0000
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.triu(a, k=-1)

1.3225  1.7304  1.4573
-0.3052 -0.3111 -0.1809
0.0000  0.0064 -1.6250
[torch.FloatTensor of size 3x3]
</code></pre><h4 id="BLAS-and-LAPACK-Operations"><a href="#BLAS-and-LAPACK-Operations" class="headerlink" title="BLAS and LAPACK Operations"></a>BLAS and LAPACK Operations</h4><blockquote>
<p>torch.addbmm</p>
</blockquote>
<p>torch.addbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) → Tensor</p>
<p>对两个批batch1和batch2内存储的矩阵进行批矩阵乘操作，附带reduced add 步骤( 所有矩阵乘结果沿着第一维相加)。矩阵mat加到最终结果。 batch1和 batch2都为包含相同数量矩阵的3维张量。 如果batch1是形为b×n×m的张量，batch1是形为b×m×p的张量，则out和mat的形状都是n×p，即 res=(beta∗M)+(alpha∗sum(batch1i@batch2i,i=0,b))<br>对类型为 FloatTensor 或 DoubleTensor 的输入，alphaand beta必须为实数，否则两个参数须为整数。</p>
<p>参数：</p>
<ul>
<li>beta (Number, optional) – 用于mat的乘子</li>
<li>mat (Tensor) – 相加矩阵</li>
<li>alpha (Number, optional) – 用于batch1@batch2的乘子</li>
<li>batch1 (Tensor) – 第一批相乘矩阵</li>
<li>batch2 (Tensor) – 第二批相乘矩阵</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子:</p>
<pre><code>&gt;&gt;&gt; M = torch.randn(3, 5)
&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; torch.addbmm(M, batch1, batch2)

-3.1162  11.0071   7.3102   0.1824  -7.6892
1.8265   6.0739   0.4589  -0.5641  -5.4283
-9.3387  -0.1794  -1.2318  -6.8841  -4.7239
[torch.FloatTensor of size 3x5]
</code></pre><blockquote>
<p>torch.addmm</p>
</blockquote>
<p>torch.addmm(beta=1, mat, alpha=1, mat1, mat2, out=None) → Tensor</p>
<p>对矩阵mat1和mat2进行矩阵乘操作。矩阵mat加到最终结果。如果mat1 是一个 n×m张量，mat2 是一个 m×p张量，那么out和mat的形状为n×p。 alpha 和 beta 分别是两个矩阵 mat1@mat2和mat的比例因子，即， out=(beta∗M)+(alpha∗mat1@mat2)</p>
<p>对类型为 FloatTensor 或 DoubleTensor 的输入，betaand alpha必须为实数，否则两个参数须为整数。</p>
<p>参数 ：</p>
<ul>
<li>beta (Number, optional) – 用于mat的乘子</li>
<li>mat (Tensor) – 相加矩阵</li>
<li>alpha (Number, optional) – 用于mat1@mat2的乘子</li>
<li>mat1 (Tensor) – 第一个相乘矩阵</li>
<li>mat2 (Tensor) – 第二个相乘矩阵</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; M = torch.randn(2, 3)
&gt;&gt;&gt; mat1 = torch.randn(2, 3)
&gt;&gt;&gt; mat2 = torch.randn(3, 3)
&gt;&gt;&gt; torch.addmm(M, mat1, mat2)

-0.4095 -1.9703  1.3561
5.7674 -4.9760  2.7378
[torch.FloatTensor of size 2x3]
</code></pre><blockquote>
<p>torch.addmv</p>
</blockquote>
<p>torch.addmv(beta=1, tensor, alpha=1, mat, vec, out=None) → Tensor</p>
<p>对矩阵mat和向量vec对进行相乘操作。向量tensor加到最终结果。如果mat 是一个 n×m维矩阵，vec 是一个 m维向量，那么out和mat的为n元向量。 可选参数<em>alpha</em> 和 beta 分别是 mat∗vec和mat的比例因子，即， out=(beta∗tensor)+(alpha∗(mat@vec))</p>
<p>对类型为<em>FloatTensor</em>或<em>DoubleTensor</em>的输入，alphaand beta必须为实数，否则两个参数须为整数。</p>
<p>参数 ：</p>
<ul>
<li>beta (Number, optional) – 用于mat的乘子</li>
<li>mat (Tensor) – 相加矩阵</li>
<li>alpha (Number, optional) – 用于mat1@vec的乘子</li>
<li>mat (Tensor) – 相乘矩阵</li>
<li>vec (Tensor) – 相乘向量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; M = torch.randn(2)
&gt;&gt;&gt; mat = torch.randn(2, 3)
&gt;&gt;&gt; vec = torch.randn(3)
&gt;&gt;&gt; torch.addmv(M, mat, vec)

-2.0939
-2.2950
[torch.FloatTensor of size 2]
</code></pre><blockquote>
<p>torch.addr</p>
</blockquote>
<p>torch.addr(beta=1, mat, alpha=1, vec1, vec2, out=None) → Tensor<br>对向量vec1和vec2对进行张量积操作。矩阵mat加到最终结果。如果vec1 是一个 n维向量，vec2 是一个 m维向量，那么矩阵mat的形状须为n×m。 可选参数<em>beta</em> 和 alpha 分别是两个矩阵 mat和 vec1@vec2的比例因子，即，resi=(beta∗Mi)+(alpha∗batch1i×batch2i)</p>
<p>对类型为<em>FloatTensor</em>或<em>DoubleTensor</em>的输入，alphaand beta必须为实数，否则两个参数须为整数。</p>
<p>参数 ：</p>
<ul>
<li>beta (Number, optional) – 用于mat的乘子</li>
<li>mat (Tensor) – 相加矩阵</li>
<li>alpha (Number, optional) – 用于两向量vec1，vec2外积的乘子</li>
<li>vec1 (Tensor) – 第一个相乘向量</li>
<li>vec2 (Tensor) – 第二个相乘向量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; vec1 = torch.arange(1, 4)
&gt;&gt;&gt; vec2 = torch.arange(1, 3)
&gt;&gt;&gt; M = torch.zeros(3, 2)
&gt;&gt;&gt; torch.addr(M, vec1, vec2)
1  2
2  4
3  6
[torch.FloatTensor of size 3x2]
</code></pre><blockquote>
<p>torch.baddbmm</p>
</blockquote>
<p>torch.baddbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) → Tensor</p>
<p>对两个批batch1和batch2内存储的矩阵进行批矩阵乘操作，矩阵mat加到最终结果。 batch1和  batch2都为包含相同数量矩阵的3维张量。 如果batch1是形为b×n×m的张量，batch1是形为b×m×p的张量，则out和mat的形状都是n×p，即 resi=(beta∗Mi)+(alpha∗batch1i×batch2i)<br>对类型为<em>FloatTensor</em>或<em>DoubleTensor</em>的输入，alphaand beta必须为实数，否则两个参数须为整数。</p>
<p>参数：</p>
<ul>
<li>beta (Number, optional) – 用于mat的乘子</li>
<li>mat (Tensor) – 相加矩阵</li>
<li>alpha (Number, optional) – 用于batch1@batch2的乘子</li>
<li>batch1 (Tensor) – 第一批相乘矩阵</li>
<li>batch2 (Tensor) – 第二批相乘矩阵</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; M = torch.randn(10, 3, 5)
&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size()
torch.Size([10, 3, 5])
</code></pre><blockquote>
<p>torch.bmm</p>
</blockquote>
<p>torch.bmm(batch1, batch2, out=None) → Tensor<br>对存储在两个批batch1和batch2内的矩阵进行批矩阵乘操作。batch1和 batch2都为包含相同数量矩阵的3维张量。 如果batch1是形为b×n×m的张量，batch1是形为b×m×p的张量，则out和mat的形状都是n×p，即 res=(beta∗M)+(alpha∗sum(batch1i@batch2i,i=0,b))<br>对类型为 FloatTensor 或 DoubleTensor 的输入，alphaand beta必须为实数，否则两个参数须为整数。</p>
<p>参数：</p>
<ul>
<li>batch1 (Tensor) – 第一批相乘矩阵</li>
<li>batch2 (Tensor) – 第二批相乘矩阵</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; res = torch.bmm(batch1, batch2)
&gt;&gt;&gt; res.size()
torch.Size([10, 3, 5])
</code></pre><blockquote>
<p>torch.btrifact</p>
</blockquote>
<p>torch.btrifact(A, info=None) → Tensor, IntTensor<br>返回一个元组，包含LU 分解和pivots 。 可选参数info决定是否对每个minibatch样本进行分解。info are from dgetrf and a non-zero value indicates an error occurred. 如果用CUDA的话，这个值来自于CUBLAS，否则来自LAPACK。</p>
<p>参数： </p>
<ul>
<li>A (Tensor) – 待分解张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)
&gt;&gt;&gt; A_LU = A.btrifact()
</code></pre><blockquote>
<p>torch.btrisolve</p>
</blockquote>
<p>torch.btrisolve(b, LU_data, LU_pivots) → Tensor<br>返回线性方程组Ax=b的LU解。</p>
<p>参数：</p>
<ul>
<li>b (Tensor) – RHS 张量.</li>
<li>LU_data (Tensor) – Pivoted LU factorization of A from btrifact.</li>
<li>LU_pivots (IntTensor) – LU 分解的Pivots.</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)
&gt;&gt;&gt; b = torch.randn(2, 3)
&gt;&gt;&gt; A_LU = torch.btrifact(A)
&gt;&gt;&gt; x = b.btrisolve(*A_LU)
&gt;&gt;&gt; torch.norm(A.bmm(x.unsqueeze(2)) - b)
6.664001874625056e-08
</code></pre><blockquote>
<p>torch.dot</p>
</blockquote>
<p>torch.dot(tensor1, tensor2) → float<br>计算两个张量的点乘(内乘),两个张量都为1-D 向量.</p>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; torch.dot(torch.Tensor([2, 3]), torch.Tensor([2, 1]))
7.0
</code></pre><blockquote>
<p>torch.eig</p>
</blockquote>
<p>torch.eig(a, eigenvectors=False, out=None) -&gt; (Tensor, Tensor)<br>计算实方阵a 的特征值和特征向量</p>
<p>参数：</p>
<ul>
<li>a (Tensor) – 方阵，待计算其特征值和特征向量</li>
<li>eigenvectors (bool) – 布尔值，如果True，则同时计算特征值和特征向量，否则只计算特征值。</li>
<li>out (tuple, optional) – 输出元组</li>
</ul>
<p>返回值：<br>元组，包括：</p>
<ul>
<li>e (Tensor): a 的右特征向量</li>
<li>v (Tensor): 如果eigenvectors为True，则为包含特征向量的张量; 否则为空张量</li>
</ul>
<p>返回值类型： (Tensor, Tensor)</p>
<blockquote>
<p>torch.gels</p>
</blockquote>
<p>torch.gels(B, A, out=None) → Tensor<br>对形如m×n的满秩矩阵a计算其最小二乘和最小范数问题的解。 如果m&gt;=n,gels对最小二乘问题进行求解，即：<br>minimize∥AX−B∥F<br>如果m&lt;n,gels求解最小范数问题，即：<br>minimize∥X∥Fsubject toabAX=B<br>返回矩阵X的前n 行包含解。余下的行包含以下残差信息: 相应列从第n 行开始计算的每列的欧式距离。</p>
<font color="#ff0000" face="黑体">注意： 返回矩阵总是被转置，无论输入矩阵的原始布局如何，总会被转置；即，总是有 stride (1, m) 而不是 (m, 1).<br></font>

<p>参数：</p>
<ul>
<li>B (Tensor) – 矩阵B</li>
<li>A (Tensor) – m×n矩阵</li>
<li>out (tuple, optional) – 输出元组</li>
</ul>
<p>返回值： 元组，包括：</p>
<ul>
<li>X (Tensor): 最小二乘解</li>
<li>qr (Tensor): QR 分解的细节</li>
</ul>
<p>返回值类型： (Tensor, Tensor)</p>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; A = torch.Tensor([[1, 1, 1],
...                   [2, 3, 4],
...                   [3, 5, 2],
...                   [4, 2, 5],
...                   [5, 4, 3]])
&gt;&gt;&gt; B = torch.Tensor([[-10, -3],
                    [ 12, 14],
                    [ 14, 12],
                    [ 16, 16],
                    [ 18, 16]])
&gt;&gt;&gt; X, _ = torch.gels(B, A)
&gt;&gt;&gt; X
2.0000  1.0000
1.0000  1.0000
1.0000  2.0000
[torch.FloatTensor of size 3x2]
</code></pre><blockquote>
<p>torch.geqrf</p>
</blockquote>
<p>torch.geqrf(input, out=None) -&gt; (Tensor, Tensor)</p>
<p>这是一个直接调用LAPACK的底层函数。 一般使用torch.qr()</p>
<p>计算输入的QR 分解，但是并不会分别创建Q,R两个矩阵，而是直接调用LAPACK 函数 Rather, this directly calls the underlying LAPACK function ?geqrf which produces a sequence of ‘elementary reflectors’.</p>
<p>参考 LAPACK文档获取更详细信息。</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入矩阵</li>
<li>out (tuple, optional) – 元组，包含输出张量 (Tensor, Tensor)</li>
</ul>
<blockquote>
<p>torch.ger</p>
</blockquote>
<p>torch.ger(vec1, vec2, out=None) → Tensor<br>计算两向量vec1,vec2的张量积。如果vec1的长度为n,vec2长度为m，则输出out应为形如n x m的矩阵。</p>
<p>参数:</p>
<ul>
<li>vec1 (Tensor) – 1D 输入向量</li>
<li>vec2 (Tensor) – 1D 输入向量</li>
<li>out (tuple, optional) – 输出张量</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; v1 = torch.arange(1, 5)
&gt;&gt;&gt; v2 = torch.arange(1, 4)
&gt;&gt;&gt; torch.ger(v1, v2)

1   2   3
2   4   6
3   6   9
4   8  12
[torch.FloatTensor of size 4x3]
</code></pre><blockquote>
<p>torch.gesv</p>
</blockquote>
<p>torch.gesv(B, A, out=None) -&gt; (Tensor, Tensor)</p>
<p>X,LU=torch.gesv(B,A)，返回线性方程组AX=B的解。</p>
<p>LU 包含两个矩阵L，U。A须为非奇异方阵，如果A是一个m×m矩阵，B 是m×k矩阵，则LU 是m×m矩阵， X为m×k矩阵</p>
<p>参数：</p>
<ul>
<li>B (Tensor) – m×k矩阵</li>
<li>A (Tensor) – m×m矩阵</li>
<li>out (Tensor, optional) – 可选地输出矩阵X</li>
</ul>
<p>例子:</p>
<pre><code>&gt;&gt;&gt; A = torch.Tensor([[6.80, -2.11,  5.66,  5.97,  8.23],
...                   [-6.05, -3.30,  5.36, -4.44,  1.08],
...                   [-0.45,  2.58, -2.70,  0.27,  9.04],
...                   [8.32,  2.71,  4.35,  -7.17,  2.14],
...                   [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()
&gt;&gt;&gt; B = torch.Tensor([[4.02,  6.19, -8.22, -7.57, -3.03],
...                   [-1.56,  4.00, -8.67,  1.75,  2.86],
...                   [9.81, -4.09, -4.57, -8.61,  8.99]]).t()
&gt;&gt;&gt; X, LU = torch.gesv(B, A)
&gt;&gt;&gt; torch.dist(B, torch.mm(A, X))
9.250057093890353e-06
</code></pre><blockquote>
<p>torch.inverse</p>
</blockquote>
<p>torch.inverse(input, out=None) → Tensor</p>
<p>对方阵输入input 取逆。</p>
<font color="#ff0000" face="黑体">注意： 返回矩阵总是被转置，无论输入矩阵的原始布局如何，总会被转置；即，总是有 stride (1, m) 而不是 (m, 1).<br></font>

<p>参数 ：</p>
<ul>
<li>input (Tensor) – 输入2维张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子:</p>
<pre><code>&gt;&gt;&gt; x = torch.rand(10, 10)
&gt;&gt;&gt; x

0.7800  0.2267  0.7855  0.9479  0.5914  0.7119  0.4437  0.9131  0.1289  0.1982
0.0045  0.0425  0.2229  0.4626  0.6210  0.0207  0.6338  0.7067  0.6381  0.8196
0.8350  0.7810  0.8526  0.9364  0.7504  0.2737  0.0694  0.5899  0.8516  0.3883
0.6280  0.6016  0.5357  0.2936  0.7827  0.2772  0.0744  0.2627  0.6326  0.9153
0.7897  0.0226  0.3102  0.0198  0.9415  0.9896  0.3528  0.9397  0.2074  0.6980
0.5235  0.6119  0.6522  0.3399  0.3205  0.5555  0.8454  0.3792  0.4927  0.6086
0.1048  0.0328  0.5734  0.6318  0.9802  0.4458  0.0979  0.3320  0.3701  0.0909
0.2616  0.3485  0.4370  0.5620  0.5291  0.8295  0.7693  0.1807  0.0650  0.8497
0.1655  0.2192  0.6913  0.0093  0.0178  0.3064  0.6715  0.5101  0.2561  0.3396
0.4370  0.4695  0.8333  0.1180  0.4266  0.4161  0.0699  0.4263  0.8865  0.2578
[torch.FloatTensor of size 10x10]

&gt;&gt;&gt; x = torch.rand(10, 10)
&gt;&gt;&gt; y = torch.inverse(x)
&gt;&gt;&gt; z = torch.mm(x, y)
&gt;&gt;&gt; z

1.0000  0.0000  0.0000 -0.0000  0.0000  0.0000  0.0000  0.0000 -0.0000 -0.0000
0.0000  1.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000 -0.0000 -0.0000 -0.0000
0.0000  0.0000  1.0000 -0.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000 -0.0000
0.0000  0.0000  0.0000  1.0000  0.0000  0.0000  0.0000 -0.0000 -0.0000  0.0000
0.0000  0.0000 -0.0000 -0.0000  1.0000  0.0000  0.0000 -0.0000 -0.0000 -0.0000
0.0000  0.0000  0.0000 -0.0000  0.0000  1.0000 -0.0000 -0.0000 -0.0000 -0.0000
0.0000  0.0000  0.0000 -0.0000  0.0000  0.0000  1.0000  0.0000 -0.0000  0.0000
0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000  1.0000 -0.0000  0.0000
-0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000 -0.0000  1.0000 -0.0000
-0.0000  0.0000 -0.0000 -0.0000 -0.0000  0.0000 -0.0000 -0.0000  0.0000  1.0000
[torch.FloatTensor of size 10x10]

&gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(10))) # Max nonzero
5.096662789583206e-07
</code></pre><blockquote>
<p>torch.mm</p>
</blockquote>
<p>torch.mm(mat1, mat2, out=None) → Tensor</p>
<p>对矩阵mat1和mat2进行相乘。 如果mat1 是一个n×m 张量，mat2 是一个 m×p 张量，将会输出一个 n×p 张量out。</p>
<p>参数 ：</p>
<ul>
<li>mat1 (Tensor) – 第一个相乘矩阵</li>
<li>mat2 (Tensor) – 第二个相乘矩阵</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子:</p>
<pre><code>&gt;&gt;&gt; mat1 = torch.randn(2, 3)
&gt;&gt;&gt; mat2 = torch.randn(3, 3)
&gt;&gt;&gt; torch.mm(mat1, mat2)
0.0519 -0.3304  1.2232
4.3910 -5.1498  2.7571
[torch.FloatTensor of size 2x3]
</code></pre><blockquote>
<p>torch.mv</p>
</blockquote>
<p>torch.mv(mat, vec, out=None) → Tensor</p>
<p>对矩阵mat和向量vec进行相乘。 如果mat 是一个n×m张量，vec 是一个m元 1维张量，将会输出一个n 元 1维张量。</p>
<p>参数 ：</p>
<ul>
<li>mat (Tensor) – 相乘矩阵</li>
<li>vec (Tensor) – 相乘向量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p>例子:</p>
<pre><code>&gt;&gt;&gt; mat = torch.randn(2, 3)
&gt;&gt;&gt; vec = torch.randn(3)
&gt;&gt;&gt; torch.mv(mat, vec)
-2.0939
-2.2950
[torch.FloatTensor of size 2]
</code></pre><hr>
<pre><code>torch.orgqr
torch.orgqr()
torch.ormqr
torch.ormqr()
torch.potrf
torch.potrf()
torch.potri
torch.potri()
torch.potrs
torch.potrs()
torch.pstrf
torch.pstrf()
</code></pre><hr>
<blockquote>
<p>torch.qr</p>
</blockquote>
<p>torch.qr(input, out=None) -&gt; (Tensor, Tensor)</p>
<p>计算输入矩阵的QR分解：返回两个矩阵q ,r， 使得 x=q∗r ，这里q 是一个半正交矩阵与 r 是一个上三角矩阵</p>
<p>本函数返回一个thin(reduced)QR分解。</p>
<font color="#ff0000" face="黑体">注意 如果输入很大，可能可能会丢失精度。<br></font>

<font color="#ff0000" face="黑体">注意 本函数依赖于你的LAPACK实现，虽然总能返回一个合法的分解，但不同平台可能得到不同的结果。<br></font>

<p>参数：</p>
<ul>
<li>input (Tensor) – 输入的2维张量</li>
<li>out (tuple, optional) – 输出元组tuple，包含Q和R</li>
</ul>
<p>例子:</p>
<pre><code>&gt;&gt;&gt; a = torch.Tensor([[12, -51, 4], [6, 167, -68], [-4, 24, -41]])
&gt;&gt;&gt; q, r = torch.qr(a)
&gt;&gt;&gt; q

-0.8571  0.3943  0.3314
-0.4286 -0.9029 -0.0343
0.2857 -0.1714  0.9429
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; r

-14.0000  -21.0000   14.0000
0.0000 -175.0000   70.0000
0.0000    0.0000  -35.0000
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.mm(q, r).round()

12  -51    4
6  167  -68
-4   24  -41
[torch.FloatTensor of size 3x3]

&gt;&gt;&gt; torch.mm(q.t(), q).round()

1 -0  0
-0  1  0
0  0  1
[torch.FloatTensor of size 3x3]
</code></pre><blockquote>
<p>torch.svd</p>
</blockquote>
<p>torch.svd(input, some=True, out=None) -&gt; (Tensor, Tensor, Tensor)</p>
<p>U,S,V=torch.svd(A)。 返回对形如 n×m的实矩阵 A 进行奇异值分解的结果，使得 A=USV’∗。 U 形状为 n×n，S 形状为 n×m ，V 形状为 m×m</p>
<p>some 代表了需要计算的奇异值数目。如果 some=True, it computes some and some=False computes all.</p>
<p>Irrespective of the original strides, the returned matrix U will be transposed, i.e. with strides (1, n) instead of (n, 1).</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入的2维张量</li>
<li>some (bool, optional) – 布尔值，控制需计算的奇异值数目</li>
<li>out (tuple, optional) – 结果tuple</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.Tensor([[8.79,  6.11, -9.15,  9.57, -3.49,  9.84],
...                   [9.93,  6.91, -7.93,  1.64,  4.02,  0.15],
...                   [9.83,  5.04,  4.86,  8.83,  9.80, -8.99],
...                   [5.45, -0.27,  4.85,  0.74, 10.00, -6.02],
...                   [3.16,  7.98,  3.01,  5.80,  4.27, -5.31]]).t()
&gt;&gt;&gt; a

8.7900   9.9300   9.8300   5.4500   3.1600
6.1100   6.9100   5.0400  -0.2700   7.9800
-9.1500  -7.9300   4.8600   4.8500   3.0100
9.5700   1.6400   8.8300   0.7400   5.8000
-3.4900   4.0200   9.8000  10.0000   4.2700
9.8400   0.1500  -8.9900  -6.0200  -5.3100
[torch.FloatTensor of size 6x5]

&gt;&gt;&gt; u, s, v = torch.svd(a)
&gt;&gt;&gt; u

-0.5911  0.2632  0.3554  0.3143  0.2299
-0.3976  0.2438 -0.2224 -0.7535 -0.3636
-0.0335 -0.6003 -0.4508  0.2334 -0.3055
-0.4297  0.2362 -0.6859  0.3319  0.1649
-0.4697 -0.3509  0.3874  0.1587 -0.5183
0.2934  0.5763 -0.0209  0.3791 -0.6526
[torch.FloatTensor of size 6x5]

&gt;&gt;&gt; s

27.4687
22.6432
8.5584
5.9857
2.0149
[torch.FloatTensor of size 5]

&gt;&gt;&gt; v

-0.2514  0.8148 -0.2606  0.3967 -0.2180
-0.3968  0.3587  0.7008 -0.4507  0.1402
-0.6922 -0.2489 -0.2208  0.2513  0.5891
-0.3662 -0.3686  0.3859  0.4342 -0.6265
-0.4076 -0.0980 -0.4932 -0.6227 -0.4396
[torch.FloatTensor of size 5x5]

&gt;&gt;&gt; torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))
8.934150226306685e-06
</code></pre><blockquote>
<p>torch.symeig</p>
</blockquote>
<p>torch.symeig(input, eigenvectors=False, upper=True, out=None) -&gt; (Tensor, Tensor)<br>e,V=torch.symeig(input) 返回实对称矩阵input的特征值和特征向量。</p>
<p>input 和 V 为 m×m 矩阵，e 是一个m 维向量。 此函数计算intput的所有特征值(和特征向量)，使得 input=Vdiag(e)V′<br>布尔值参数eigenvectors 规定是否只计算特征向量。如果为False，则只计算特征值；若设为True，则两者都会计算。 因为输入矩阵 input 是对称的，所以默认只需要上三角矩阵。如果参数upper为 False，下三角矩阵部分也被利用。</p>
<font color="#ff0000" face="黑体">注意： 返回矩阵总是被转置，无论输入矩阵的原始布局如何，总会被转置；即，总是有 stride (1, m) 而不是 (m, 1).<br></font>

<p>参数：</p>
<ul>
<li>input (Tensor) – 输入对称矩阵</li>
<li>eigenvectors (boolean, optional) – 布尔值（可选），控制是否计算特征向量</li>
<li>upper (boolean, optional) – 布尔值（可选），控制是否考虑上三角或下三角区域</li>
<li>out (tuple, optional) – 输出元组(Tensor, Tensor)</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; a = torch.Tensor([[ 1.96,  0.00,  0.00,  0.00,  0.00],
...                   [-6.49,  3.80,  0.00,  0.00,  0.00],
...                   [-0.47, -6.39,  4.17,  0.00,  0.00],
...                   [-7.20,  1.50, -1.51,  5.70,  0.00],
...                   [-0.65, -6.34,  2.67,  1.80, -7.10]]).t()

&gt;&gt;&gt; e, v = torch.symeig(a, eigenvectors=True)
&gt;&gt;&gt; e

-11.0656
-6.2287
0.8640
8.8655
16.0948
[torch.FloatTensor of size 5]

&gt;&gt;&gt; v

-0.2981 -0.6075  0.4026 -0.3745  0.4896
-0.5078 -0.2880 -0.4066 -0.3572 -0.6053
-0.0816 -0.3843 -0.6600  0.5008  0.3991
-0.0036 -0.4467  0.4553  0.6204 -0.4564
-0.8041  0.4480  0.1725  0.3108  0.1622
[torch.FloatTensor of size 5x5]
</code></pre><p>torch.trtrs<br> torch.trtrs()</p>

      
    </div>

      
  <div>

    <div style="text-align:center;color: #000;font-size:14px;">>>>----------本文结束^_^感谢您的阅读----------<<<</div>

</div>
  
  </div>

    
      <div>


    
    
    


    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/pytorch/" rel="tag"><i class="fa fa-tag"></i> pytorch</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/27/基于facenet的实时人脸检测/" rel="next" title="基于facenet的实时人脸检测">
                <i class="fa fa-chevron-left"></i> 基于facenet的实时人脸检测
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/08/01/vim基本使用方法/" rel="prev" title="vim基本使用方法">
                vim基本使用方法 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.png"
                alt="ZOUZHEN" />
            
              <p class="site-author-name" itemprop="name">ZOUZHEN</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">23</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">10</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">21</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/zouzhen" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:zouzhen94@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#基本概念"><span class="nav-number">1.</span> <span class="nav-text">基本概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NumPy-Bridge"><span class="nav-number">2.</span> <span class="nav-text">NumPy Bridge</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#随机抽样-Random-sampling"><span class="nav-number">2.1.</span> <span class="nav-text">随机抽样 Random sampling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#序列化-Serialization"><span class="nav-number">2.2.</span> <span class="nav-text">序列化 Serialization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#并行化-Parallelism"><span class="nav-number">2.3.</span> <span class="nav-text">并行化 Parallelism</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#数学操作Math-operations"><span class="nav-number">2.4.</span> <span class="nav-text">数学操作Math operations</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Pointwise-Ops"><span class="nav-number">2.4.1.</span> <span class="nav-text">Pointwise Ops</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Reduction-Ops"><span class="nav-number">2.4.2.</span> <span class="nav-text">Reduction Ops</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#比较操作-Comparison-Ops"><span class="nav-number">2.5.</span> <span class="nav-text">比较操作 Comparison Ops</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#其它操作-Other-Operations"><span class="nav-number">2.6.</span> <span class="nav-text">其它操作 Other Operations</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BLAS-and-LAPACK-Operations"><span class="nav-number">2.7.</span> <span class="nav-text">BLAS and LAPACK Operations</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZOUZHEN</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动 v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Mist</a> v6.3.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>














  













  
  
    <script type="text/javascript" src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script>
  

  
  
    <script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.min.js"></script>
  

  
  
    <script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.ui.min.js"></script>
  

  
  
    <script type="text/javascript" src="/vendors/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



	





  





  








  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  
  
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'KYQ5TBa8gXaju4laYBXJ9vyg-gzGzoHsz',
        appKey: 'IU40hn4f9XUsyeACqdVYHnql',
        placeholder: 'ヾﾉ≧∀≦)o 来呀！快活呀！~',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("KYQ5TBa8gXaju4laYBXJ9vyg-gzGzoHsz", "IU40hn4f9XUsyeACqdVYHnql");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            
            counter.save(null, {
              success: function(counter) {
                
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(counter.get('time'));
                
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            
              var newcounter = new Counter();
              /* Set ACL */
              var acl = new AV.ACL();
              acl.setPublicReadAccess(true);
              acl.setPublicWriteAccess(true);
              newcounter.setACL(acl);
              /* End Set ACL */
              newcounter.set("title", title);
              newcounter.set("url", url);
              newcounter.set("time", 1);
              newcounter.save(null, {
                success: function(newcounter) {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
                },
                error: function(newcounter, error) {
                  console.log('Failed to create');
                }
              });
            
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  

  


  
  

  

  

  

  

  


  
   <script type="text/javascript" color="0,0,255" opacity='0.7' zIndex="-2" count="99" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script> 

</body>
</html>
