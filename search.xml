<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习——线性回归算法]]></title>
    <url>%2F2018%2F10%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[线性回归简述在统计学中，线性回归（Linear Regression）是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合（自变量都是一次方）。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。简单来说，就是找到一条直线去拟合数据点。如下图： 优点：结果易于理解，计算上不复杂。缺点：对非线性数据拟合不好。适用数据类型：数值型和标称型数据。算法类型：回归算法 线性回归的模型函数如下： $$h_\theta = \theta^Tx$$ 它的损失函数如下： $$J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2$$ 通过训练数据集寻找参数的最优解，即求解可以得到$minJ(θ)$的参数向量$θ$,其中这里的参数向量也可以分为参数和$w$和$b$,分别表示权重和偏置值。求解最优解的方法有最小二乘法和梯度下降法。 梯度下降法 梯度下降算法的思想如下(这里以一元线性回归为例)： 首先，我们有一个代价函数，假设是$J(θ_0,θ_1)$，我们的目标是$minθ_0,θ_1 J(θ_0,θ_1)$。 接下来的做法是： 首先是随机选择一个参数的组合$(θ_0,θ_1)$,一般是设$θ_0=0,θ_1=0$; 然后是不断改变$(θ_0,θ_1)$，并计算代价函数，直到一个局部最小值。之所以是局部最小值，是因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值，选择不同的初始参数组合，可能会找到不同的局部最小值。下面给出梯度下降算法的公式：repeat until convergence{ $$\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(θ_0,θ_1)(for j =0 and j=1)$$}也就是在梯度下降中，不断重复上述公式直到收敛，也就是找到局部最小值局部最小值。其中符号$:=$是赋值符号的意思。 而应用梯度下降法到线性回归，则公式如下： $$\theta_0 := \theta_0 - \alpha\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2$$$$\theta_1 := \theta_1 - \alpha\frac{1}{2m}\sum_{i=1}^m((h_\theta(x^i)-y^i)\cdot x^i)^2$$ 公式中的$\alpha$称为学习率(learning rate)，它决定了我们沿着能让代价函数下降程度最大的方向向下迈进的步子有多大。在梯度下降中，还涉及都一个参数更新的问题，即更新$(\theta_0,\theta_1)$,一般我们的做法是同步更新.最后，上述梯度下降算法公式实际上是一个叫批量梯度下降(batch gradient descent)，即它在每次梯度下降中都是使用整个训练集的数据，所以公式中是带有$ \sum_{i=1}^m $. 岭回归（ridge regression） 岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价，获得回归系数更为符合实际、更可靠的回归方法，对病态数据的耐受性远远强于最小二乘法。 岭回归分析法是从根本上消除复共线性影响的统计方法。岭回归模型通过在相关矩阵中引入一个很小的岭参数$K（1&gt;K&gt;0）$，并将它加到主对角线元素上，从而降低参数的最小二乘估计中复共线特征向量的影响，减小复共线变量系数最小二乘估计的方法，以保证参数估计更接近真实情况。岭回归分析将所有的变量引入模型中，比逐步回归分析提供更多的信息。 代码实现 使用sklearn包中的线性回归算法 123456789101112131415161718192021222324252627282930313233343536373839404142434445import matplotlib.pyplot as pltimport numpy as npfrom sklearn import datasets, linear_modelfrom sklearn.metrics import mean_squared_error, r2_score# Load the diabetes datasetdiabetes = datasets.load_diabetes()# Use only one featurediabetes_X = diabetes.data[:, np.newaxis, 2]# Split the data into training/testing setsdiabetes_X_train = diabetes_X[:-20]diabetes_X_test = diabetes_X[-20:]# Split the targets into training/testing setsdiabetes_y_train = diabetes.target[:-20]diabetes_y_test = diabetes.target[-20:]# Create linear regression objectregr = linear_model.LinearRegression()# Train the model using the training setsregr.fit(diabetes_X_train, diabetes_y_train)# Make predictions using the testing setdiabetes_y_pred = regr.predict(diabetes_X_test)# The coefficientsprint('Coefficients: \n', regr.coef_)# The mean squared errorprint("Mean squared error: %.2f" % mean_squared_error(diabetes_y_test, diabetes_y_pred))# Explained variance score: 1 is perfect predictionprint('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))# Plot outputsplt.scatter(diabetes_X_test, diabetes_y_test, color='black')plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)plt.xticks(())plt.yticks(())plt.show() 使用代码实现算法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596import osimport numpy as npimport pandas as pdimport matplotlib.pylab as pltfrom sklearn import linear_model# 计算损失函数def computeCost(X, y, theta): inner = np.power(((X * theta.T) - y), 2) return np.sum(inner) / (2 * len(X))# 梯度下降算法def gradientDescent(X, y, theta, alpha, iters): temp = np.matrix(np.zeros(theta.shape)) parameters = int(theta.ravel().shape[1]) cost = np.zeros(iters) for i in range(iters): error = (X * theta.T) - y for j in range(parameters): # 计算误差对权值的偏导数 term = np.multiply(error, X[:, j]) # 更新权值 temp[0, j] = theta[0, j] - ((alpha / len(X)) * np.sum(term)) theta = temp cost[i] = computeCost(X, y, theta) return theta, costdataPath = os.path.join('data', 'ex1data1.txt')data = pd.read_csv(dataPath, header=None, names=['Population', 'Profit'])# print(data.head())# print(data.describe())# data.plot(kind='scatter', x='Population', y='Profit', figsize=(12, 8))# 在数据起始位置添加1列数值为1的数据data.insert(0, 'Ones', 1)print(data.shape)cols = data.shape[1]X = data.iloc[:, 0:cols-1]y = data.iloc[:, cols-1:cols]# 从数据帧转换成numpy的矩阵格式X = np.matrix(X.values)y = np.matrix(y.values)# theta = np.matrix(np.array([0, 0]))theta = np.matrix(np.zeros((1, cols-1)))print(theta)print(X.shape, theta.shape, y.shape)cost = computeCost(X, y, theta)print("cost = ", cost)# 初始化学习率和迭代次数alpha = 0.01iters = 1000# 执行梯度下降算法g, cost = gradientDescent(X, y, theta, alpha, iters)print(g)# 可视化结果x = np.linspace(data.Population.min(),data.Population.max(),100)f = g[0, 0] + (g[0, 1] * x)fig, ax = plt.subplots(figsize=(12, 8))ax.plot(x, f, 'r', label='Prediction')ax.scatter(data.Population, data.Profit, label='Training Data')ax.legend(loc=2)ax.set_xlabel('Population')ax.set_ylabel('Profit')ax.set_title('Predicted Profit vs. Population Size')fig, ax = plt.subplots(figsize=(12, 8))ax.plot(np.arange(iters), cost, 'r')ax.set_xlabel('Iteration')ax.set_ylabel('Cost')ax.set_title('Error vs. Training Epoch')# 使用sklearn 包里面实现的线性回归算法model = linear_model.LinearRegression()model.fit(X, y)x = np.array(X[:, 1].A1)# 预测结果f = model.predict(X).flatten()# 可视化fig, ax = plt.subplots(figsize=(12, 8))ax.plot(x, f, 'r', label='Prediction')ax.scatter(data.Population, data.Profit, label='Training Data')ax.legend(loc=2)ax.set_xlabel('Population')ax.set_ylabel('Profit')ax.set_title('Predicted Profit vs. Population Size(using sklearn)')plt.show()]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>线性回归算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scikit-learn整理]]></title>
    <url>%2F2018%2F10%2F17%2Fscikit-learn%E6%95%B4%E7%90%86%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Flask整理]]></title>
    <url>%2F2018%2F10%2F17%2FFlask%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[使用工具Flask后端 + Postgresql数据库 + JS前端（我未使用）Flask搭建 确定确定目录结构 app/algorithms: 用来存放相关的算法文件 app/models: 用来存放数据库的操作 app/web: 用来存放路由和视图函数 manage: flask的启动文件 确定路由注册方式 使用蓝图形式来注册路由 确定数据库操作方式 使用sqlalchemy及psycopg2来控制Postgresql数据库 由于主要是用来进行数据读取的，所以采用非ORM方式构建的表结构，这种方式方便进行查询过滤操作 基于sqlalchemy的Postgresql数据库访问操作 创建表结构 12345678910111213141516171819from sqlalchemy.engine import create_enginefrom sqlalchemy.schema import MetaData, Table, Column, ForeignKey, Sequencefrom sqlalchemy.types import *from sqlalchemy.sql.expression import select,and_from datetime import datetimeengine = create_engine('postgres://user:password@hosts/builder', echo=True)metadata = MetaData()metadata.bind = engine # 创建桥梁索引表bridges_table = Table('bridges', metadata, Column('id', Integer, primary_key=True), Column('org_id', Integer, nullable=False), Column('user_id', Integer, nullable=False), Column('name', VARCHAR(length=255), nullable=False), Column('created_date', TIMESTAMP, nullable=False), Column('finished_date', TIMESTAMP, nullable=True), # autoload=True, ) 这种方式，有助于进行表查询，具体的相关API介绍及使用放那格式可点此查看。 相关操作 1234567891011121314151617181920212223# 添加数据def add(): s = book_table.insert().values(title='测试写入2',time=datetime.now()) c = engine.execute(s) c.close() return c.inserted_primary_key# 查询数据def query_code(id): info = &#123;'id': '', 'title': ''&#125; s = select([bridge_jobs_table.c.id.label('name')]).where(and_(bridge_jobs_table.c.kind=='桩基',bridge_jobs_table.c.name=='起钻')) codename_query = engine.execute(s) print(codename_query.keys()) for row in codename_query: print(row[0]) codename_query.close() return info# 更新数据def updata(id, title): s = book_table.update().where(book_table.c.id == id).values(title=title, id=id) c = engine.execute(s) c.close() Flask相关知识 路由操作 静态路由 123@web.route('/hello', methods=['POST', 'GET'])def hello(): return 'hello world!' 参数路由 123@web.route('/hello/&lt;string:name&gt;', methods=['POST', 'GET'])def hello(name): return 'hello %d '% name JSON返回 123@web.route('/hello/&lt;string:name&gt;', methods=['POST', 'GET'])def hello(name): return jsonify('hello %d '% name) 使用蓝图方式注册路由 12345678910111213141516from flask import Flaskdef create_app(): app = Flask(__name__) app.config.from_object('app.setting') register_blueprint(app) # db.init_app(app) # db.create_app(app=app) return appdef register_blueprint(app): from app.web.view import web app.register_blueprint(web)]]></content>
      <categories>
        <category>Python后端</category>
      </categories>
      <tags>
        <tag>flask框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自省]]></title>
    <url>%2F2018%2F10%2F06%2F%E8%87%AA%E7%9C%81%2F</url>
    <content type="text"><![CDATA[function doDecrypt (pwd, onError) { console.log('in doDecrypt'); const txt = document.getElementById('enc_content').innerHTML; let plantext; try { const bytes = CryptoJS.AES.decrypt(txt, pwd); var plaintext = bytes.toString(CryptoJS.enc.Utf8); } catch(err) { if(onError) { onError(err); } return; } document.getElementById('enc_content').innerHTML = plaintext; document.getElementById('enc_content').style.display = 'block'; document.getElementById('enc_passwd').style.display = 'none'; if(typeof MathJax !== 'undefined') { MathJax.Hub.Queue( ['resetEquationNumbers', MathJax.InputJax.TeX], ['PreProcess', MathJax.Hub], ['Reprocess', MathJax.Hub] ); } } U2FsdGVkX18uwfGWNf4XT5mPLGEnK+Q30jsK7ElRMP3IFcnVsWYlUpnv8VrvNskj/8W/i++jVYapDlYH+J7BN59UGRc8HINPkYMgtSym1PkpX51xMi0WWvdNZ18abbtsHPj6hWgKXY1G0awoGrQnUNaSGAulnfngUuhHn2t5f05rY8X8PHXxwntWktUmbK79DvhEaYnmrlH6nIbuSwdCxdc3WRNoFyTyy8DRw5sVE0xbWZT4GX5KSak/8y2VZQywt+elLcPSdVUptJ4AwspC56YR+Man8r4LytOzJfk0QKPu/isjVRz5skqO2tMGVFRg88PmPbDeqM63YPY+dCz4DwDBeFr6lBhV7vKdzRq4Lhw= var onError = function(error) { document.getElementById("enc_error").innerHTML = "password error!" }; function decrypt() { var passwd = document.getElementById("enc_pwd_input").value; console.log(passwd); doDecrypt(passwd, onError); }]]></content>
      <categories>
        <category>感悟</category>
      </categories>
      <tags>
        <tag>自我反省</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PostgreSQL学习记录]]></title>
    <url>%2F2018%2F09%2F26%2FPostgreSQL%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[selectfetchall()fetchmany()fetchone()]]></content>
  </entry>
  <entry>
    <title><![CDATA[docker基础]]></title>
    <url>%2F2018%2F09%2F03%2Fdocker%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Python入门整理]]></title>
    <url>%2F2018%2F08%2F21%2FPython%E5%85%A5%E9%97%A8%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[Python基础 python的执行问题 1 python文件的后缀名可以是任意的 2 考虑到文件之间的导入问题，统一以.py文件结尾 基本运算符 1 算数运算符（结果为值） 2 赋值运算符（结果为值） 3 比较运算符（结果为布尔值） 4 逻辑运算符（结果为布尔值） 5 成员运算符（结果为布尔值） 基本数据类型 1 数字 2 字符串 3 列表 4 元组 5 字典 6 布尔值]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow入门整理]]></title>
    <url>%2F2018%2F08%2F05%2Ftensorflow%E5%85%A5%E9%97%A8%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[基本概念 tf.placeholder() 是tensorflow的一种特殊变量，这种变量并非在初始化时定义好内容，而是在训练的时候才将数据填入其中。]]></content>
      <categories>
        <category>框架</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim基本使用方法]]></title>
    <url>%2F2018%2F08%2F01%2Fvim%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[vi的基本概念基本上vi可以分为三种状态，分别是命令模式（command mode）、插入模式（Insert mode）和底行模式（last line mode），各模式的功能区分如下： 命令行模式command mode）控制屏幕光标的移动，字符、字或行的删除，移动复制某区段及进入Insert mode下，或者到 last line mode。 插入模式（Insert mode）只有在Insert mode下，才可以做文字输入，按「ESC」键可回到命令行模式。 底行模式（last line mode）将文件保存或退出vi，也可以设置编辑环境，如寻找字符串、列出行号……等。 不过一般我们在使用时把vi简化成两个模式，就是将底行模式（last line mode）也算入命令行模式command mode）。 vi的基本操作 进入vi在系统提示符号输入vi及文件名称后，就进入vi全屏幕编辑画面：$ vi myfile。不过有一点要特别注意，就是您进入vi之后，是处于「命令行模式（command mode）」，您要切换到「插入模式（Insert mode）」才能够输入文字。初次使用vi的人都会想先用上下左右键移动光标，结果电脑一直哔哔叫，把自己气个半死，所以进入vi后，先不要乱动，转换到「插入模式（Insert mode）」再说吧！ 切换至插入模式（Insert mode）编辑文件在「命令行模式（command mode）」下按一下字母「i」就可以进入「插入模式（Insert mode）」，这时候你就可以开始输入文字了。 Insert 的切换您目前处于「插入模式（Insert mode）」，您就只能一直输入文字，如果您发现输错了字！想用光标键往回移动，将该字删除，就要先按一下「ESC」键转到「命令行模式（command mode）」再删除文字。 退出vi及保存文件在「命令行模式（command mode）」下，按一下「：」冒号键进入「Last line mode」，例如：: w filename （输入 「w filename」将文章以指定的文件名filename保存）: wq (输入「wq」，存盘并退出vi): q! (输入q!， 不存盘强制退出vi) 命令行模式（command mode）功能键 插入模式按「i」切换进入插入模式「insert mode」，按“i”进入插入模式后是从光标当前位置开始输入文件；按「a」进入插入模式后，是从目前光标所在位置的下一个位置开始输入文字；按「o」进入插入模式后，是插入新的一行，从行首开始输入文字。 从插入模式切换为命令行模式按「ESC」键。 移动光标vi可以直接用键盘上的光标来上下左右移动，但正规的vi是用小写英文字母「h」、「j」、「k」、「l」，分别控制光标左、下、上、右移一格。按「ctrl」+「b」：屏幕往“后”移动一页。按「ctrl」+「f」：屏幕往“前”移动一页。按「ctrl」+「u」：屏幕往“后”移动半页。按「ctrl」+「d」：屏幕往“前”移动半页。按数字「0」：移到文章的开头。按「G」：移动到文章的最后。按「$」：移动到光标所在行的“行尾”。按「^」：移动到光标所在行的“行首”按「w」：光标跳到下个字的开头按「e」：光标跳到下个字的字尾按「b」：光标回到上个字的开头按「#l」：光标移到该行的第#个位置，如：5l,56l。 删除文字「x」：每按一次，删除光标所在位置的“后面”一个字符。「#x」：例如，「6x」表示删除光标所在位置的“后面”6个字符。「X」：大写的X，每按一次，删除光标所在位置的“前面”一个字符。「#X」：例如，「20X」表示删除光标所在位置的“前面”20个字符。「dd」：删除光标所在行。「#dd」：从光标所在行开始删除#行 复制「yw」：将光标所在之处到字尾的字符复制到缓冲区中。「#yw」：复制#个字到缓冲区「yy」：复制光标所在行到缓冲区。「#yy」：例如，「6yy」表示拷贝从光标所在的该行“往下数”6行文字。「p」：将缓冲区内的字符贴到光标所在位置。注意：所有与“y”有关的复制命令都必须与“p”配合才能完成复制与粘贴功能。 替换「r」：替换光标所在处的字符。「R」：替换光标所到之处的字符，直到按下「ESC」键为止。 回复上一次操作「u」：如果您误执行一个命令，可以马上按下「u」，回到上一个操作。按多次“u”可以执行多次回复。 更改「cw」：更改光标所在处的字到字尾处「c#w」：例如，「c3w」表示更改3个字 跳至指定的行「ctrl」+「g」列出光标所在行的行号。「#G」：例如，「15G」，表示移动光标至文章的第15行行首。 Last line mode下命令简介 在使用「last line mode」之前，请记住先按「ESC」键确定您已经处于「command mode」下后，再按「：」冒号即可进入「last line mode」。 列出行号「set nu」：输入「set nu」后，会在文件中的每一行前面列出行号。 跳到文件中的某一行「#」：「#」号表示一个数字，在冒号后输入一个数字，再按回车键就会跳到该行了，如输入数字15，再回车，就会跳到文章的第15行。 查找字符「/关键字」：先按「/」键，再输入您想寻找的字符，如果第一次找的关键字不是您想要的，可以一直按「n」会往后寻找到您要的关键字为止。「?关键字」：先按「?」键，再输入您想寻找的字符，如果第一次找的关键字不是您想要的，可以一直按「n」会往前寻找到您要的关键字为止。 保存文件「w」：在冒号输入字母「w」就可以将文件保存起来。 离开vi「q」：按「q」就是退出，如果无法离开vi，可以在「q」后跟一个「!」强制离开vi。「qw」：一般建议离开时，搭配「w」一起使用，这样在退出的时候还可以保存文件。 vi命令列表 下表列出命令模式下的一些键的功能： h左移光标一个字符 l右移光标一个字符 k光标上移一行 j光标下移一行 ^光标移动至行首 0数字“0”，光标移至文章的开头 G光标移至文章的最后 $光标移动至行尾 Ctrl+f向前翻屏 Ctrl+b向后翻屏 Ctrl+d向前翻半屏 Ctrl+u向后翻半屏 i在光标位置前插入字符 a在光标所在位置的后一个字符开始增加 o插入新的一行，从行首开始输入 ESC从输入状态退至命令状态 x删除光标后面的字符 #x删除光标后的＃个字符 X(大写X)，删除光标前面的字符 #X删除光标前面的#个字符 dd删除光标所在的行 #dd删除从光标所在行数的#行 yw复制光标所在位置的一个字 #yw复制光标所在位置的#个字 yy复制光标所在位置的一行 #yy复制从光标所在行数的#行 p粘贴 u取消操作 cw更改光标所在位置的一个字 #cw更改光标所在位置的#个字 下表列出行命令模式下的一些指令 w filename储存正在编辑的文件为filename wq filename储存正在编辑的文件为filename，并退出vi q!放弃所有修改，退出vi set nu显示行号 /或?查找，在/后输入要查找的内容 n与/或?一起使用，如果查找的内容不是想要找的关键字，按n或向后（与/联用）或向前（与?联用）继续查找，直到找到为止。 高手总结的图：]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch入门]]></title>
    <url>%2F2018%2F08%2F01%2FPyTorch%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[基本概念 张量 张量类似于NumPy的ndarray，另外还有Tensors也可用于GPU以加速计算。 from __future__ import print_function import torch 构造一个未初始化的5x3矩阵： x = torch.empty(5, 3) print(x) tensor([[ 3.2401e+18, 0.0000e+00, 1.3474e-08], [ 4.5586e-41, 1.3476e-08, 4.5586e-41], [ 1.3476e-08, 4.5586e-41, 1.3474e-08], [ 4.5586e-41, 1.3475e-08, 4.5586e-41], [ 1.3476e-08, 4.5586e-41, 1.3476e-08]]) 构造一个矩阵填充的零和dtype long： x = torch.zeros(5, 3, dtype=torch.long) print(x) tensor([[ 0, 0, 0], [ 0, 0, 0], [ 0, 0, 0], [ 0, 0, 0], [ 0, 0, 0]]) 直接从数据构造张量： x = torch.tensor([5.5, 3]) print(x) tensor([ 5.5000, 3.0000]) 或者根据现有的张量创建张量。除非用户提供新值，否则这些方法将重用输入张量的属性，例如dtype x = x.new_ones(5, 3, dtype=torch.double) # new_* methods take in sizes print(x) x = torch.randn_like(x, dtype=torch.float) # override dtype! print(x) # result has the same size tensor([[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.]], dtype=torch.float64) tensor([[ 0.2641, 0.0149, 0.7355], [ 0.6106, -1.2480, 1.0592], [ 2.6305, 0.5582, 0.3042], [-1.4410, 2.4951, -0.0818], [ 0.8605, 0.0001, -0.7220]]) 得到它的大小： print(x.size()) torch.Size([5, 3]) 注意 torch.Size 实际上是一个元组，因此它支持所有元组操作。 操作 操作有多种语法。在下面的示例中，我们将查看添加操作。 增加：语法1 y = torch.rand(5, 3) print(x + y) tensor([[ 0.7355, 0.2798, 0.9392], [ 1.0300, -0.6085, 1.7991], [ 2.8120, 1.2438, 1.2999], [-1.0534, 2.8053, 0.0163], [ 1.4088, 0.9000, -0.1172]]) 增加：语法2 print(torch.add(x, y)) tensor([[ 0.7355, 0.2798, 0.9392], [ 1.0300, -0.6085, 1.7991], [ 2.8120, 1.2438, 1.2999], [-1.0534, 2.8053, 0.0163], [ 1.4088, 0.9000, -0.1172]]) 增加：提供输出张量作为参数 result = torch.empty(5, 3) torch.add(x, y, out=result) print(result) tensor([[ 0.7355, 0.2798, 0.9392], [ 1.0300, -0.6085, 1.7991], [ 2.8120, 1.2438, 1.2999], [-1.0534, 2.8053, 0.0163], [ 1.4088, 0.9000, -0.1172]]) 增加：就地 # adds x to y y.add_(x) print(y) tensor([[ 0.7355, 0.2798, 0.9392], [ 1.0300, -0.6085, 1.7991], [ 2.8120, 1.2438, 1.2999], [-1.0534, 2.8053, 0.0163], [ 1.4088, 0.9000, -0.1172]]) 注意 任何使原位张量变形的操作都是用。后固定的。例如：x.copy(y)，x.t_()，将改变x。 你可以使用标准的NumPy索引与所有的铃声和​​口哨！ print(x[:, 1]) tensor([ 0.0149, -1.2480, 0.5582, 2.4951, 0.0001]) 调整大小：如果要调整大小/重塑张量，可以使用torch.view： x = torch.randn(4, 4) y = x.view(16) z = x.view(-1, 8) # the size -1 is inferred from other dimensions print(x.size(), y.size(), z.size()) torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) 如果你有一个元素张量，用于.item()获取值作为Python数字 x = torch.randn(1) print(x) print(x.item()) tensor([ 1.3159]) 1.3159412145614624 NumPy Bridge将Torch Tensor转换为NumPy阵列（反之亦然）是一件轻而易举的事。Torch Tensor和NumPy阵列将共享其底层内存位置，更改一个将改变另一个。 将Torch Tensor转换为NumPy数组 a = torch.ones(5) print(a) tensor([ 1., 1., 1., 1., 1.]) b = a.numpy() print(b) [1. 1. 1. 1. 1.] 了解numpy数组的值如何变化。 a.add_(1) print(a) print(b) tensor([ 2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.] 将NumPy数组转换为Torch Tensor 了解更改np阵列如何自动更改Torch Tensor import numpy as np a = np.ones(5) b = torch.from_numpy(a) np.add(a, 1, out=a) print(a) print(b) [2. 2. 2. 2. 2.] tensor([ 2., 2., 2., 2., 2.], dtype=torch.float64) 除了CharTensor之外，CPU上的所有Tensors都支持转换为NumPy并返回。 CUDA Tensors 可以使用该.to方法将张量移动到任何设备上。 # let us run this cell only if CUDA is available # We will use ``torch.device`` objects to move tensors in and out of GPU if torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) # a CUDA device object y = torch.ones_like(x, device=device) # directly create a tensor on GPU x = x.to(device) # or just use strings ``.to(&quot;cuda&quot;)`` z = x + y print(z) print(z.to(&quot;cpu&quot;, torch.double)) # ``.to`` can also change dtype together! tensor([ 2.3159], device=&apos;cuda:0&apos;) tensor([ 2.3159], dtype=torch.float64) torch torch.eye torch.eye(n, m=None, out=None)返回一个2维张量，对角线位置全1，其它位置全0 参数: n (int ) – 行数 m (int, optional) – 列数.如果为None,则默认为n out (Tensor, optinal) - Output tensor 返回值: 对角线位置全1，其它位置全0的2维张量 返回值类型: Tensor 例子: &gt;&gt;&gt; torch.eye(3) 1 0 0 0 1 0 0 0 1 [torch.FloatTensor of size 3x3] from_numpy torch.from_numpy(ndarray) → TensorNumpy桥，将numpy.ndarray 转换为pytorch的 Tensor。 返回的张量tensor和numpy的ndarray共享同一内存空间。修改一个会导致另外一个也被修改。返回的张量不能改变大小。 例子: &gt;&gt;&gt; a = numpy.array([1, 2, 3]) &gt;&gt;&gt; t = torch.from_numpy(a) &gt;&gt;&gt; t torch.LongTensor([1, 2, 3]) &gt;&gt;&gt; t[0] = -1 &gt;&gt;&gt; a array([-1, 2, 3]) torch.linspace torch.linspace(start, end, steps=100, out=None) → Tensor返回一个1维张量，包含在区间start 和 end 上均匀间隔的steps个点。 输出1维张量的长度为steps。 参数: start (float) – 序列的起始点 end (float) – 序列的最终值 steps (int) – 在start 和 end间生成的样本数 out (Tensor, optional) – 结果张量 例子: &gt;&gt;&gt; torch.linspace(3, 10, steps=5) 3.0000 4.7500 6.5000 8.2500 10.0000 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.linspace(-10, 10, steps=5) -10 -5 0 5 10 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5) -10 -5 0 5 10 [torch.FloatTensor of size 5] torch.logspace torch.logspace(start, end, steps=100, out=None) → Tensor返回一个1维张量，包含在区间 10start 和 10end上以对数刻度均匀间隔的steps个点。 输出1维张量的长度为steps。 参数: start (float) – 序列的起始点 end (float) – 序列的最终值 steps (int) – 在start 和 end间生成的样本数 out (Tensor, optional) – 结果张量 例子: &gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5) 1.0000e-10 1.0000e-05 1.0000e+00 1.0000e+05 1.0000e+10 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5) 1.2589 2.1135 3.5481 5.9566 10.0000 [torch.FloatTensor of size 5] torch.ones torch.ones(*sizes, out=None) → Tensor返回一个全为1 的张量，形状由可变参数sizes定义。 参数: sizes (int…) – 整数序列，定义了输出形状 out (Tensor, optional) – 结果张量 例子: &gt;&gt;&gt; torch.ones(2, 3) 1 1 1 1 1 1 [torch.FloatTensor of size 2x3] &gt;&gt;&gt; torch.ones(5) 1 1 1 1 1 [torch.FloatTensor of size 5] torch.rand torch.rand(*sizes, out=None) → Tensor返回一个张量，包含了从区间[0,1)的均匀分布中抽取的一组随机数，形状由可变参数sizes 定义。 参数: sizes (int…) – 整数序列，定义了输出形状 out (Tensor, optinal) - 结果张量 例子： &gt;&gt;&gt; torch.rand(4) 0.9193 0.3347 0.3232 0.7715 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.rand(2, 3) 0.5010 0.5140 0.0719 0.1435 0.5636 0.0538 [torch.FloatTensor of size 2x3] torch.randn torch.randn(*sizes, out=None) → Tensor返回一个张量，包含了从标准正态分布(均值为0，方差为 1，即高斯白噪声)中抽取一组随机数，形状由可变参数sizes定义。 参数: sizes (int…) – 整数序列，定义了输出形状 out (Tensor, optinal) - 结果张量 例子： &gt;&gt;&gt; torch.randn(4) -0.1145 0.0094 -1.1717 0.9846 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.randn(2, 3) 1.4339 0.3351 -1.0999 1.5458 -0.9643 -0.3558 [torch.FloatTensor of size 2x3] torch.randperm torch.randperm(n, out=None) → LongTensor给定参数n，返回一个从0 到n -1 的随机整数排列。 参数: n (int) – 上边界(不包含) 例子： &gt;&gt;&gt; torch.randperm(4) 2 1 3 0 [torch.LongTensor of size 4] torch.arange torch.arange(start, end, step=1, out=None) → Tensor返回一个1维张量，长度为 floor((end−start)/step)。包含从start到end，以step为步长的一组序列值(默认步长为1)。 参数: start (float) – 序列的起始点 end (float) – 序列的终止点 step (float) – 相邻点的间隔大小 out (Tensor, optional) – 结果张量例子： &gt;&gt;&gt; torch.arange(1, 4) 1 2 3 [torch.FloatTensor of size 3] &gt;&gt;&gt; torch.arange(1, 2.5, 0.5) 1.0000 1.5000 2.0000 [torch.FloatTensor of size 3] torch.range torch.range(start, end, step=1, out=None) → Tensor返回一个1维张量，有 floor((end−start)/step)+1 个元素。包含在半开区间[start, end）从start开始，以step为步长的一组值。 step 是两个值之间的间隔，即 xi+1=xi+step 警告：建议使用函数 torch.arange()参数: start (float) – 序列的起始点 end (float) – 序列的最终值 step (int) – 相邻点的间隔大小out (Tensor, optional) – 结果张量例子： &gt;&gt;&gt; torch.range(1, 4) 1 2 3 4 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.range(1, 4, 0.5) 1.0000 1.5000 2.0000 2.5000 3.0000 3.5000 4.0000 [torch.FloatTensor of size 7]&gt; torch.zerostorch.zeros(sizes, out=None) → Tensor返回一个全为标量 0 的张量，形状由可变参数sizes 定义。参数: sizes (int…) – 整数序列，定义了输出形状( out (Tensor, optional) – 结果张量例子： &gt;&gt;&gt; torch.zeros(2, 3) 0 0 0 0 0 0 [torch.FloatTensor of size 2x3] &gt;&gt;&gt; torch.zeros(5) 0 0 0 0 0 [torch.FloatTensor of size 5]#### 索引,切片,连接,换位Indexing, Slicing, Joining, Mutating Ops—&gt; torch.cattorch.cat(inputs, dimension=0) → Tensor在给定维度上对输入的张量序列seq 进行连接操作。torch.cat()可以看做 torch.split() 和 torch.chunk()的反操作。 cat() 函数可以通过下面例子更好的理解。参数: inputs (sequence of Tensors) – 可以是任意相同Tensor 类型的python 序列 dimension (int, optional) – 沿着此维连接张量序列。例子： &gt;&gt;&gt; x = torch.randn(2, 3) &gt;&gt;&gt; x 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 2x3] &gt;&gt;&gt; torch.cat((x, x, x), 0) 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 6x3] &gt;&gt;&gt; torch.cat((x, x, x), 1) 0.5983 -0.0341 2.4918 0.5983 -0.0341 2.4918 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 1.5981 -0.5265 -0.8735 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 2x9]&gt;torch.chunktorch.chunk(tensor, chunks, dim=0)在给定维度(轴)上将输入张量进行分块儿。参数: tensor (Tensor) – 待分块的输入张量 chunks (int) – 分块的个数 dim (int) – 沿着此维度进行分块&gt; torch.gathertorch.gather(input, dim, index, out=None) → Tensor沿给定轴dim，将输入索引张量index指定位置的值进行聚合。对一个3维张量，输出可以定义为： out[i][j][k] = tensor[index[i][j][k]][j][k] # dim=0 out[i][j][k] = tensor[i][index[i][j][k]][k] # dim=1 out[i][j][k] = tensor[i][j][index[i][j][k]] # dim=3例子： &gt;&gt;&gt; t = torch.Tensor([[1,2],[3,4]]) &gt;&gt;&gt; torch.gather(t, 1, torch.LongTensor([[0,0],[1,0]])) 1 1 4 3 [torch.FloatTensor of size 2x2]参数: input (Tensor) – 源张量 dim (int) – 索引的轴 index (LongTensor) – 聚合元素的下标 out (Tensor, optional) – 目标张量&gt; torch.index_selecttorch.index_select(input, dim, index, out=None) → Tensor沿着指定维度对输入进行切片，取index中指定的相应项(index为一个LongTensor)，然后返回到一个新的张量， 返回的张量与原始张量Tensor有相同的维度(在指定轴上)。注意： 返回的张量不与原始张量共享内存空间。 参数: input (Tensor) – 输入张量 dim (int) – 索引的轴 index (LongTensor) – 包含索引下标的一维张量 out (Tensor, optional) – 目标张量 例子： &gt;&gt;&gt; x = torch.randn(3, 4) &gt;&gt;&gt; x 1.2045 2.4084 0.4001 1.1372 0.5596 1.5677 0.6219 -0.7954 1.3635 -1.2313 -0.5414 -1.8478 [torch.FloatTensor of size 3x4] &gt;&gt;&gt; indices = torch.LongTensor([0, 2]) &gt;&gt;&gt; torch.index_select(x, 0, indices) 1.2045 2.4084 0.4001 1.1372 1.3635 -1.2313 -0.5414 -1.8478 [torch.FloatTensor of size 2x4] &gt;&gt;&gt; torch.index_select(x, 1, indices) 1.2045 0.4001 0.5596 0.6219 1.3635 -0.5414 [torch.FloatTensor of size 3x2] torch.masked_select torch.masked_select(input, mask, out=None) → Tensor根据掩码张量mask中的二元值，取输入张量中的指定项( mask为一个 ByteTensor)，将取值返回到一个新的1D张量， 张量 mask须跟input张量有相同数量的元素数目，但形状或维度不需要相同。 注意： 返回的张量不与原始张量共享内存空间。 参数: input (Tensor) – 输入张量 mask (ByteTensor) – 掩码张量，包含了二元索引值 out (Tensor, optional) – 目标张量 例子： &gt;&gt;&gt; x = torch.randn(3, 4) &gt;&gt;&gt; x 1.2045 2.4084 0.4001 1.1372 0.5596 1.5677 0.6219 -0.7954 1.3635 -1.2313 -0.5414 -1.8478 [torch.FloatTensor of size 3x4] &gt;&gt;&gt; indices = torch.LongTensor([0, 2]) &gt;&gt;&gt; torch.index_select(x, 0, indices) 1.2045 2.4084 0.4001 1.1372 1.3635 -1.2313 -0.5414 -1.8478 [torch.FloatTensor of size 2x4] &gt;&gt;&gt; torch.index_select(x, 1, indices) 1.2045 0.4001 0.5596 0.6219 1.3635 -0.5414 [torch.FloatTensor of size 3x2] torch.nonzero torch.nonzero(input, out=None) → LongTensor返回一个包含输入input中非零元素索引的张量。输出张量中的每行包含输入中非零元素的索引。 如果输入input有n维，则输出的索引张量output的形状为 z x n, 这里 z 是输入张量input中所有非零元素的个数。 参数: input (Tensor) – 源张量 out (LongTensor, optional) – 包含索引值的结果张量 例子： &gt;&gt;&gt; torch.nonzero(torch.Tensor([1, 1, 1, 0, 1])) 0 1 2 4 [torch.LongTensor of size 4x1] &gt;&gt;&gt; torch.nonzero(torch.Tensor([[0.6, 0.0, 0.0, 0.0], ... [0.0, 0.4, 0.0, 0.0], ... [0.0, 0.0, 1.2, 0.0], ... [0.0, 0.0, 0.0,-0.4]])) 0 0 1 1 2 2 3 3 [torch.LongTensor of size 4x2] torch.split torch.split(tensor, split_size, dim=0)将输入张量分割成相等形状的chunks（如果可分）。 如果沿指定维的张量形状大小不能被split_size 整分， 则最后一个分块会小于其它分块。 参数: tensor (Tensor) – 待分割张量 split_size (int) – 单个分块的形状大小 dim (int) – 沿着此维进行分割 torch.squeeze torch.squeeze(input, dim=None, out=None)将输入张量形状中的1 去除并返回。 如果输入是形如(A×1×B×1×C×1×D)，那么输出形状就为： (A×B×C×D)当给定dim时，那么挤压操作只在给定维度上。例如，输入形状为: (A×1×B), squeeze(input, 0) 将会保持张量不变，只有用 squeeze(input, 1)，形状会变成 (A×B)。 注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。 参数: input (Tensor) – 输入张量 dim (int, optional) – 如果给定，则input只会在给定维度挤压 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; x = torch.zeros(2,1,2,1,2) &gt;&gt;&gt; x.size() (2L, 1L, 2L, 1L, 2L) &gt;&gt;&gt; y = torch.squeeze(x) &gt;&gt;&gt; y.size() (2L, 2L, 2L) &gt;&gt;&gt; y = torch.squeeze(x, 0) &gt;&gt;&gt; y.size() (2L, 1L, 2L, 1L, 2L) &gt;&gt;&gt; y = torch.squeeze(x, 1) &gt;&gt;&gt; y.size() (2L, 2L, 1L, 2L) torch.stack[source] torch.stack(sequence, dim=0)沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。 参数: sqequence (Sequence) – 待连接的张量序列 dim (int) – 插入的维度。必须介于 0 与 待连接的张量序列数之间。 torch.t torch.t(input, out=None) → Tensor输入一个矩阵（2维张量），并转置0, 1维。 可以被视为函数transpose(input, 0, 1)的简写函数。 参数: input (Tensor) – 输入张量 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; x = torch.randn(2, 3) &gt;&gt;&gt; x 0.4834 0.6907 1.3417 -0.1300 0.5295 0.2321 [torch.FloatTensor of size 2x3] &gt;&gt;&gt; torch.t(x) 0.4834 -0.1300 0.6907 0.5295 1.3417 0.2321 [torch.FloatTensor of size 3x2] torch.transpose torch.transpose(input, dim0, dim1, out=None) → Tensor返回输入矩阵input的转置。交换维度dim0和dim1。 输出张量与输入张量共享内存，所以改变其中一个会导致另外一个也被修改。 参数: input (Tensor) – 输入张量 dim0 (int) – 转置的第一维 dim1 (int) – 转置的第二维 &gt;&gt;&gt; x = torch.randn(2, 3) &gt;&gt;&gt; x 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 2x3] &gt;&gt;&gt; torch.transpose(x, 0, 1) 0.5983 1.5981 -0.0341 -0.5265 2.4918 -0.8735 [torch.FloatTensor of size 3x2] torch.unbind torch.unbind(tensor, dim=0)[source]移除指定维后，返回一个元组，包含了沿着指定维切片后的各个切片 参数: tensor (Tensor) – 输入张量 dim (int) – 删除的维度 torch.unsqueeze torch.unsqueeze(input, dim, out=None)返回一个新的张量，对输入的制定位置插入维度 1 注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。 如果dim为负，则将会被转化dim+input.dim()+1 参数: tensor (Tensor) – 输入张量 dim (int) – 插入维度的索引 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; x = torch.Tensor([1, 2, 3, 4]) &gt;&gt;&gt; torch.unsqueeze(x, 0) 1 2 3 4 [torch.FloatTensor of size 1x4] &gt;&gt;&gt; torch.unsqueeze(x, 1) 1 2 3 4 [torch.FloatTensor of size 4x1] 随机抽样 Random sampling torch.manual_seed torch.manual_seed(seed)设定生成随机数的种子，并返回一个 torch._C.Generator 对象. 参数: seed (int or long) – 种子. torch.initial_seed torch.initial_seed()返回生成随机数的原始种子值（python long）。 torch.get_rng_state torch.get_rng_state()[source]返回随机生成器状态(ByteTensor) torch.set_rng_state torch.set_rng_state(new_state)[source]设定随机生成器状态 参数: new_state (torch.ByteTensor) – 期望的状态 torch.default_generator torch.default_generator = &lt;torch._C.Generator object&gt; torch.bernoulli torch.bernoulli(input, out=None) → Tensor从伯努利分布中抽取二元随机数(0 或者 1)。 输入张量须包含用于抽取上述二元随机值的概率。 因此，输入中的所有值都必须在［0,1］区间，即 0&lt;=inputi&lt;=1输出张量的第i个元素值， 将会以输入张量的第i个概率值等于1。 返回值将会是与输入相同大小的张量，每个值为0或者1 参数: input (Tensor) – 输入为伯努利分布的概率值 out (Tensor, optional) – 输出张量(可选) 例子： &gt;&gt;&gt; a = torch.Tensor(3, 3).uniform_(0, 1) # generate a uniform random matrix with range [0, 1] &gt;&gt;&gt; a 0.7544 0.8140 0.9842 0.5282 0.0595 0.6445 0.1925 0.9553 0.9732 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.bernoulli(a) 1 1 1 0 0 1 0 1 1 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing &quot;1&quot; is 1 &gt;&gt;&gt; torch.bernoulli(a) 1 1 1 1 1 1 1 1 1 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing &quot;1&quot; is 0 &gt;&gt;&gt; torch.bernoulli(a) 0 0 0 0 0 0 0 0 0 [torch.FloatTensor of size 3x3] torch.multinomial torch.multinomial(input, num_samples,replacement=False, out=None) → LongTensor返回一个张量，每行包含从input相应行中定义的多项分布中抽取的num_samples个样本。 [注意]:输入input每行的值不需要总和为1 (这里我们用来做权重)，但是必须非负且总和不能为0。 当抽取样本时，依次从左到右排列(第一个样本对应第一列)。 如果输入input是一个向量，输出out也是一个相同长度num_samples的向量。如果输入input是有 m行的矩阵，输出out是形如m×n的矩阵。 如果参数replacement 为 True, 则样本抽取可以重复。否则，一个样本在每行不能被重复抽取。 参数num_samples必须小于input长度(即，input的列数，如果是input是一个矩阵)。 参数: input (Tensor) – 包含概率值的张量 num_samples (int) – 抽取的样本数 replacement (bool, optional) – 布尔值，决定是否能重复抽取 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; weights = torch.Tensor([0, 10, 3, 0]) # create a Tensor of weights &gt;&gt;&gt; torch.multinomial(weights, 4) 1 2 0 0 [torch.LongTensor of size 4] &gt;&gt;&gt; torch.multinomial(weights, 4, replacement=True) 1 2 1 2 [torch.LongTensor of size 4] torch.normal() torch.normal(means, std, out=None)返回一个张量，包含从给定参数means,std的离散正态分布中抽取随机数。 均值means是一个张量，包含每个输出元素相关的正态分布的均值。 std是一个张量，包含每个输出元素相关的正态分布的标准差。 均值和标准差的形状不须匹配，但每个张量的元素个数须相同。 参数: means (Tensor) – 均值 std (Tensor) – 标准差 out (Tensor) – 可选的输出张量 例子： &gt;&gt;&gt; torch.normal(means=torch.arange(1, 11), std=torch.arange(1, 0, -0.1)) 1.5104 1.6955 2.4895 4.9185 4.9895 6.9155 7.3683 8.1836 8.7164 9.8916 [torch.FloatTensor of size 10] torch.normal(mean=0.0, std, out=None) 与上面函数类似，所有抽取的样本共享均值。 参数: means (Tensor,optional) – 所有分布均值 std (Tensor) – 每个元素的标准差 out (Tensor) – 可选的输出张量 例子: &gt;&gt;&gt; torch.normal(mean=0.5, std=torch.arange(1, 6)) 0.5723 0.0871 -0.3783 -2.5689 10.7893 [torch.FloatTensor of size 5] torch.normal(means, std=1.0, out=None)与上面函数类似，所有抽取的样本共享标准差。 参数: means (Tensor) – 每个元素的均值 std (float, optional) – 所有分布的标准差 out (Tensor) – 可选的输出张量 例子: &gt;&gt;&gt; torch.normal(means=torch.arange(1, 6)) 1.1681 2.8884 3.7718 2.5616 4.2500 [torch.FloatTensor of size 5] 序列化 Serialization torch.saves[source] torch.save(obj, f, pickle_module=, pickle_protocol=2)保存一个对象到一个硬盘文件上 参考: Recommended approach for saving a model 参数： obj – 保存对象 f － 类文件对象 (返回文件描述符)或一个保存文件名的字符串 pickle_module – 用于pickling元数据和对象的模块 pickle_protocol – 指定pickle protocal 可以覆盖默认参数 torch.load[source] torch.load(f, map_location=None, pickle_module=)从磁盘文件中读取一个通过torch.save()保存的对象。torch.load() 可通过参数map_location 动态地进行内存重映射，使其能从不动设备中读取文件。一般调用时，需两个参数: storage 和 location tag. 返回不同地址中的storage，或着返回None (此时地址可以通过默认方法进行解析). 如果这个参数是字典的话，意味着其是从文件的地址标记到当前系统的地址标记的映射。 默认情况下， location tags中 “cpu”对应host tensors，‘cuda:device_id’ (e.g. ‘cuda:2’) 对应cuda tensors。 用户可以通过register_package进行扩展，使用自己定义的标记和反序列化方法。 参数: f – 类文件对象 (返回文件描述符)或一个保存文件名的字符串 map_location – 一个函数或字典规定如何remap存储位置 pickle_module – 用于unpickling元数据和对象的模块 (必须匹配序列化文件时的pickle_module ) 例子: &gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;) # Load all tensors onto the CPU &gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location=lambda storage, loc: storage) # Map tensors from GPU 1 to GPU 0 &gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location={&apos;cuda:1&apos;:&apos;cuda:0&apos;}) 并行化 Parallelism torch.get_num_threads torch.get_num_threads() → int获得用于并行化CPU操作的OpenMP线程数 torch.set_num_threads torch.set_num_threads(int)设定用于并行化CPU操作的OpenMP线程数 数学操作Math operations Pointwise Ops torch.abs torch.abs(input, out=None) → Tensor计算输入张量的每个元素绝对值 例子： &gt;&gt;&gt; torch.abs(torch.FloatTensor([-1, -2, 3])) FloatTensor([1, 2, 3]) torch.acos(input, out=None) → Tensortorch.acos(input, out=None) → Tensor 返回一个新张量，包含输入张量每个元素的反余弦。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.acos(a) 2.2608 1.2956 1.1075 nan [torch.FloatTensor of size 4] torch.add() torch.add(input, value, out=None)对输入张量input逐元素加上标量值value，并返回结果到一个新的张量out，即 out=tensor+value。 如果输入input是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】 input (Tensor) – 输入张量 value (Number) – 添加到输入每个元素的数 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 0.4050 -1.2227 1.8688 -0.4185 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.add(a, 20) 20.4050 18.7773 21.8688 19.5815 [torch.FloatTensor of size 4] torch.add(input, value=1, other, out=None) other 张量的每个元素乘以一个标量值value，并加到iput 张量上。返回结果到输出张量out。即，out=input+(other∗value) 两个张量 input and other的尺寸不需要匹配，但元素总数必须一样。 注意 :当两个张量形状不匹配时，输入张量的形状会作为输出张量的尺寸。 如果other是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】 参数: input (Tensor) – 第一个输入张量 value (Number) – 用于第二个张量的尺寸因子 other (Tensor) – 第二个输入张量 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; import torch &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.9310 2.0330 0.0852 -0.2941 [torch.FloatTensor of size 4] &gt;&gt;&gt; b = torch.randn(2, 2) &gt;&gt;&gt; b 1.0663 0.2544 -0.1513 0.0749 [torch.FloatTensor of size 2x2] &gt;&gt;&gt; torch.add(a, 10, b) 9.7322 4.5770 -1.4279 0.4552 [torch.FloatTensor of size 4] torch.addcdiv torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None) → Tensor用tensor2对tensor1逐元素相除，然后乘以标量值value 并加到tensor。 张量的形状不需要匹配，但元素数量必须一致。 如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。 参数： tensor (Tensor) – 张量，对 tensor1 ./ tensor 进行相加 value (Number, optional) – 标量，对 tensor1 ./ tensor2 进行相乘 tensor1 (Tensor) – 张量，作为被除数(分子) tensor2 (Tensor) –张量，作为除数(分母) out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; t = torch.randn(2, 3) &gt;&gt;&gt; t1 = torch.randn(1, 6) &gt;&gt;&gt; t2 = torch.randn(6, 1) &gt;&gt;&gt; torch.addcdiv(t, 0.1, t1, t2) 0.0122 -0.0188 -0.2354 0.7396 -1.5721 1.2878 [torch.FloatTensor of size 2x3] torch.addcmul torch.addcmul(tensor, value=1, tensor1, tensor2, out=None) → Tensor用tensor2对tensor1逐元素相乘，并对结果乘以标量值value然后加到tensor。 张量的形状不需要匹配，但元素数量必须一致。 如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。 参数： tensor (Tensor) – 张量，对tensor1 ./ tensor 进行相加 value (Number, optional) – 标量，对 tensor1 . tensor2 进行相乘 tensor1 (Tensor) – 张量，作为乘子1 tensor2 (Tensor) –张量，作为乘子2 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; t = torch.randn(2, 3) &gt;&gt;&gt; t1 = torch.randn(1, 6) &gt;&gt;&gt; t2 = torch.randn(6, 1) &gt;&gt;&gt; torch.addcmul(t, 0.1, t1, t2) 0.0122 -0.0188 -0.2354 0.7396 -1.5721 1.2878 [torch.FloatTensor of size 2x3] torch.asin torch.asin(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的反正弦函数 参数： tensor (Tensor) – 输入张量 nout (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.asin(a) -0.6900 0.2752 0.4633 nan [torch.FloatTensor of size 4] torch.atan torch.atan(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的反正切函数 参数： tensor (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.atan(a) -0.5669 0.2653 0.4203 0.9196 [torch.FloatTensor of size 4] torch.atan2 torch.atan2(input1, input2, out=None) → Tensor返回一个新张量，包含两个输入张量input1和input2的反正切函数 参数： input1 (Tensor) – 第一个输入张量 input2 (Tensor) – 第二个输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.atan2(a, torch.randn(4)) -2.4167 2.9755 0.9363 1.6613 [torch.FloatTensor of size 4] torch.ceil torch.ceil(input, out=None) → Tensor天井函数，对输入input张量每个元素向上取整, 即取不小于每个元素的最小整数，并返回结果到输出。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.ceil(a) 2 1 -0 -0 [torch.FloatTensor of size 4] torch.clamp torch.clamp(input, min, max, out=None) → Tensor将输入input张量每个元素的夹紧到区间 $[min,max]$，并返回结果到一个新张量。 操作定义如下： | min, if x_i &lt; min y_i = | x_i, if min &lt;= x_i &lt;= max | max, if x_i &gt; max 如果输入是FloatTensor or DoubleTensor类型，则参数min max 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，min， max取整数、实数皆可。】 参数： input (Tensor) – 输入张量 min (Number) – 限制范围下限 max (Number) – 限制范围上限 Nout (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.clamp(a, min=-0.5, max=0.5) 0.5000 0.3912 -0.5000 -0.5000 [torch.FloatTensor of size 4] torch.clamp(input, *, min, out=None) → Tensor 将输入input张量每个元素的限制到不小于min ，并返回结果到一个新张量。 如果输入是FloatTensor or DoubleTensor类型，则参数 min 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，min取整数、实数皆可。】 参数： input (Tensor) – 输入张量 value (Number) – 限制范围下限 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.clamp(a, min=0.5) 1.3869 0.5000 0.5000 0.5000 [torch.FloatTensor of size 4] torch.clamp(input, *, max, out=None) → Tensor 将输入input张量每个元素的限制到不大于max ，并返回结果到一个新张量。 如果输入是FloatTensor or DoubleTensor类型，则参数 max 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，max取整数、实数皆可。】 参数： input (Tensor) – 输入张量 value (Number) – 限制范围上限 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.clamp(a, max=0.5) 0.5000 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] torch.cos torch.cos(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的余弦。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.cos(a) 0.8041 0.9633 0.9018 0.2557 [torch.FloatTensor of size 4] torch.cosh torch.cosh(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的双曲余弦。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.cosh(a) 1.2095 1.0372 1.1015 1.9917 [torch.FloatTensor of size 4] torch.div() torch.div(input, value, out=None)将input逐元素除以标量值value，并返回结果到输出张量out。 即 out=tensor/value如果输入是FloatTensor or DoubleTensor类型，则参数 value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】 参数： input (Tensor) – 输入张量 value (Number) – 除数 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(5) &gt;&gt;&gt; a -0.6147 -1.1237 -0.1604 -0.6853 0.1063 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.div(a, 0.5) -1.2294 -2.2474 -0.3208 -1.3706 0.2126 [torch.FloatTensor of size 5] torch.div(input, other, out=None) 两张量input和other逐元素相除，并将结果返回到输出。即， outi=inputi/otheri两张量形状不须匹配，但元素数须一致。 注意：当形状不匹配时，input的形状作为输出张量的形状。 参数： input (Tensor) – 张量(分子) other (Tensor) – 张量(分母) out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4,4) &gt;&gt;&gt; a -0.1810 0.4017 0.2863 -0.1013 0.6183 2.0696 0.9012 -1.5933 0.5679 0.4743 -0.0117 -0.1266 -0.1213 0.9629 0.2682 1.5968 [torch.FloatTensor of size 4x4] &gt;&gt;&gt; b = torch.randn(8, 2) &gt;&gt;&gt; b 0.8774 0.7650 0.8866 1.4805 -0.6490 1.1172 1.4259 -0.8146 1.4633 -0.1228 0.4643 -0.6029 0.3492 1.5270 1.6103 -0.6291 [torch.FloatTensor of size 8x2] &gt;&gt;&gt; torch.div(a, b) -0.2062 0.5251 0.3229 -0.0684 -0.9528 1.8525 0.6320 1.9559 0.3881 -3.8625 -0.0253 0.2099 -0.3473 0.6306 0.1666 -2.5381 [torch.FloatTensor of size 4x4] torch.exp torch.exp(tensor, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的指数。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 &gt;&gt;&gt; torch.exp(torch.Tensor([0, math.log(2)])) torch.FloatTensor([1, 2]) torch.floor torch.floor(input, out=None) → Tensor床函数: 返回一个新张量，包含输入input张量每个元素的floor，即不小于元素的最大整数。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.floor(a) 1 0 -1 -1 [torch.FloatTensor of size 4] torch.fmod torch.fmod(input, divisor, out=None) → Tensor计算除法余数。 除数与被除数可能同时含有整数和浮点数。此时，余数的正负与被除数相同。 参数： input (Tensor) – 被除数 divisor (Tensor or float) – 除数，一个数或与被除数相同类型的张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; torch.fmod(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2) torch.FloatTensor([-1, -0, -1, 1, 0, 1]) &gt;&gt;&gt; torch.fmod(torch.Tensor([1, 2, 3, 4, 5]), 1.5) torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5]) 参考: torch.remainder(), 计算逐元素余数， 相当于python 中的 % 操作符。 torch.frac torch.frac(tensor, out=None) → Tensor返回每个元素的分数部分。 例子： &gt;&gt;&gt; torch.frac(torch.Tensor([1, 2.5, -3.2]) torch.FloatTensor([0, 0.5, -0.2]) torch.lerp torch.lerp(start, end, weight, out=None)对两个张量以start，end做线性插值， 将结果返回到输出张量。即，outi=starti+weight∗(endi−starti) 参数： start (Tensor) – 起始点张量 end (Tensor) – 终止点张量 weight (float) – 插值公式的weight out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; start = torch.arange(1, 5) &gt;&gt;&gt; end = torch.Tensor(4).fill_(10) &gt;&gt;&gt; start 1 2 3 4 [torch.FloatTensor of size 4] &gt;&gt;&gt; end 10 10 10 10 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.lerp(start, end, 0.5) 5.5000 6.0000 6.5000 7.0000 [torch.FloatTensor of size 4] torch.log torch.log(input, out=None) → Tensor计算input 的自然对数 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(5) &gt;&gt;&gt; a -0.4183 0.3722 -0.3091 0.4149 0.5857 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.log(a) nan -0.9883 nan -0.8797 -0.5349 [torch.FloatTensor of size 5] torch.log1p torch.log1p(input, out=None) → Tensor计算 input+1的自然对数 yi=log(xi+1) 注意：对值比较小的输入，此函数比torch.log()更准确。 如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(5) &gt;&gt;&gt; a -0.4183 0.3722 -0.3091 0.4149 0.5857 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.log1p(a) -0.5418 0.3164 -0.3697 0.3471 0.4611 [torch.FloatTensor of size 5] torch.mul torch.mul(input, value, out=None)用标量值value乘以输入input的每个元素，并返回一个新的结果张量。 out=tensor∗value如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】 参数： input (Tensor) – 输入张量 value (Number) – 乘到每个元素的数 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(3) &gt;&gt;&gt; a -0.9374 -0.5254 -0.6069 [torch.FloatTensor of size 3] &gt;&gt;&gt; torch.mul(a, 100) -93.7411 -52.5374 -60.6908 [torch.FloatTensor of size 3] torch.mul(input, other, out=None) 两个张量input,other按元素进行相乘，并返回到输出张量。即计算outi=inputi∗otheri两计算张量形状不须匹配，但总元素数须一致。 注意：当形状不匹配时，input的形状作为输入张量的形状。 参数： input (Tensor) – 第一个相乘张量 other (Tensor) – 第二个相乘张量 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4,4) &gt;&gt;&gt; a -0.7280 0.0598 -1.4327 -0.5825 -0.1427 -0.0690 0.0821 -0.3270 -0.9241 0.5110 0.4070 -1.1188 -0.8308 0.7426 -0.6240 -1.1582 [torch.FloatTensor of size 4x4] &gt;&gt;&gt; b = torch.randn(2, 8) &gt;&gt;&gt; b 0.0430 -1.0775 0.6015 1.1647 -0.6549 0.0308 -0.1670 1.0742 -1.2593 0.0292 -0.0849 0.4530 1.2404 -0.4659 -0.1840 0.5974 [torch.FloatTensor of size 2x8] &gt;&gt;&gt; torch.mul(a, b) -0.0313 -0.0645 -0.8618 -0.6784 0.0934 -0.0021 -0.0137 -0.3513 1.1638 0.0149 -0.0346 -0.5068 -1.0304 -0.3460 0.1148 -0.6919 [torch.FloatTensor of size 4x4] torch.neg torch.neg(input, out=None) → Tensor返回一个新张量，包含输入input 张量按元素取负。 即， out=−1∗input 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(5) &gt;&gt;&gt; a -0.4430 1.1690 -0.8836 -0.4565 0.2968 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.neg(a) 0.4430 -1.1690 0.8836 0.4565 -0.2968 [torch.FloatTensor of size 5] torch.pow torch.pow(input, exponent, out=None)对输入input的按元素求exponent次幂值，并返回结果张量。 幂值exponent 可以为单一 float 数或者与input相同元素数的张量。 当幂值为标量时，执行操作：outi=xexponent 当幂值为张量时，执行操作：outi=xexponenti 参数： input (Tensor) – 输入张量 exponent (float or Tensor) – 幂值 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.5274 -0.8232 -2.1128 1.7558 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.pow(a, 2) 0.2781 0.6776 4.4640 3.0829 [torch.FloatTensor of size 4] &gt;&gt;&gt; exp = torch.arange(1, 5) &gt;&gt;&gt; a = torch.arange(1, 5) &gt;&gt;&gt; a 1 2 3 4 [torch.FloatTensor of size 4] &gt;&gt;&gt; exp 1 2 3 4 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.pow(a, exp) 1 4 27 256 [torch.FloatTensor of size 4] torch.pow(base, input, out=None) base 为标量浮点值,input为张量， 返回的输出张量 out 与输入张量相同形状。 执行操作为:outi=baseinputi 参数： base (float) – 标量值，指数的底 input ( Tensor) – 幂值 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; exp = torch.arange(1, 5) &gt;&gt;&gt; base = 2 &gt;&gt;&gt; torch.pow(base, exp) 2 4 8 16 [torch.FloatTensor of size 4] torch.reciprocal torch.reciprocal(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的倒数，即 1.0/x。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.reciprocal(a) 0.7210 2.5565 -1.1583 -1.8289 [torch.FloatTensor of size 4] torch.remainder torch.remainder(input, divisor, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的除法余数。 除数与被除数可能同时包含整数或浮点数。余数与除数有相同的符号。 参数： input (Tensor) – 被除数 divisor (Tensor or float) – 除数，一个数或者与除数相同大小的张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; torch.remainder(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2) torch.FloatTensor([1, 0, 1, 1, 0, 1]) &gt;&gt;&gt; torch.remainder(torch.Tensor([1, 2, 3, 4, 5]), 1.5) torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5]) 参考: 函数torch.fmod() 同样可以计算除法余数，相当于 C 的 库函数fmod() torch.round torch.round(input, out=None) → Tensor返回一个新张量，将输入input张量每个元素舍入到最近的整数。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.2290 1.3409 -0.5662 -0.0899 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.round(a) 1 1 -1 -0 [torch.FloatTensor of size 4] torch.rsqrt torch.rsqrt(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的平方根倒数。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.2290 1.3409 -0.5662 -0.0899 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.rsqrt(a) 0.9020 0.8636 nan nan [torch.FloatTensor of size 4] torch.sigmoid torch.sigmoid(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的sigmoid值。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.4972 1.3512 0.1056 -0.2650 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.sigmoid(a) 0.3782 0.7943 0.5264 0.4341 [torch.FloatTensor of size 4] torch.sign torch.sign(input, out=None) → Tensor符号函数：返回一个新张量，包含输入input张量每个元素的正负。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.sign(a) -1 1 1 1 [torch.FloatTensor of size 4] torch.sin torch.sin(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的正弦。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.sin(a) -0.5944 0.2684 0.4322 0.9667 [torch.FloatTensor of size 4] torch.sinh torch.sinh(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的双曲正弦。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.sinh(a) -0.6804 0.2751 0.4619 1.7225 [torch.FloatTensor of size 4] torch.sqrt torch.sqrt(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的平方根。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.2290 1.3409 -0.5662 -0.0899 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.sqrt(a) 1.1086 1.1580 nan nan [torch.FloatTensor of size 4] torch.tan torch.tan(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的正切。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.tan(a) -0.7392 0.2786 0.4792 3.7801 [torch.FloatTensor of size 4] torch.tanh torch.tanh(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的双曲正切。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.tanh(a) -0.5625 0.2653 0.4193 0.8648 [torch.FloatTensor of size 4] torch.trunc torch.trunc(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的截断值(标量x的截断值是最接近其的整数，其比x更接近零。简而言之，有符号数的小数部分被舍弃)。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.4972 1.3512 0.1056 -0.2650 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.trunc(a) -0 1 0 -0 [torch.FloatTensor of size 4] Reduction Ops torch.cumprod torch.cumprod(input, dim, out=None) → Tensor返回输入沿指定维度的累积积。例如，如果输入是一个N 元向量，则结果也是一个N 元向量，第i 个输出元素值为$ yi=x1∗x2∗x3∗…∗xi $ 参数： input (Tensor) – 输入张量 dim (int) – 累积积操作的维度 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(10) &gt;&gt;&gt; a 1.1148 1.8423 1.4143 -0.4403 1.2859 -1.2514 -0.4748 1.1735 -1.6332 -0.4272 [torch.FloatTensor of size 10] &gt;&gt;&gt; torch.cumprod(a, dim=0) 1.1148 2.0537 2.9045 -1.2788 -1.6444 2.0578 -0.9770 -1.1466 1.8726 -0.8000 [torch.FloatTensor of size 10] &gt;&gt;&gt; a[5] = 0.0 &gt;&gt;&gt; torch.cumprod(a, dim=0) 1.1148 2.0537 2.9045 -1.2788 -1.6444 -0.0000 0.0000 0.0000 -0.0000 0.0000 [torch.FloatTensor of size 10] torch.cumsum torch.cumsum(input, dim, out=None) → Tensor返回输入沿指定维度的累积和。例如，如果输入是一个N元向量，则结果也是一个N元向量，第i 个输出元素值为 yi=x1+x2+x3+…+xi 参数： input (Tensor) – 输入张量 dim (int) – 累积和操作的维度 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(10) &gt;&gt;&gt; a -0.6039 -0.2214 -0.3705 -0.0169 1.3415 -0.1230 0.9719 0.6081 -0.1286 1.0947 [torch.FloatTensor of size 10] &gt;&gt;&gt; torch.cumsum(a, dim=0) -0.6039 -0.8253 -1.1958 -1.2127 0.1288 0.0058 0.9777 1.5858 1.4572 2.5519 [torch.FloatTensor of size 10] torch.dist torch.dist(input, other, p=2, out=None) → Tensor返回 (input - other) 的 p范数 。 参数： input (Tensor) – 输入张量 other (Tensor) – 右侧输入张量 p (float, optional) – 所计算的范数 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; x = torch.randn(4) &gt;&gt;&gt; x 0.2505 -0.4571 -0.3733 0.7807 [torch.FloatTensor of size 4] &gt;&gt;&gt; y = torch.randn(4) &gt;&gt;&gt; y 0.7782 -0.5185 1.4106 -2.4063 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.dist(x, y, 3.5) 3.302832063224223 &gt;&gt;&gt; torch.dist(x, y, 3) 3.3677282206393286 &gt;&gt;&gt; torch.dist(x, y, 0) inf &gt;&gt;&gt; torch.dist(x, y, 1) 5.560028076171875 torch.mean torch.mean(input) → float返回输入张量所有元素的均值。 参数： input (Tensor) – 输入张量 例子： &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a -0.2946 -0.9143 2.1809 [torch.FloatTensor of size 1x3] &gt;&gt;&gt; torch.mean(a) 0.32398951053619385 torch.mean(input, dim, out=None) → Tensor 返回输入张量给定维度dim上每行的均值。 输出形状与输入相同，除了给定维度上为1. 参数： input (Tensor) – 输入张量 dim (int) – the dimension to reduce out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a -1.2738 -0.3058 0.1230 -1.9615 0.8771 -0.5430 -0.9233 0.9879 1.4107 0.0317 -0.6823 0.2255 -1.3854 0.4953 -0.2160 0.2435 [torch.FloatTensor of size 4x4] &gt;&gt;&gt; torch.mean(a, 1) -0.8545 0.0997 0.2464 -0.2157 [torch.FloatTensor of size 4x1] torch.median torch.median(input, dim=-1, values=None, indices=None) -&gt; (Tensor, LongTensor)返回输入张量给定维度每行的中位数，同时返回一个包含中位数的索引的LongTensor。 dim值默认为输入张量的最后一维。 输出形状与输入相同，除了给定维度上为1. 注意: 这个函数还没有在torch.cuda.Tensor中定义 参数： input (Tensor) – 输入张量 dim (int) – 缩减的维度 values (Tensor, optional) – 结果张量 indices (Tensor, optional) – 返回的索引结果张量 例子： &gt;&gt;&gt; a -0.6891 -0.6662 0.2697 0.7412 0.5254 -0.7402 0.5528 -0.2399 [torch.FloatTensor of size 4x2] &gt;&gt;&gt; a = torch.randn(4, 5) &gt;&gt;&gt; a 0.4056 -0.3372 1.0973 -2.4884 0.4334 2.1336 0.3841 0.1404 -0.1821 -0.7646 -0.2403 1.3975 -2.0068 0.1298 0.0212 -1.5371 -0.7257 -0.4871 -0.2359 -1.1724 [torch.FloatTensor of size 4x5] &gt;&gt;&gt; torch.median(a, 1) ( 0.4056 0.1404 0.0212 -0.7257 [torch.FloatTensor of size 4x1] , 0 2 4 1 [torch.LongTensor of size 4x1] ) torch.mode torch.mode(input, dim=-1, values=None, indices=None) -&gt; (Tensor, LongTensor)返回给定维dim上，每行的众数值。 同时返回一个LongTensor，包含众数职的索引。dim值默认为输入张量的最后一维。 输出形状与输入相同，除了给定维度上为1. 注意: 这个函数还没有在torch.cuda.Tensor中定义 参数： input (Tensor) – 输入张量 dim (int) – 缩减的维度 values (Tensor, optional) – 结果张量 indices (Tensor, optional) – 返回的索引张量 例子： &gt;&gt;&gt; a -0.6891 -0.6662 0.2697 0.7412 0.5254 -0.7402 0.5528 -0.2399 [torch.FloatTensor of size 4x2] &gt;&gt;&gt; a = torch.randn(4, 5) &gt;&gt;&gt; a 0.4056 -0.3372 1.0973 -2.4884 0.4334 2.1336 0.3841 0.1404 -0.1821 -0.7646 -0.2403 1.3975 -2.0068 0.1298 0.0212 -1.5371 -0.7257 -0.4871 -0.2359 -1.1724 [torch.FloatTensor of size 4x5] &gt;&gt;&gt; torch.mode(a, 1) ( -2.4884 -0.7646 -2.0068 -1.5371 [torch.FloatTensor of size 4x1] , 3 4 2 0 [torch.LongTensor of size 4x1] ) torch.norm torch.norm(input, p=2) → float返回输入张量input 的p 范数。 参数： input (Tensor) – 输入张量 p (float,optional) – 范数计算中的幂指数值 例子： &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a -0.4376 -0.5328 0.9547 [torch.FloatTensor of size 1x3] &gt;&gt;&gt; torch.norm(a, 3) 1.0338925067372466 torch.norm(input, p, dim, out=None) → Tensor 返回输入张量给定维dim 上每行的p 范数。 输出形状与输入相同，除了给定维度上为1. 参数： input (Tensor) – 输入张量 p (float) – 范数计算中的幂指数值 dim (int) – 缩减的维度 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4, 2) &gt;&gt;&gt; a -0.6891 -0.6662 0.2697 0.7412 0.5254 -0.7402 0.5528 -0.2399 [torch.FloatTensor of size 4x2] &gt;&gt;&gt; torch.norm(a, 2, 1) 0.9585 0.7888 0.9077 0.6026 [torch.FloatTensor of size 4x1] &gt;&gt;&gt; torch.norm(a, 0, 1) 2 2 2 2 [torch.FloatTensor of size 4x1] torch.prod torch.prod(input) → float返回输入张量input 所有元素的积。 参数： input (Tensor) – 输入张量 例子： &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a 0.6170 0.3546 0.0253 [torch.FloatTensor of size 1x3] &gt;&gt;&gt; torch.prod(a) 0.005537458061418483 orch.prod(input, dim, out=None) → Tensor 返回输入张量给定维度上每行的积。 输出形状与输入相同，除了给定维度上为1. 参数： input (Tensor) – 输入张量 dim (int) – 缩减的维度 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4, 2) &gt;&gt;&gt; a 0.1598 -0.6884 -0.1831 -0.4412 -0.9925 -0.6244 -0.2416 -0.8080 [torch.FloatTensor of size 4x2] &gt;&gt;&gt; torch.prod(a, 1) -0.1100 0.0808 0.6197 0.1952 [torch.FloatTensor of size 4x1] torch.std torch.std(input) → float返回输入张量input 所有元素的标准差。 参数： input (Tensor) – 输入张量 例子： &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a -1.3063 1.4182 -0.3061 [torch.FloatTensor of size 1x3] &gt;&gt;&gt; torch.std(a) 1.3782334731508061 torch.std(input, dim, out=None) → Tensor 返回输入张量给定维度上每行的标准差。 输出形状与输入相同，除了给定维度上为1. 参数： input (Tensor) – 输入张量 dim (int) – 缩减的维度 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a 0.1889 -2.4856 0.0043 1.8169 -0.7701 -0.4682 -2.2410 0.4098 0.1919 -1.1856 -1.0361 0.9085 0.0173 1.0662 0.2143 -0.5576 [torch.FloatTensor of size 4x4] &gt;&gt;&gt; torch.std(a, dim=1) 1.7756 1.1025 1.0045 0.6725 [torch.FloatTensor of size 4x1] torch.sum torch.sum(input) → float返回输入张量input 所有元素的和。 输出形状与输入相同，除了给定维度上为1. 参数： input (Tensor) – 输入张量 例子： &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a 0.6170 0.3546 0.0253 [torch.FloatTensor of size 1x3] &gt;&gt;&gt; torch.sum(a) 0.9969287421554327 torch.sum(input, dim, out=None) → Tensor 返回输入张量给定维度上每行的和。 输出形状与输入相同，除了给定维度上为1. 参数： input (Tensor) – 输入张量 dim (int) – 缩减的维度 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a -0.4640 0.0609 0.1122 0.4784 -1.3063 1.6443 0.4714 -0.7396 -1.3561 -0.1959 1.0609 -1.9855 2.6833 0.5746 -0.5709 -0.4430 [torch.FloatTensor of size 4x4] &gt;&gt;&gt; torch.sum(a, 1) 0.1874 0.0698 -2.4767 2.2440 [torch.FloatTensor of size 4x1] torch.var torch.var(input) → float返回输入张量所有元素的方差 输出形状与输入相同，除了给定维度上为1. 参数： input (Tensor) – 输入张量 例子： &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a -1.3063 1.4182 -0.3061 [torch.FloatTensor of size 1x3] &gt;&gt;&gt; torch.var(a) 1.899527506513334 torch.var(input, dim, out=None) → Tensor 返回输入张量给定维度上每行的方差。 输出形状与输入相同，除了给定维度上为1. 参数： input (Tensor) – 输入张量 dim (int) – the dimension to reduce out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a -1.2738 -0.3058 0.1230 -1.9615 0.8771 -0.5430 -0.9233 0.9879 1.4107 0.0317 -0.6823 0.2255 -1.3854 0.4953 -0.2160 0.2435 [torch.FloatTensor of size 4x4] &gt;&gt;&gt; torch.var(a, 1) 0.8859 0.9509 0.7548 0.6949 [torch.FloatTensor of size 4x1] 比较操作 Comparison Ops torch.eq torch.eq(input, other, out=None) → Tensor比较元素相等性。第二个参数可为一个数或与第一个参数同类型形状的张量。 参数： input (Tensor) – 待比较张量 other (Tensor or float) – 比较张量或数 out (Tensor, optional) – 输出张量，须为 ByteTensor类型 or 与input同类型 返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(相等为1，不等为0 ) 返回类型： Tensor 例子： &gt;&gt;&gt; torch.eq(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 1 0 0 1 [torch.ByteTensor of size 2x2] torch.equal torch.equal(tensor1, tensor2) → bool如果两个张量有相同的形状和元素值，则返回True ，否则 False。 例子： &gt;&gt;&gt; torch.equal(torch.Tensor([1, 2]), torch.Tensor([1, 2])) True torch.ge torch.ge(input, other, out=None) → Tensor逐元素比较input和other，即是否 input&gt;=other。 如果两个张量有相同的形状和元素值，则返回True ，否则 False。 第二个参数可以为一个数或与第一个参数相同形状和类型的张量 参数: input (Tensor) – 待对比的张量 other (Tensor or float) – 对比的张量或float值 out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;= other )。 返回类型： Tensor 例子： &gt;&gt;&gt; torch.ge(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 1 1 0 1 [torch.ByteTensor of size 2x2] torch.gt torch.gt(input, other, out=None) → Tensor逐元素比较input和other ， 即是否input&gt;other 如果两个张量有相同的形状和元素值，则返回True ，否则 False。 第二个参数可以为一个数或与第一个参数相同形状和类型的张量 参数: input (Tensor) – 要对比的张量 other (Tensor or float) – 要对比的张量或float值 out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;= other )。 返回类型： Tensor 例子： &gt;&gt;&gt; torch.gt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 0 1 0 0 [torch.ByteTensor of size 2x2] torch.kthvalue torch.kthvalue(input, k, dim=None, out=None) -&gt; (Tensor, LongTensor)取输入张量input指定维上第k 个最小值。如果不指定dim，则默认为input的最后一维。 返回一个元组 (values,indices)，其中indices是原始输入张量input中沿dim维的第 k 个最小值下标。 参数: input (Tensor) – 要对比的张量 k (int) – 第 k 个最小值 dim (int, optional) – 沿着此维进行排序 out (tuple, optional) – 输出元组 (Tensor, LongTensor) 可选地给定作为 输出 buffers 例子： &gt;&gt;&gt; x = torch.arange(1, 6) &gt;&gt;&gt; x 1 2 3 4 5 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.kthvalue(x, 4) ( 4 [torch.FloatTensor of size 1] , 3 [torch.LongTensor of size 1] ) torch.le torch.le(input, other, out=None) → Tensor逐元素比较input和other ， 即是否input&lt;=other 第二个参数可以为一个数或与第一个参数相同形状和类型的张量 参数: input (Tensor) – 要对比的张量 other (Tensor or float ) – 对比的张量或float值 out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;= other )。 返回类型： Tensor 例子： &gt;&gt;&gt; torch.le(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 1 0 1 1 [torch.ByteTensor of size 2x2] torch.lt torch.lt(input, other, out=None) → Tensor逐元素比较input和other ， 即是否 input&lt;other 第二个参数可以为一个数或与第一个参数相同形状和类型的张量 参数: input (Tensor) – 要对比的张量 other (Tensor or float ) – 对比的张量或float值 out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。 input： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 tensor &gt;= other )。 返回类型： Tensor 例子： &gt;&gt;&gt; torch.lt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 0 0 1 0 [torch.ByteTensor of size 2x2] torch.max torch.max()返回输入张量所有元素的最大值。 参数: input (Tensor) – 输入张量 例子： &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a 0.4729 -0.2266 -0.2085 [torch.FloatTensor of size 1x3] &gt;&gt;&gt; torch.max(a) 0.4729 torch.max(input, dim, max=None, max_indices=None) -&gt; (Tensor, LongTensor) 返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引。 输出形状中，将dim维设定为1，其它与输入形状保持一致。 参数: input (Tensor) – 输入张量 dim (int) – 指定的维度 max (Tensor, optional) – 结果张量，包含给定维度上的最大值 max_indices (LongTensor, optional) – 结果张量，包含给定维度上每个最大值的位置索引 例子： &gt;&gt; a = torch.randn(4, 4) &gt;&gt; a 0.0692 0.3142 1.2513 -0.5428 0.9288 0.8552 -0.2073 0.6409 1.0695 -0.0101 -2.4507 -1.2230 0.7426 -0.7666 0.4862 -0.6628 torch.FloatTensor of size 4x4] &gt;&gt;&gt; torch.max(a, 1) ( 1.2513 0.9288 1.0695 0.7426 [torch.FloatTensor of size 4x1] , 2 0 0 0 [torch.LongTensor of size 4x1] ) torch.max(input, other, out=None) → Tensor 返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引。 即，outi=max(inputi,otheri)输出形状中，将dim维设定为1，其它与输入形状保持一致。 参数: input (Tensor) – 输入张量 other (Tensor) – 输出张量 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] &gt;&gt;&gt; b = torch.randn(4) &gt;&gt;&gt; b 1.0067 -0.8010 0.6258 0.3627 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.max(a, b) 1.3869 0.3912 0.6258 0.3627 [torch.FloatTensor of size 4] torch.min torch.min(input) → float返回输入张量所有元素的最小值。 参数: input (Tensor) – 输入张量 例子： &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a 0.4729 -0.2266 -0.2085 [torch.FloatTensor of size 1x3] &gt;&gt;&gt; torch.min(a) -0.22663167119026184 torch.min(input, dim, min=None, min_indices=None) -&gt; (Tensor, LongTensor) 返回输入张量给定维度上每行的最小值，并同时返回每个最小值的位置索引。 输出形状中，将dim维设定为1，其它与输入形状保持一致。 参数: input (Tensor) – 输入张量 dim (int) – 指定的维度 min (Tensor, optional) – 结果张量，包含给定维度上的最小值 min_indices (LongTensor, optional) – 结果张量，包含给定维度上每个最小值的位置索引 例子： &gt;&gt; a = torch.randn(4, 4) &gt;&gt; a 0.0692 0.3142 1.2513 -0.5428 0.9288 0.8552 -0.2073 0.6409 1.0695 -0.0101 -2.4507 -1.2230 0.7426 -0.7666 0.4862 -0.6628 torch.FloatTensor of size 4x4] &gt;&gt; torch.min(a, 1) 0.5428 0.2073 2.4507 0.7666 torch.FloatTensor of size 4x1] 3 2 2 1 torch.LongTensor of size 4x1] torch.min(input, other, out=None) → Tensor input中逐元素与other相应位置的元素对比，返回最小值到输出张量。即，outi=min(tensori,otheri)两张量形状不需匹配，但元素数须相同。 注意：当形状不匹配时，input的形状作为返回张量的形状。 参数: input (Tensor) – 输入张量 other (Tensor) – 第二个输入张量 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] &gt;&gt;&gt; b = torch.randn(4) &gt;&gt;&gt; b 1.0067 -0.8010 0.6258 0.3627 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.min(a, b) 1.0067 -0.8010 -0.8634 -0.5468 [torch.FloatTensor of size 4] torch.ne torch.ne(input, other, out=None) → Tensor逐元素比较input和other ， 即是否 input!=other。 第二个参数可以为一个数或与第一个参数相同形状和类型的张量 参数: input (Tensor) – 待对比的张量 other (Tensor or float) – 对比的张量或float值 out (Tensor, optional) – 输出张量。必须为ByteTensor或者与input相同类型。 返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果 (如果 tensor != other 为True ，返回1)。 返回类型： Tensor 例子： &gt;&gt;&gt; torch.ne(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 0 1 1 0 [torch.ByteTensor of size 2x2] torch.sort torch.sort(input, dim=None, descending=False, out=None) -&gt; (Tensor, LongTensor)对输入张量input沿着指定维按升序排序。如果不给定dim，则默认为输入的最后一维。如果指定参数descending为True，则按降序排序 返回元组 (sorted_tensor, sorted_indices) ， sorted_indices 为原始输入中的下标。 参数: input (Tensor) – 要对比的张量 dim (int, optional) – 沿着此维排序 descending (bool, optional) – 布尔值，控制升降排序 out (tuple, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。 例子： &gt;&gt;&gt; x = torch.randn(3, 4) &gt;&gt;&gt; sorted, indices = torch.sort(x) &gt;&gt;&gt; sorted -1.6747 0.0610 0.1190 1.4137 -1.4782 0.7159 1.0341 1.3678 -0.3324 -0.0782 0.3518 0.4763 [torch.FloatTensor of size 3x4] &gt;&gt;&gt; indices 0 1 3 2 2 1 0 3 3 1 0 2 [torch.LongTensor of size 3x4] &gt;&gt;&gt; sorted, indices = torch.sort(x, 0) &gt;&gt;&gt; sorted -1.6747 -0.0782 -1.4782 -0.3324 0.3518 0.0610 0.4763 0.1190 1.0341 0.7159 1.4137 1.3678 [torch.FloatTensor of size 3x4] &gt;&gt;&gt; indices 0 2 1 2 2 0 2 0 1 1 0 1 [torch.LongTensor of size 3x4] torch.topk torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor)沿给定dim维度返回输入张量input中 k 个最大值。 如果不指定dim，则默认为input的最后一维。 如果为largest为 False ，则返回最小的 k 个值。 返回一个元组 (values,indices)，其中indices是原始输入张量input中测元素下标。 如果设定布尔值sorted 为True，将会确保返回的 k 个值被排序。 参数: input (Tensor) – 输入张量 k (int) – “top-k”中的k dim (int, optional) – 排序的维 largest (bool, optional) – 布尔值，控制返回最大或最小值 sorted (bool, optional) – 布尔值，控制返回值是否排序 out (tuple, optional) – 可选输出张量 (Tensor, LongTensor) output buffers 例子： &gt;&gt;&gt; x = torch.arange(1, 6) &gt;&gt;&gt; x 1 2 3 4 5 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.topk(x, 3) ( 5 4 3 [torch.FloatTensor of size 3] , 4 3 2 [torch.LongTensor of size 3] ) &gt;&gt;&gt; torch.topk(x, 3, 0, largest=False) ( 1 2 3 [torch.FloatTensor of size 3] , 0 1 2 [torch.LongTensor of size 3] ) 其它操作 Other Operations torch.cross torch.cross(input, other, dim=-1, out=None) → Tensor返回沿着维度dim上，两个张量input和other的向量积（叉积）。 input和other 必须有相同的形状，且指定的dim维上size必须为3。 如果不指定dim，则默认为第一个尺度为3的维。 参数： input (Tensor) – 输入张量 other (Tensor) – 第二个输入张量 dim (int, optional) – 沿着此维进行叉积操作 out (Tensor,optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4, 3) &gt;&gt;&gt; a -0.6652 -1.0116 -0.6857 0.2286 0.4446 -0.5272 0.0476 0.2321 1.9991 0.6199 1.1924 -0.9397 [torch.FloatTensor of size 4x3] &gt;&gt;&gt; b = torch.randn(4, 3) &gt;&gt;&gt; b -0.1042 -1.1156 0.1947 0.9947 0.1149 0.4701 -1.0108 0.8319 -0.0750 0.9045 -1.3754 1.0976 [torch.FloatTensor of size 4x3] &gt;&gt;&gt; torch.cross(a, b, dim=1) -0.9619 0.2009 0.6367 0.2696 -0.6318 -0.4160 -1.6805 -2.0171 0.2741 0.0163 -1.5304 -1.9311 [torch.FloatTensor of size 4x3] &gt;&gt;&gt; torch.cross(a, b) -0.9619 0.2009 0.6367 0.2696 -0.6318 -0.4160 -1.6805 -2.0171 0.2741 0.0163 -1.5304 -1.9311 [torch.FloatTensor of size 4x3] torch.diag torch.diag(input, diagonal=0, out=None) → Tensor如果输入是一个向量(1D 张量)，则返回一个以input为对角线元素的2D方阵如果输入是一个矩阵(2D 张量)，则返回一个包含input对角线元素的1D张量参数diagonal指定对角线: diagonal = 0, 主对角线diagonal &gt; 0, 主对角线之上diagonal &lt; 0, 主对角线之下 参数： input (Tensor) – 输入张量 diagonal (int, optional) – 指定对角线 out (Tensor, optional) – 输出张量 例子： 取得以input为对角线的方阵： &gt;&gt;&gt; a = torch.randn(3) &gt;&gt;&gt; a 1.0480 -2.3405 -1.1138 [torch.FloatTensor of size 3] &gt;&gt;&gt; torch.diag(a) 1.0480 0.0000 0.0000 0.0000 -2.3405 0.0000 0.0000 0.0000 -1.1138 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.diag(a, 1) 0.0000 1.0480 0.0000 0.0000 0.0000 0.0000 -2.3405 0.0000 0.0000 0.0000 0.0000 -1.1138 0.0000 0.0000 0.0000 0.0000 [torch.FloatTensor of size 4x4] 取得给定矩阵第k个对角线: &gt;&gt;&gt; a = torch.randn(3, 3) &gt;&gt;&gt; a -1.5328 -1.3210 -1.5204 0.8596 0.0471 -0.2239 -0.6617 0.0146 -1.0817 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.diag(a, 0) -1.5328 0.0471 -1.0817 [torch.FloatTensor of size 3] &gt;&gt;&gt; torch.diag(a, 1) -1.3210 -0.2239 [torch.FloatTensor of size 2] torch.histc torch.histc(input, bins=100, min=0, max=0, out=None) → Tensor计算输入张量的直方图。以min和max为range边界，将其均分成bins个直条，然后将排序好的数据划分到各个直条(bins)中。如果min和max都为0, 则利用数据中的最大最小值作为边界。 参数： input (Tensor) – 输入张量 bins (int) – 直方图 bins(直条)的个数(默认100个) min (int) – range的下边界(包含) max (int) – range的上边界(包含) out (Tensor, optional) – 结果张量 返回： 直方图 返回类型：张量 例子： &gt;&gt;&gt; torch.histc(torch.FloatTensor([1, 2, 1]), bins=4, min=0, max=3) FloatTensor([0, 2, 1, 0]) torch.renorm torch.renorm(input, p, dim, maxnorm, out=None) → Tensor返回一个张量，包含规范化后的各个子张量，使得沿着dim维划分的各子张量的p范数小于maxnorm。 注意 如果p范数的值小于maxnorm，则当前子张量不需要修改。 注意: 更详细解释参考torch7 以及Hinton et al. 2012, p. 2 参数： input (Tensor) – 输入张量 p (float) – 范数的p dim (int) – 沿着此维切片，得到张量子集 maxnorm (float) – 每个子张量的范数的最大值 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; x = torch.ones(3, 3) &gt;&gt;&gt; x[1].fill_(2) &gt;&gt;&gt; x[2].fill_(3) &gt;&gt;&gt; x 1 1 1 2 2 2 3 3 3 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.renorm(x, 1, 0, 5) 1.0000 1.0000 1.0000 1.6667 1.6667 1.6667 1.6667 1.6667 1.6667 [torch.FloatTensor of size 3x3] torch.trace torch.trace(input) → float返回输入2维矩阵对角线元素的和(迹) 例子： &gt;&gt;&gt; x = torch.arange(1, 10).view(3, 3) &gt;&gt;&gt; x 1 2 3 4 5 6 7 8 9 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.trace(x) 15.0 torch.tril torch.tril(input, k=0, out=None) → Tensor返回一个张量out，包含输入矩阵(2D张量)的下三角部分，out其余部分被设为0。这里所说的下三角部分为矩阵指定对角线diagonal之上的元素。 参数k控制对角线: k = 0, 主对角线 k &gt; 0, 主对角线之上 k &lt; 0, 主对角线之下 参数： input (Tensor) – 输入张量 k (int, optional) – 指定对角线 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(3,3) &gt;&gt;&gt; a 1.3225 1.7304 1.4573 -0.3052 -0.3111 -0.1809 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.tril(a) 1.3225 0.0000 0.0000 -0.3052 -0.3111 0.0000 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.tril(a, k=1) 1.3225 1.7304 0.0000 -0.3052 -0.3111 -0.1809 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.tril(a, k=-1) 0.0000 0.0000 0.0000 -0.3052 0.0000 0.0000 1.2469 0.0064 0.0000 [torch.FloatTensor of size 3x3] torch.triu torch.triu(input, k=0, out=None) → Tensor返回一个张量，包含输入矩阵(2D张量)的上三角部分，其余部分被设为0。这里所说的上三角部分为矩阵指定对角线diagonal之上的元素。 参数k控制对角线: k = 0, 主对角线 k &gt; 0, 主对角线之上 k &lt; 0, 主对角线之下 参数： input (Tensor) – 输入张量 k (int, optional) – 指定对角线 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(3,3) &gt;&gt;&gt; a 1.3225 1.7304 1.4573 -0.3052 -0.3111 -0.1809 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.triu(a) 1.3225 1.7304 1.4573 0.0000 -0.3111 -0.1809 0.0000 0.0000 -1.6250 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.triu(a, k=1) 0.0000 1.7304 1.4573 0.0000 0.0000 -0.1809 0.0000 0.0000 0.0000 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.triu(a, k=-1) 1.3225 1.7304 1.4573 -0.3052 -0.3111 -0.1809 0.0000 0.0064 -1.6250 [torch.FloatTensor of size 3x3] BLAS and LAPACK Operations torch.addbmm torch.addbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) → Tensor 对两个批batch1和batch2内存储的矩阵进行批矩阵乘操作，附带reduced add 步骤( 所有矩阵乘结果沿着第一维相加)。矩阵mat加到最终结果。 batch1和 batch2都为包含相同数量矩阵的3维张量。 如果batch1是形为b×n×m的张量，batch1是形为b×m×p的张量，则out和mat的形状都是n×p，即 res=(beta∗M)+(alpha∗sum(batch1i@batch2i,i=0,b))对类型为 FloatTensor 或 DoubleTensor 的输入，alphaand beta必须为实数，否则两个参数须为整数。 参数： beta (Number, optional) – 用于mat的乘子 mat (Tensor) – 相加矩阵 alpha (Number, optional) – 用于batch1@batch2的乘子 batch1 (Tensor) – 第一批相乘矩阵 batch2 (Tensor) – 第二批相乘矩阵 out (Tensor, optional) – 输出张量 例子: &gt;&gt;&gt; M = torch.randn(3, 5) &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) &gt;&gt;&gt; torch.addbmm(M, batch1, batch2) -3.1162 11.0071 7.3102 0.1824 -7.6892 1.8265 6.0739 0.4589 -0.5641 -5.4283 -9.3387 -0.1794 -1.2318 -6.8841 -4.7239 [torch.FloatTensor of size 3x5] torch.addmm torch.addmm(beta=1, mat, alpha=1, mat1, mat2, out=None) → Tensor 对矩阵mat1和mat2进行矩阵乘操作。矩阵mat加到最终结果。如果mat1 是一个 n×m张量，mat2 是一个 m×p张量，那么out和mat的形状为n×p。 alpha 和 beta 分别是两个矩阵 mat1@mat2和mat的比例因子，即， out=(beta∗M)+(alpha∗mat1@mat2) 对类型为 FloatTensor 或 DoubleTensor 的输入，betaand alpha必须为实数，否则两个参数须为整数。 参数 ： beta (Number, optional) – 用于mat的乘子 mat (Tensor) – 相加矩阵 alpha (Number, optional) – 用于mat1@mat2的乘子 mat1 (Tensor) – 第一个相乘矩阵 mat2 (Tensor) – 第二个相乘矩阵 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; M = torch.randn(2, 3) &gt;&gt;&gt; mat1 = torch.randn(2, 3) &gt;&gt;&gt; mat2 = torch.randn(3, 3) &gt;&gt;&gt; torch.addmm(M, mat1, mat2) -0.4095 -1.9703 1.3561 5.7674 -4.9760 2.7378 [torch.FloatTensor of size 2x3] torch.addmv torch.addmv(beta=1, tensor, alpha=1, mat, vec, out=None) → Tensor 对矩阵mat和向量vec对进行相乘操作。向量tensor加到最终结果。如果mat 是一个 n×m维矩阵，vec 是一个 m维向量，那么out和mat的为n元向量。 可选参数alpha 和 beta 分别是 mat∗vec和mat的比例因子，即， out=(beta∗tensor)+(alpha∗(mat@vec)) 对类型为FloatTensor或DoubleTensor的输入，alphaand beta必须为实数，否则两个参数须为整数。 参数 ： beta (Number, optional) – 用于mat的乘子 mat (Tensor) – 相加矩阵 alpha (Number, optional) – 用于mat1@vec的乘子 mat (Tensor) – 相乘矩阵 vec (Tensor) – 相乘向量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; M = torch.randn(2) &gt;&gt;&gt; mat = torch.randn(2, 3) &gt;&gt;&gt; vec = torch.randn(3) &gt;&gt;&gt; torch.addmv(M, mat, vec) -2.0939 -2.2950 [torch.FloatTensor of size 2] torch.addr torch.addr(beta=1, mat, alpha=1, vec1, vec2, out=None) → Tensor对向量vec1和vec2对进行张量积操作。矩阵mat加到最终结果。如果vec1 是一个 n维向量，vec2 是一个 m维向量，那么矩阵mat的形状须为n×m。 可选参数beta 和 alpha 分别是两个矩阵 mat和 vec1@vec2的比例因子，即，resi=(beta∗Mi)+(alpha∗batch1i×batch2i) 对类型为FloatTensor或DoubleTensor的输入，alphaand beta必须为实数，否则两个参数须为整数。 参数 ： beta (Number, optional) – 用于mat的乘子 mat (Tensor) – 相加矩阵 alpha (Number, optional) – 用于两向量vec1，vec2外积的乘子 vec1 (Tensor) – 第一个相乘向量 vec2 (Tensor) – 第二个相乘向量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; vec1 = torch.arange(1, 4) &gt;&gt;&gt; vec2 = torch.arange(1, 3) &gt;&gt;&gt; M = torch.zeros(3, 2) &gt;&gt;&gt; torch.addr(M, vec1, vec2) 1 2 2 4 3 6 [torch.FloatTensor of size 3x2] torch.baddbmm torch.baddbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) → Tensor 对两个批batch1和batch2内存储的矩阵进行批矩阵乘操作，矩阵mat加到最终结果。 batch1和 batch2都为包含相同数量矩阵的3维张量。 如果batch1是形为b×n×m的张量，batch1是形为b×m×p的张量，则out和mat的形状都是n×p，即 resi=(beta∗Mi)+(alpha∗batch1i×batch2i)对类型为FloatTensor或DoubleTensor的输入，alphaand beta必须为实数，否则两个参数须为整数。 参数： beta (Number, optional) – 用于mat的乘子 mat (Tensor) – 相加矩阵 alpha (Number, optional) – 用于batch1@batch2的乘子 batch1 (Tensor) – 第一批相乘矩阵 batch2 (Tensor) – 第二批相乘矩阵 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; M = torch.randn(10, 3, 5) &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) &gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size() torch.Size([10, 3, 5]) torch.bmm torch.bmm(batch1, batch2, out=None) → Tensor对存储在两个批batch1和batch2内的矩阵进行批矩阵乘操作。batch1和 batch2都为包含相同数量矩阵的3维张量。 如果batch1是形为b×n×m的张量，batch1是形为b×m×p的张量，则out和mat的形状都是n×p，即 res=(beta∗M)+(alpha∗sum(batch1i@batch2i,i=0,b))对类型为 FloatTensor 或 DoubleTensor 的输入，alphaand beta必须为实数，否则两个参数须为整数。 参数： batch1 (Tensor) – 第一批相乘矩阵 batch2 (Tensor) – 第二批相乘矩阵 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) &gt;&gt;&gt; res = torch.bmm(batch1, batch2) &gt;&gt;&gt; res.size() torch.Size([10, 3, 5]) torch.btrifact torch.btrifact(A, info=None) → Tensor, IntTensor返回一个元组，包含LU 分解和pivots 。 可选参数info决定是否对每个minibatch样本进行分解。info are from dgetrf and a non-zero value indicates an error occurred. 如果用CUDA的话，这个值来自于CUBLAS，否则来自LAPACK。 参数： A (Tensor) – 待分解张量 例子： &gt;&gt;&gt; A = torch.randn(2, 3, 3) &gt;&gt;&gt; A_LU = A.btrifact() torch.btrisolve torch.btrisolve(b, LU_data, LU_pivots) → Tensor返回线性方程组Ax=b的LU解。 参数： b (Tensor) – RHS 张量. LU_data (Tensor) – Pivoted LU factorization of A from btrifact. LU_pivots (IntTensor) – LU 分解的Pivots. 例子： &gt;&gt;&gt; A = torch.randn(2, 3, 3) &gt;&gt;&gt; b = torch.randn(2, 3) &gt;&gt;&gt; A_LU = torch.btrifact(A) &gt;&gt;&gt; x = b.btrisolve(*A_LU) &gt;&gt;&gt; torch.norm(A.bmm(x.unsqueeze(2)) - b) 6.664001874625056e-08 torch.dot torch.dot(tensor1, tensor2) → float计算两个张量的点乘(内乘),两个张量都为1-D 向量. 例子： &gt;&gt;&gt; torch.dot(torch.Tensor([2, 3]), torch.Tensor([2, 1])) 7.0 torch.eig torch.eig(a, eigenvectors=False, out=None) -&gt; (Tensor, Tensor)计算实方阵a 的特征值和特征向量 参数： a (Tensor) – 方阵，待计算其特征值和特征向量 eigenvectors (bool) – 布尔值，如果True，则同时计算特征值和特征向量，否则只计算特征值。 out (tuple, optional) – 输出元组 返回值：元组，包括： e (Tensor): a 的右特征向量 v (Tensor): 如果eigenvectors为True，则为包含特征向量的张量; 否则为空张量 返回值类型： (Tensor, Tensor) torch.gels torch.gels(B, A, out=None) → Tensor对形如m×n的满秩矩阵a计算其最小二乘和最小范数问题的解。 如果m&gt;=n,gels对最小二乘问题进行求解，即：minimize∥AX−B∥F如果m&lt;n,gels求解最小范数问题，即：minimize∥X∥Fsubject toabAX=B返回矩阵X的前n 行包含解。余下的行包含以下残差信息: 相应列从第n 行开始计算的每列的欧式距离。 注意： 返回矩阵总是被转置，无论输入矩阵的原始布局如何，总会被转置；即，总是有 stride (1, m) 而不是 (m, 1). 参数： B (Tensor) – 矩阵B A (Tensor) – m×n矩阵 out (tuple, optional) – 输出元组 返回值： 元组，包括： X (Tensor): 最小二乘解 qr (Tensor): QR 分解的细节 返回值类型： (Tensor, Tensor) 例子： &gt;&gt;&gt; A = torch.Tensor([[1, 1, 1], ... [2, 3, 4], ... [3, 5, 2], ... [4, 2, 5], ... [5, 4, 3]]) &gt;&gt;&gt; B = torch.Tensor([[-10, -3], [ 12, 14], [ 14, 12], [ 16, 16], [ 18, 16]]) &gt;&gt;&gt; X, _ = torch.gels(B, A) &gt;&gt;&gt; X 2.0000 1.0000 1.0000 1.0000 1.0000 2.0000 [torch.FloatTensor of size 3x2] torch.geqrf torch.geqrf(input, out=None) -&gt; (Tensor, Tensor) 这是一个直接调用LAPACK的底层函数。 一般使用torch.qr() 计算输入的QR 分解，但是并不会分别创建Q,R两个矩阵，而是直接调用LAPACK 函数 Rather, this directly calls the underlying LAPACK function ?geqrf which produces a sequence of ‘elementary reflectors’. 参考 LAPACK文档获取更详细信息。 参数: input (Tensor) – 输入矩阵 out (tuple, optional) – 元组，包含输出张量 (Tensor, Tensor) torch.ger torch.ger(vec1, vec2, out=None) → Tensor计算两向量vec1,vec2的张量积。如果vec1的长度为n,vec2长度为m，则输出out应为形如n x m的矩阵。 参数: vec1 (Tensor) – 1D 输入向量 vec2 (Tensor) – 1D 输入向量 out (tuple, optional) – 输出张量 例子： &gt;&gt;&gt; v1 = torch.arange(1, 5) &gt;&gt;&gt; v2 = torch.arange(1, 4) &gt;&gt;&gt; torch.ger(v1, v2) 1 2 3 2 4 6 3 6 9 4 8 12 [torch.FloatTensor of size 4x3] torch.gesv torch.gesv(B, A, out=None) -&gt; (Tensor, Tensor) X,LU=torch.gesv(B,A)，返回线性方程组AX=B的解。 LU 包含两个矩阵L，U。A须为非奇异方阵，如果A是一个m×m矩阵，B 是m×k矩阵，则LU 是m×m矩阵， X为m×k矩阵 参数： B (Tensor) – m×k矩阵 A (Tensor) – m×m矩阵 out (Tensor, optional) – 可选地输出矩阵X 例子: &gt;&gt;&gt; A = torch.Tensor([[6.80, -2.11, 5.66, 5.97, 8.23], ... [-6.05, -3.30, 5.36, -4.44, 1.08], ... [-0.45, 2.58, -2.70, 0.27, 9.04], ... [8.32, 2.71, 4.35, -7.17, 2.14], ... [-9.67, -5.14, -7.26, 6.08, -6.87]]).t() &gt;&gt;&gt; B = torch.Tensor([[4.02, 6.19, -8.22, -7.57, -3.03], ... [-1.56, 4.00, -8.67, 1.75, 2.86], ... [9.81, -4.09, -4.57, -8.61, 8.99]]).t() &gt;&gt;&gt; X, LU = torch.gesv(B, A) &gt;&gt;&gt; torch.dist(B, torch.mm(A, X)) 9.250057093890353e-06 torch.inverse torch.inverse(input, out=None) → Tensor 对方阵输入input 取逆。 注意： 返回矩阵总是被转置，无论输入矩阵的原始布局如何，总会被转置；即，总是有 stride (1, m) 而不是 (m, 1). 参数 ： input (Tensor) – 输入2维张量 out (Tensor, optional) – 输出张量 例子: &gt;&gt;&gt; x = torch.rand(10, 10) &gt;&gt;&gt; x 0.7800 0.2267 0.7855 0.9479 0.5914 0.7119 0.4437 0.9131 0.1289 0.1982 0.0045 0.0425 0.2229 0.4626 0.6210 0.0207 0.6338 0.7067 0.6381 0.8196 0.8350 0.7810 0.8526 0.9364 0.7504 0.2737 0.0694 0.5899 0.8516 0.3883 0.6280 0.6016 0.5357 0.2936 0.7827 0.2772 0.0744 0.2627 0.6326 0.9153 0.7897 0.0226 0.3102 0.0198 0.9415 0.9896 0.3528 0.9397 0.2074 0.6980 0.5235 0.6119 0.6522 0.3399 0.3205 0.5555 0.8454 0.3792 0.4927 0.6086 0.1048 0.0328 0.5734 0.6318 0.9802 0.4458 0.0979 0.3320 0.3701 0.0909 0.2616 0.3485 0.4370 0.5620 0.5291 0.8295 0.7693 0.1807 0.0650 0.8497 0.1655 0.2192 0.6913 0.0093 0.0178 0.3064 0.6715 0.5101 0.2561 0.3396 0.4370 0.4695 0.8333 0.1180 0.4266 0.4161 0.0699 0.4263 0.8865 0.2578 [torch.FloatTensor of size 10x10] &gt;&gt;&gt; x = torch.rand(10, 10) &gt;&gt;&gt; y = torch.inverse(x) &gt;&gt;&gt; z = torch.mm(x, y) &gt;&gt;&gt; z 1.0000 0.0000 0.0000 -0.0000 0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 1.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 -0.0000 -0.0000 0.0000 0.0000 1.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 1.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 1.0000 0.0000 0.0000 -0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 0.0000 1.0000 -0.0000 -0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 0.0000 0.0000 1.0000 0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 -0.0000 1.0000 -0.0000 0.0000 -0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 -0.0000 -0.0000 1.0000 -0.0000 -0.0000 0.0000 -0.0000 -0.0000 -0.0000 0.0000 -0.0000 -0.0000 0.0000 1.0000 [torch.FloatTensor of size 10x10] &gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(10))) # Max nonzero 5.096662789583206e-07 torch.mm torch.mm(mat1, mat2, out=None) → Tensor 对矩阵mat1和mat2进行相乘。 如果mat1 是一个n×m 张量，mat2 是一个 m×p 张量，将会输出一个 n×p 张量out。 参数 ： mat1 (Tensor) – 第一个相乘矩阵 mat2 (Tensor) – 第二个相乘矩阵 out (Tensor, optional) – 输出张量 例子: &gt;&gt;&gt; mat1 = torch.randn(2, 3) &gt;&gt;&gt; mat2 = torch.randn(3, 3) &gt;&gt;&gt; torch.mm(mat1, mat2) 0.0519 -0.3304 1.2232 4.3910 -5.1498 2.7571 [torch.FloatTensor of size 2x3] torch.mv torch.mv(mat, vec, out=None) → Tensor 对矩阵mat和向量vec进行相乘。 如果mat 是一个n×m张量，vec 是一个m元 1维张量，将会输出一个n 元 1维张量。 参数 ： mat (Tensor) – 相乘矩阵 vec (Tensor) – 相乘向量 out (Tensor, optional) – 输出张量 例子: &gt;&gt;&gt; mat = torch.randn(2, 3) &gt;&gt;&gt; vec = torch.randn(3) &gt;&gt;&gt; torch.mv(mat, vec) -2.0939 -2.2950 [torch.FloatTensor of size 2] torch.orgqr torch.orgqr() torch.ormqr torch.ormqr() torch.potrf torch.potrf() torch.potri torch.potri() torch.potrs torch.potrs() torch.pstrf torch.pstrf() torch.qr torch.qr(input, out=None) -&gt; (Tensor, Tensor) 计算输入矩阵的QR分解：返回两个矩阵q ,r， 使得 x=q∗r ，这里q 是一个半正交矩阵与 r 是一个上三角矩阵 本函数返回一个thin(reduced)QR分解。 注意 如果输入很大，可能可能会丢失精度。 注意 本函数依赖于你的LAPACK实现，虽然总能返回一个合法的分解，但不同平台可能得到不同的结果。 参数： input (Tensor) – 输入的2维张量 out (tuple, optional) – 输出元组tuple，包含Q和R 例子: &gt;&gt;&gt; a = torch.Tensor([[12, -51, 4], [6, 167, -68], [-4, 24, -41]]) &gt;&gt;&gt; q, r = torch.qr(a) &gt;&gt;&gt; q -0.8571 0.3943 0.3314 -0.4286 -0.9029 -0.0343 0.2857 -0.1714 0.9429 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; r -14.0000 -21.0000 14.0000 0.0000 -175.0000 70.0000 0.0000 0.0000 -35.0000 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.mm(q, r).round() 12 -51 4 6 167 -68 -4 24 -41 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.mm(q.t(), q).round() 1 -0 0 -0 1 0 0 0 1 [torch.FloatTensor of size 3x3] torch.svd torch.svd(input, some=True, out=None) -&gt; (Tensor, Tensor, Tensor) U,S,V=torch.svd(A)。 返回对形如 n×m的实矩阵 A 进行奇异值分解的结果，使得 A=USV’∗。 U 形状为 n×n，S 形状为 n×m ，V 形状为 m×m some 代表了需要计算的奇异值数目。如果 some=True, it computes some and some=False computes all. Irrespective of the original strides, the returned matrix U will be transposed, i.e. with strides (1, n) instead of (n, 1). 参数： input (Tensor) – 输入的2维张量 some (bool, optional) – 布尔值，控制需计算的奇异值数目 out (tuple, optional) – 结果tuple 例子： &gt;&gt;&gt; a = torch.Tensor([[8.79, 6.11, -9.15, 9.57, -3.49, 9.84], ... [9.93, 6.91, -7.93, 1.64, 4.02, 0.15], ... [9.83, 5.04, 4.86, 8.83, 9.80, -8.99], ... [5.45, -0.27, 4.85, 0.74, 10.00, -6.02], ... [3.16, 7.98, 3.01, 5.80, 4.27, -5.31]]).t() &gt;&gt;&gt; a 8.7900 9.9300 9.8300 5.4500 3.1600 6.1100 6.9100 5.0400 -0.2700 7.9800 -9.1500 -7.9300 4.8600 4.8500 3.0100 9.5700 1.6400 8.8300 0.7400 5.8000 -3.4900 4.0200 9.8000 10.0000 4.2700 9.8400 0.1500 -8.9900 -6.0200 -5.3100 [torch.FloatTensor of size 6x5] &gt;&gt;&gt; u, s, v = torch.svd(a) &gt;&gt;&gt; u -0.5911 0.2632 0.3554 0.3143 0.2299 -0.3976 0.2438 -0.2224 -0.7535 -0.3636 -0.0335 -0.6003 -0.4508 0.2334 -0.3055 -0.4297 0.2362 -0.6859 0.3319 0.1649 -0.4697 -0.3509 0.3874 0.1587 -0.5183 0.2934 0.5763 -0.0209 0.3791 -0.6526 [torch.FloatTensor of size 6x5] &gt;&gt;&gt; s 27.4687 22.6432 8.5584 5.9857 2.0149 [torch.FloatTensor of size 5] &gt;&gt;&gt; v -0.2514 0.8148 -0.2606 0.3967 -0.2180 -0.3968 0.3587 0.7008 -0.4507 0.1402 -0.6922 -0.2489 -0.2208 0.2513 0.5891 -0.3662 -0.3686 0.3859 0.4342 -0.6265 -0.4076 -0.0980 -0.4932 -0.6227 -0.4396 [torch.FloatTensor of size 5x5] &gt;&gt;&gt; torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t())) 8.934150226306685e-06 torch.symeig torch.symeig(input, eigenvectors=False, upper=True, out=None) -&gt; (Tensor, Tensor)e,V=torch.symeig(input) 返回实对称矩阵input的特征值和特征向量。 input 和 V 为 m×m 矩阵，e 是一个m 维向量。 此函数计算intput的所有特征值(和特征向量)，使得 input=Vdiag(e)V′布尔值参数eigenvectors 规定是否只计算特征向量。如果为False，则只计算特征值；若设为True，则两者都会计算。 因为输入矩阵 input 是对称的，所以默认只需要上三角矩阵。如果参数upper为 False，下三角矩阵部分也被利用。 注意： 返回矩阵总是被转置，无论输入矩阵的原始布局如何，总会被转置；即，总是有 stride (1, m) 而不是 (m, 1). 参数： input (Tensor) – 输入对称矩阵 eigenvectors (boolean, optional) – 布尔值（可选），控制是否计算特征向量 upper (boolean, optional) – 布尔值（可选），控制是否考虑上三角或下三角区域 out (tuple, optional) – 输出元组(Tensor, Tensor) 例子： &gt;&gt;&gt; a = torch.Tensor([[ 1.96, 0.00, 0.00, 0.00, 0.00], ... [-6.49, 3.80, 0.00, 0.00, 0.00], ... [-0.47, -6.39, 4.17, 0.00, 0.00], ... [-7.20, 1.50, -1.51, 5.70, 0.00], ... [-0.65, -6.34, 2.67, 1.80, -7.10]]).t() &gt;&gt;&gt; e, v = torch.symeig(a, eigenvectors=True) &gt;&gt;&gt; e -11.0656 -6.2287 0.8640 8.8655 16.0948 [torch.FloatTensor of size 5] &gt;&gt;&gt; v -0.2981 -0.6075 0.4026 -0.3745 0.4896 -0.5078 -0.2880 -0.4066 -0.3572 -0.6053 -0.0816 -0.3843 -0.6600 0.5008 0.3991 -0.0036 -0.4467 0.4553 0.6204 -0.4564 -0.8041 0.4480 0.1725 0.3108 0.1622 [torch.FloatTensor of size 5x5] torch.trtrs torch.trtrs()]]></content>
      <categories>
        <category>框架</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于facenet的实时人脸检测]]></title>
    <url>%2F2018%2F07%2F27%2F%E5%9F%BA%E4%BA%8Efacenet%E7%9A%84%E5%AE%9E%E6%97%B6%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[参考自 https://github.com/shanren7/real_time_face_recognition 本人的项目代码 https://github.com/zouzhen/real_time_face_recognize 虽然名字相同，但里面的内容可是有很大的不同由于不能满足当前的tensorflow版本，以及未能满足设计要求，进行了优化与重新设计 基于facenet的实时人脸检测工作环境 python 3.6 tensorflow==1.9.0(可运行在无gpu版) 代码结构real_time_face_recognize |—— model_check_point（保存人脸识别模型） |—— models（储存了facenet采用的神经网络模型） |—— detect_face.py(主要实现人脸的检测，同时返回可能的人脸框) |—— facenet.py（这里存储了facenet的主要函数） |—— real_time_face_recognize.py(实现了实时人脸检测) 运行 从 https://github.com/davidsandberg/facenet 中下载预训练的分类模型，放在model_check_point下 使用pip install requirements.txt安装需要的包，建议在virtualenv环境安装 在目录下新建picture文件，将需要识别的人的图片放入其中，每人放入一张清晰的图片即可 执行python real_time_face_recognize.py 注意除可在facenet作者的github中下载模型外，我自己基于lfw训练集训练了一个模型，点击]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>人脸识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB学习记录]]></title>
    <url>%2F2018%2F07%2F16%2FMongoDB%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[MongoDB转载自https://github.com/zxhyJack/MyBlog/blob/master/mongodb/mongodb.md 基本概念 文档 是键值对的有序集合，这是MongDB的核心概念 集合 集合就是一组文档 动态模式 集合是动态模式的，这意味着集合里面的文档可以是是各式各样的 命名 集合使用名称进行命名 数据库 由多个集合构成数据库，一个MongDB实例可以承载多个数据库，每个数据库拥有0个或多个集合 基本操作 在终端运行mongod命令，启动时，shell将自动连接MongDB数据库，需确保数据库已启动，可充分利用Javascript的标准库，还可定义和调用Javascript函数。 创建数据库 use database_name 如果数据库存在，则进入指定数据库，否则，创建数据库此时需要写入数据，数据库才能真正创建成功 查看所有数据库 show databases | dbs 创建集合 db.createCollection(collection_name) 删除数据库先进入要删除的数据库，然后执行命令 db.dropDatabase() 删除集合 db.collection_name.drop() 增 db.collection_name.insert(document) exp: db.students.insert({ name:&apos;James&apos;, age: 32, gender:&apos;man&apos;, career:&apos;player&apos; }) 查 db.collection.find(&lt;query&gt;,&lt;projection&gt;) - query: 查询条件 - projection: 投影操作 exp: db.students.find() 改 db.collection.updateOne(&lt;query&gt;,&lt;update&gt;) // 更新第一个符合条件的集合 db.collection.updateMany(&lt;query&gt;,&lt;update&gt;) // 更新所有符合条件的集合 query: 查询条件 update： 更新的内容 exp: db.students.update({name:’James’},{$set:{gender:’woman’}}) 删 db.collection_name.deleteOne(&lt;query&gt;) // 删除第一个符合条件的集合 db.collection_name.deleteMany(&lt;query&gt;) // 删除所有符合条件的集合 exp: db.students.deleteOne({name:&apos;James&apos;}) 数据操作（重点）数据库的核心——CRUD，增加和删除较为简单，查询和修改较复杂 查询关系运算符 $gt 大于 $lt 小于 $gte 大于等于 $lte 小于等于 $eq | (key: value) 等于 $ne 不等于 先往数据库中添加一些数据 db.students.insert({&apos;name&apos;:&apos;张三&apos;,&apos;sex&apos;:&apos;男&apos;,&apos;age&apos;:19,&apos;score&apos;: 89,&apos;address&apos;: &apos;海淀区&apos;}) db.students.insert({&apos;name&apos;:&apos;李四&apos;,&apos;sex&apos;:&apos;女&apos;,&apos;age&apos;:20,&apos;score&apos;: 100,&apos;address&apos;: &apos;朝阳区&apos;}) db.students.insert({&apos;name&apos;:&apos;王五&apos;,&apos;sex&apos;:&apos;男&apos;,&apos;age&apos;:22,&apos;score&apos;: 50,&apos;address&apos;: &apos;西城区&apos;}) db.students.insert({&apos;name&apos;:&apos;赵六&apos;,&apos;sex&apos;:&apos;女&apos;,&apos;age&apos;:21,&apos;score&apos;: 60,&apos;address&apos;: &apos;东城区&apos;}) db.students.insert({&apos;name&apos;:&apos;孙七&apos;,&apos;sex&apos;:&apos;男&apos;,&apos;age&apos;:19,&apos;score&apos;: 70,&apos;address&apos;: &apos;海淀区&apos;}) db.students.insert({&apos;name&apos;:&apos;王八&apos;,&apos;sex&apos;:&apos;女&apos;,&apos;age&apos;:23,&apos;score&apos;: 90,&apos;address&apos;: &apos;海淀区&apos;}) db.students.insert({&apos;name&apos;:&apos;刘九&apos;,&apos;sex&apos;:&apos;女&apos;,&apos;age&apos;:35,&apos;score&apos;: 56,&apos;address&apos;: &apos;朝阳区&apos;}) db.students.insert({&apos;name&apos;:&apos;钱十&apos;,&apos;sex&apos;:&apos;男&apos;,&apos;age&apos;:27,&apos;score&apos;: 89,&apos;address&apos;: &apos;海淀区&apos;}) exp: 查询姓名是张三的学生信息 db.students.find({name:’张三’}).pretty() 查询性别是男的学生信息 db.students.find({sex:’男’}).pretty() 查询年龄大于19岁的学生 db.students.find({age:{$gt:19}}).pretty() 查询成绩大于等于60分的学生 db.students.find({score:{$gte:60}}).pretty() 查询姓名不是王五的信息 db.students.find({name:{$ne:’王五’}}).pretty() 逻辑运算符 $and 与 $or 或 $not | $nor 非 exp: 查询年龄在19 ~ 22岁的学生信息 db.students.find({age:{$gte:19,$lte:22}}).pretty() 逻辑运算中与连接是最容易的，只需要利用,分割多个条件即可 查询年龄小于20岁，或者成绩大于90分的学生信息 db.students.find( {$or: [ {age:{$lt:20}}, {score:{$gt:90}} ] }).pretty() 查询年龄大于等于20岁，且成绩小于等于90分的学生信息 db.students.find( {$and: [ {age:{$gte:20}}, {score:{$lte:90}} ] }).pretty() 查询年龄小于20岁的学生信息 db.students.find({age:{$lt:20}}).pretty() db.students.find({age:{$not:{$gte:20}}}).pretty() 取模$mod:[除数，余数] exp: 查询年龄除以20余1的学生信息 db.students.find({age:{$mod:[20,1]}}).pretty() 范围查询$in: 在范围之中$nin: 不在范围之中 exp: 查询姓名是”张三“、”李四、”王五“的学生 db.students.find({name: {$in:[&apos;张三&apos;,&apos;李四&apos;,&apos;王五&apos;]}}).pret ty() 查询姓名不是”张三“、”李四、”王五“的学生 db.students.find({name: {$nin:[&apos;张三&apos;,&apos;李四&apos;,&apos;王五&apos;]}}).pretty() 数组查询 $all $size $slice $elemMatch 首先在数据库中新增一些数据 db.students.insert({name:&apos;a&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,course:[&apos;语文&apos;,&apos;数学&apos;,&apos;英语&apos;,&apos;音乐&apos;,&apos;政治&apos;]}) db.students.insert({name:&apos;b&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,course:[&apos;语文&apos;,&apos;数学&apos;]}) db.students.insert({name:&apos;c&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,course:[&apos;语文&apos;,&apos;数学&apos;,&apos;英语&apos;]}) db.students.insert({name:&apos;d&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,course:[&apos;英语&apos;,&apos;音乐&apos;,&apos;政治&apos;]}) db.students.insert({name:&apos;e&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,course:[&apos;语文&apos;,&apos;政治&apos;]}) $all: 表示全都包括，用法： {$all:[内容1,内容2]} exp: 查询同时参加语文和数学的学生 db.students.find({course:{$all:[&apos;语文&apos;,&apos;数学&apos;]}}).pretty() 数组的操作，可以利用索引，使用key.index的方式来定义索引 查询数组中第二个内容是数学的学生(sh) db.students.find({&apos;course.1&apos;:&apos;数学&apos;}).pretty() $size: 控制数组元素数量 exp: 查询只有两门课程的学生 db.students.find({course:{$size: 2}}).pretty() $slice: 控制查询结果的返回数量 exp: 查询年龄是19岁的学生，要求之显示两门参加的课程 db.students.find({age:19},{course:{$slice:2}}).pretty() 此时查询返回的是前两门课程，可以设置参数来取出想要的内容 $slice:-2 //后两门 $slice: [1,2] // 第一个参数表示跳过的数据量，第二个参数表示返回的数据量 嵌套集合运算对象里面套对象 在数据库中新增数据 db.students.insert( { name:&apos;A&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;, course:[&apos;语文&apos;,&apos;数学&apos;,&apos;英语&apos;,&apos;音乐&apos;,&apos;政治&apos;], parents:[ {name:&apos;A(father)&apos;,age:50,job:&apos;工人&apos;}, {name:&apos;A(mother)&apos;,age:50,job:&apos;职员&apos;} ] }) db.students.insert( { name:&apos;B&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;, course:[&apos;语文&apos;,&apos;数学&apos;], parents:[ {name:&apos;B(father)&apos;,age:50,job:&apos;处长&apos;}, {name:&apos;B(mother)&apos;,age:50,job:&apos;局长&apos;} ] }) db.students.insert( { name:&apos;C&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;, course:[&apos;语文&apos;,&apos;数学&apos;,&apos;英语&apos;], parents:[ {name:&apos;C(father)&apos;,age:50,job:&apos;工人&apos;}, {name:&apos;C(mother)&apos;,age:50,job:&apos;局长&apos;} ] }) 对于嵌套的集合中数据的判断只能通过$elemMatch完成 语法：{ &lt;field&gt;: { $elemMatch: { &lt;query1&gt;, &lt;query2&gt;, ... } } } exp: 查询父母中有人是局长的信息 db.students.find({parents: {$elemMatch: {job: &apos;局长&apos;}}}).pretty() 判断某个字段是否存在{$exists:flag} flag为true表示存在，false表示不存在 exp: 查询具有parents成员的学生 db.students.find({parents:{$exists: true}}).pretty() 查询不具有course成员的学生 db.students.find({course: {$exists: false}}).pretty() 排序sort({ field: value }) value是1表示升序，-1表示降序 exp: 学生信息按照分数降序排列 db.students.find().sort({score:-1}).pretty() 分页显示skip(n): 跳过n条数据 limit(n): 返回n条数据 exp: 分页显示，第一页，每页显示5条数据 db.students.find({}).skip(0).limit(5).pretty() 分页显示，第二页，每页显示5条数据 db.students.find({}).skip(5).limit(5).pretty() 数据修改 | 更新updateOne() 修改匹配的第一条数据 updateMany() 修改所有匹配的数据 格式：updateOne(&lt;filter&gt;,&lt;update&gt;) 修改器$inc: 操作数字字段的数据内容 语法: {&quot;$inc&quot; : {成员 : 内容}} exp: 将所有年龄为19岁的学生成绩一律减少30分，年龄增加1 db.students.updateMany({age:19},{$inc:{score:-30,age:1}}) $set: 更新内容 语法：{$set: :{属性: 新内容}} exp: 将20岁学生的成绩修改为89 db.students.updateMany({age: 20},{$set: {score: 89}}) $unset: 删除某个属性及其内容 语法：{$unset: {属性: 1}} exp: 删除张三的年龄和成绩信息 db.students.updateOne({name:&apos;张三&apos;},{$unset: {age: 1,score: 1}}) $push: 向数组中添加数据 语法：{$push: {属性: value}} exp: 在李四的课程中添加语文 db.students.updateOne({name: &apos;李四&apos;},{$push: {course: &apos;语文&apos;}}) 如果需要向数组中添加多个数据，则需要用到$each exp: 在李四的课程中添加数学、英语 db.students.updateOne( {name:&apos;李四&apos;}, {$push: { course:{$each: [&apos;数学&apos;,&apos;英语&apos;]} } } ) $addToSet: 向数组里面添加一个新的数据 与$push的区别，$push添加的数据可能是重复的，$addToSet只有这个数据不存在时才会添加（去重） 语法：{$addToSet: {属性：value}} exp: 王五新增一门舞蹈课程 db.students.updateOne( {name:&apos;王五&apos;}, {$addToSet: {course:&apos;舞蹈&apos;}} ) $pop: 删除数组内的数据 语法：{$pop: {field: value}},value为-1表示删除第一个，value为1表示删除最后一个 exp: 删除王五的第一个课程 db.students.updateOne({name:&apos;王五&apos;},{$pop:{course:-1}}) 只是删除属性的内容，属性还在 $pull: 从数组中删除一个指定内容的数据 语法：{$pull: {field：value}} 进行数据比对，如果是该数据则删除 exp: 删除李四的语文课程 db.students.updateOne({name: &apos;李四&apos;},{$pull:{course:&apos;语文&apos;}}) $pullAll: 一次删除多个数据 语法：{$pullAll:{field:[value1,value2...]}} exp: 删除a的语文数学英语课程 db.students.updateOne({name:&apos;a&apos;},{$pullAll:{course:[&apos;语文&apos;,&apos;数学&apos;,&apos;英语&apos;]}}) $rename: 属性重命名 语法： {$rename: {旧属性名：新属性名}} exp: 把张三的name属性名改为姓名 db.students.updateOne({name:&apos;张三&apos;},{$rename:{name:&apos;姓名&apos;}})]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[facenet详解]]></title>
    <url>%2F2018%2F07%2F14%2Ffacenet%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[facenet算法初测 facenet代码地址 数据对齐详解 已训练模型下载(基于CASIA-WebFace) 主要参考博客 算法代码结构结构如图： |—— contribute (包含对人脸进行处理的函数) |—— data (原算法进行训练或测试时使用的图片数据) |—— lfw (储存的lfw数据集) |—— lfw_mtcnnpy_160 (储存的经过对齐后的图片数据) |—— models (存储训练模型) |—— src (核心功能相关的代码) |—— test (算法、模型测试相关的代码) |—— tmp (暂不清楚) |—— 其他 功能测试 预训练模型测试： python src/validate_on_lfw.py lfw_mtcnnpy_160 models\20180408-102900 然而测试结果并不是特别好，可能是仅用CPU的缘故 相似人脸对比结果： python src\compare.py models\20180408-102900 data\images\Anthony_Hopkins_0001.jpg data\images\Anthony_Hopkins_0002.jpg 不相似人脸对比结果： python src\compare.py models\20180408-102900 data\images\Anthony_Hopkins_0001.jpg lfw_mtcnnpy_160\Aaron_Eckhart\Aaron_Eckhart_0001.png]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>facenet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript使用记录]]></title>
    <url>%2F2018%2F07%2F12%2FJavaScript%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[Promise在JavaScript的世界中，所有代码都是单线程执行的。 由于这个“缺陷”，导致JavaScript的所有网络操作，浏览器事件，都必须是异步执行。异步执行可以用回调函数实现： function callback() { console.log(&apos;Done&apos;); } console.log(&apos;before setTimeout()&apos;); setTimeout(callback, 1000); // 1秒钟后调用callback函数 console.log(&apos;after setTimeout()&apos;); 观察上述代码执行，在Chrome的控制台输出可以看到： before setTimeout() after setTimeout() (等待1秒后) Done 可见，异步操作会在将来的某个时间点触发一个函数调用。 AJAX就是典型的异步操作。以之前的代码为例： request.onreadystatechange = function () { if (request.readyState === 4) { if (request.status === 200) { return success(request.responseText); } else { return fail(request.status); } } } 把回调函数success(request.responseText)和fail(request.status)写到一个AJAX操作里很正常，但是不好看，而且不利于代码复用。 有没有更好的写法？比如写成这样： var ajax = ajaxGet(&apos;http://...&apos;); ajax.ifSuccess(success) .ifFail(fail); 这种链式写法的好处在于，先统一执行AJAX逻辑，不关心如何处理结果，然后，根据结果是成功还是失败，在将来的某个时候调用success函数或fail函数。 古人云：“君子一诺千金”，这种“承诺将来会执行”的对象在JavaScript中称为Promise对象。Promise有各种开源实现，在ES6中被统一规范，由浏览器直接支持。 我们先看一个最简单的Promise例子：生成一个0-2之间的随机数，如果小于1，则等待一段时间后返回成功，否则返回失败： function test(resolve, reject) { var timeOut = Math.random() * 2; log(&apos;set timeout to: &apos; + timeOut + &apos; seconds.&apos;); setTimeout(function () { if (timeOut &lt; 1) { log(&apos;call resolve()...&apos;); resolve(&apos;200 OK&apos;); } else { log(&apos;call reject()...&apos;); reject(&apos;timeout in &apos; + timeOut + &apos; seconds.&apos;); } }, timeOut * 1000); } 这个test()函数有两个参数，这两个参数都是函数，如果执行成功，我们将调用resolve(‘200 OK’)，如果执行失败，我们将调用reject(‘timeout in ‘ + timeOut + ‘ seconds.’)。可以看出，test()函数只关心自身的逻辑，并不关心具体的resolve和reject将如何处理结果。 有了执行函数，我们就可以用一个Promise对象来执行它，并在将来某个时刻获得成功或失败的结果： var p1 = new Promise(test); var p2 = p1.then(function (result) { console.log(&apos;成功：&apos; + result); }); var p3 = p2.catch(function (reason) { console.log(&apos;失败：&apos; + reason); }); 变量p1是一个Promise对象，它负责执行test函数。由于test函数在内部是异步执行的，当test函数执行成功时，我们告诉Promise对象： // 如果成功，执行这个函数： p1.then(function (result) { console.log(&apos;成功：&apos; + result); }); 当test函数执行失败时，我们告诉Promise对象： p2.catch(function (reason) { console.log(&apos;失败：&apos; + reason); }); Promise对象可以串联起来，所以上述代码可以简化为： new Promise(test).then(function (result) { console.log(&apos;成功：&apos; + result); }).catch(function (reason) { console.log(&apos;失败：&apos; + reason); }); 可见Promise最大的好处是在异步执行的流程中，把执行代码和处理结果的代码清晰地分离了： Promise还可以做更多的事情，比如，有若干个异步任务，需要先做任务1，如果成功后再做任务2，任何任务失败则不再继续并执行错误处理函数。 要串行执行这样的异步任务，不用Promise需要写一层一层的嵌套代码。有了Promise，我们只需要简单地写： job1.then(job2).then(job3).catch(handleError); 其中，job1、job2和job3都是Promise对象。]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git使用]]></title>
    <url>%2F2018%2F07%2F12%2FGit%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[转载自https://blog.csdn.net/zwhfyy/article/details/8625228，如有侵权，请联系删除 出错信息Your local changes to the following files would be overwritten by mergeerror: Your local changes to the following files would be overwritten by merge: 123.txtPlease, commit your changes or stash them before you can merge. 如果希望保留生产服务器上所做的改动,仅仅并入新配置项, 处理方法如下: git stash git pull git stash pop 然后可以使用git diff -w +文件名 来确认代码自动合并的情况. 反过来,如果希望用代码库中的文件完全覆盖本地工作版本. 方法如下: git reset --hard git pull 其中git reset是针对版本,如果想针对文件回退本地修改,使用]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人脸识别算法发展情况]]></title>
    <url>%2F2018%2F07%2F10%2F%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%AE%97%E6%B3%95%E5%8F%91%E5%B1%95%E6%83%85%E5%86%B5%2F</url>
    <content type="text"><![CDATA[转载自https://zhuanlan.zhihu.com/p/36416906，如有侵权，请联系删除。 人脸识别概述人脸识别的目标是确定一张人脸图像的身份，即这个人是谁，这是机器学习和模式识别中的分类问题。它主要应用在身份识别和身份验证中。 人脸识别系统的组成人脸检测（Face Detection） 人脸对齐（Face Alignment） 人脸特征表征（Feature Representation） 人脸检测 人脸检测用于确定人脸在图像中的大小和位置，即解决“人脸在哪里”的问题，把真正的人脸区域从图像中裁剪出来，便于后续的人脸特征分析和识别。 人脸对齐 同一个人在不同的图像序列中可能呈现出不同的姿态和表情，这种情况是不利于人脸识别的。所以有必要将人脸图像都变换到一个统一的角度和姿态，这就是人脸对齐。它的原理是找到人脸的若干个关键点（基准点，如眼角，鼻尖，嘴角等），然后利用这些对应的关键点通过相似变换（Similarity Transform，旋转、缩放和平移）将人脸尽可能变换到标准人脸。 人脸特征表征 第三个模块是本文重点要讲的人脸识别算法，它接受的输入是标准化的人脸图像，通过特征建模得到向量化的人脸特征，最后通过分类器判别得到识别的结果。这里的关键是怎样得到对不同人脸有区分度的特征，通常我们在识别一个人时会看它的眉形、脸轮廓、鼻子形状、眼睛的类型等，人脸识别算法引擎要通过练习（训练）得到类似这样的有区分度的特征。本系列文章主要围绕人脸识别中的人脸特征表征进行展开，人脸检测和人脸对齐方法会在其它专题系列文章中进行介绍。 人脸识别算法的三个阶段人脸识别算法经历了早期算法，人工特征+分类器，深度学习3个阶段。目前深度学习算法是主流，极大的提高了人脸识别的精度。 早期算法 早期的算法有基于几何特征的算法，基于模板匹配的算法，子空间算法等多种类型。子空间算法将人脸图像当成一个高维的向量，将向量投影到低维空间中，投影之后得到的低维向量达到对不同的人具有良好的区分度。 子空间算法的典型代表是PCA（主成分分析，也称为特征脸EigenFace）[1]和LDA（线性判别分析，FisherFace）[2]。PCA的核心思想是在进行投影之后尽量多的保留原始数据的主要信息，降低数据的冗余信息，以利于后续的识别。LDA的核心思想是最大化类间差异，最小化类内差异，即保证同一个人的不同人脸图像在投影之后聚集在一起，不同人的人脸图像在投影之后被用一个大的间距分开。PCA和LDA最后都归结于求解矩阵的特征值和特征向量，这有成熟的数值算法可以实现。 PCA和LDA都是线性降维技术，但人脸在高维空间中的分布显然是非线性的，因此可以使用非线性降维算法，典型的代表是流形学习[3]和核（kernel）技术。流形学习假设向量点在高维空间中的分布具有某些几何形状，然后在保持这些几何形状约束的前提下将向量投影到低维空间中，这种投影是通过非线性变换完成的。 人工特征 + 分类器 第二阶段的人脸识别算法普遍采用了人工特征 + 分类器的思路。分类器有成熟的方案，如神经网络，支持向量机[7]，贝叶斯[8]等。这里的关键是人工特征的设计，它要能有效的区分不同的人。 描述图像的很多特征都先后被用于人脸识别问题，包括HOG、SIFT、Gabor、LBP等。它们中的典型代表是LBP（局部二值模式）特征[9]，这种特征简单却有效。LBP特征计算起来非常简单，部分解决了光照敏感问题，但还是存在姿态和表情的问题。 联合贝叶斯是对贝叶斯人脸的改进方法[8]，选用LBP和LE作为基础特征，将人脸图像的差异表示为相同人因姿态、表情等导致的差异以及不同人间的差异两个因素，用潜在变量组成的协方差，建立两张人脸的关联。文章的创新点在于将两个人脸表示进行联合建模，在人脸联合建模的时候，又使用了人脸的先验知识，将两张人脸的建模问题变为单张人脸图片的统计计算，更好的验证人脸的相关性，该方法在LFW上取得了92.4%的准确率。 人工特征的巅峰之作是出自CVPR 2013年MSRA的”Blessing of Dimisionality: High Dimensional Feature and Its Efficient Compression for Face Verification” [10]，一篇关于如何使用高维度特征在人脸验证中的文章，作者主要以LBP（Local Binary Pattern，局部二值特征）为例子，论述了高维特征和验证性能存在着正相关的关系，即人脸维度越高，验证的准确度就越高。 深度学习 第三个阶段是基于深度学习的方法，自2012年深度学习在ILSVRC-2012大放异彩后，很多研究者都在尝试将其应用在自己的方向，这极大的推动了深度学习的发展。卷积神经网络在图像分类中显示出了巨大的威力，通过学习得到的卷积核明显优于人工设计的特征+分类器的方案。在人脸识别的研究者利用卷积神经网络（CNN）对海量的人脸图片进行学习，然后对输入图像提取出对区分不同人的脸有用的特征向量，替代人工设计的特征。 在前期，研究人员在网络结构、输入数据的设计等方面尝试了各种方案，然后送入卷积神经网络进行经典的目标分类模型训练；在后期，主要的改进集中在损失函数上，即迫使卷积网络学习得到对分辨不同的人更有效的特征，这时候人脸识别领域彻底被深度学习改造了！ DeepFace[11]是CVPR2014上由Facebook提出的方法，是深度卷积神经网络在人脸识别领域的奠基之作，文中使用了3D模型来做人脸对齐任务，深度卷积神经网络针对对齐后的人脸Patch进行多类的分类学习，使用的是经典的交叉熵损失函数（Softmax）进行问题优化，最后通过特征嵌入（Feature Embedding）得到固定长度的人脸特征向量。Backbone网络使用了多层局部卷积结构（Local Convolution），原因是希望网络的不同卷积核能学习人脸不同区域的特征，但会导致参数量增大，要求数据量很大，回过头去看该策略并不是十分必要。 DeepFace在LFW上取得了97.35%的准确率，已经接近了人类的水平。之后Google推出FaceNet（Facenet论文地址），使用三元组损失函数(Triplet Loss)代替常用的Softmax交叉熵损失函数，在一个超球空间上进行优化使类内距离更紧凑，类间距离更远，最后得到了一个紧凑的128维人脸特征，其网络使用GoogLeNet的Inception模型，模型参数量较小，精度更高，在LFW上取得了99.63%的准确率，这种损失函数的思想也可以追溯到早期的LDA算法。 CVPR2014、CVPR2015香港中文大学汤晓鸥团队提出的DeepID系列是一组非常有代表性的工作，其中DeepID1[12]使用四层卷积，最后一层为Softmax，中间为Deep Hidden Identity Features，是学习到的人脸特征表示，并使用Multi-patch分别训练模型最后组合成高维特征，人脸验证阶段使用联合贝叶斯的方法；通过学习一个多类（10000类，每个类大约有20个实例）人脸识别任务来学习特征，文中指出，随着训练时要预测的人脸类越多，DeepID的泛化能力就越强。 人脸识别算法仓库 ageitgey/face_recognition: https://github.com/ageitgey/face_recognition davidsandberg/facenet: https://github.com/davidsandberg/facenet cmusatyalab/openface: https://github.com/cmusatyalab/openface kpzhang93/MTCNN_face_detection_alignment(人脸检测): https://github.com/kpzhang93/MTCNN_face_detection_alignment deepinsight/insightface: https://github.com/deepinsight/insightface nyoki-mtl/keras-facenet: https://github.com/nyoki-mtl/keras-facenet yuyang-huang/keras-inception-resnet-v2(网络结构): https://github.com/yuyang-huang/keras-inception-resnet-v2 yobibyte/yobiface: https://github.com/yobibyte/yobiface/tree/master/src 相关博客 应用一个基于Python的开源人脸识别库，face_recognition:https://blog.csdn.net/hongbin_xu/article/details/76284134 TensorFlow–实现人脸识别实验精讲 （Face Recognition using Tensorflow）:https://blog.csdn.net/niutianzhuang/article/details/79191167 基于卷积神经网络和tensorflow实现的人脸识别:https://blog.csdn.net/hy13684802853/article/details/79780805 keras/构建卷积神经网络人脸识别: https://blog.csdn.net/szj_huhu/article/details/75202254 人脸识别–(opencv、dlib、keras-TensorFlow）:https://blog.csdn.net/u014258362/article/details/80688224 TensorFlow实现人脸识别(5)——-利用训练好的模型实时进行人脸检测:https://blog.csdn.net/yunge812/article/details/79447584 基于keras的人脸识别:https://blog.csdn.net/Julymycin/article/details/79182222 史上最全的FaceNet源码使用方法和讲解（一）（附预训练模型下载）:https://blog.csdn.net/u013044310/article/details/79556099https://github.com/boyliwensheng/understand_facenet(作者整理代码) 基于 MTCNN/TensorFlow 实现人脸检测:https://blog.csdn.net/Mr_EvanChen/article/details/77650883 计算机视觉实时目标检测 TensorFlow Object Detection APIhttps://blog.csdn.net/chenhaifeng2016/article/details/74205717]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>人脸识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nodejs与Django的跨域问题]]></title>
    <url>%2F2018%2F07%2F08%2FNodejs%E4%B8%8EDjango%E7%9A%84%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Nodejs与Django的跨域问题由于采用前后端分离的编程方式，Django的csrf_token验证失效，出现跨域问题，在此记录一下解决方法。 1 安装django-cors-headers pip install django-cors-headers 2 配置settings.py文件 OK！问题解决！]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>Django</tag>
        <tag>Nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django使用MongoDB数据库]]></title>
    <url>%2F2018%2F07%2F08%2FDjango%E4%BD%BF%E7%94%A8MongoDB%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[之前学习使用Django搭建在线教育平台使用的是Mysql数据库，现在考虑到公司以后的发展及当前技术需求，更换为MongoDB数据库。在此记录一下更改操作： 1 首先安装mongoengine，并在setting中设置对应的位置 pip instal mongoengine 2 设置默认的数据库信息 3 设置Model 总结之前因为使用Django自带的admin后台管理系统，所以在像Mysql一样迁移数据库时出现错误。后来分析发现，目前使用Django所做的工作不需要用到后台管理系统，仅仅是作为一个后台服务，因此可直接运行。至此，设置完成。]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7为firewalld添加开放端口及相关操作]]></title>
    <url>%2F2018%2F07%2F05%2FCentOS7%E4%B8%BAfirewalld%E6%B7%BB%E5%8A%A0%E5%BC%80%E6%94%BE%E7%AB%AF%E5%8F%A3%E5%8F%8A%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[firewalld的基本使用启动： systemctl start firewalld 查看状态： systemctl status firewalld 停止： systemctl disable firewalld 禁用： systemctl stop firewalld systemctl是CentOS7的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。启动一个服务：systemctl start firewalld.service 关闭一个服务：systemctlstop firewalld.service 重启一个服务：systemctlrestart firewalld.service 显示一个服务的状态：systemctlstatus firewalld.service 在开机时启用一个服务：systemctlenable firewalld.service 在开机时禁用一个服务：systemctldisable firewalld.service 查看服务是否开机启动：systemctlis-enabled firewalld.service 查看已启动的服务列表：systemctllist-unit-files|grep enabled 查看启动失败的服务列表：systemctl--failed 配置firewalld-cmd查看版本： firewall-cmd --version 查看帮助： firewall-cmd --help 显示状态： firewall-cmd --state 查看所有打开的端口： firewall-cmd--zone=public --list-ports 更新防火墙规则： firewall-cmd --reload 查看区域信息: firewall-cmd--get-active-zones 查看指定接口所属区域： firewall-cmd--get-zone-of-interface=eth0 拒绝所有包：firewall-cmd --panic-on 取消拒绝状态： firewall-cmd --panic-off 查看是否拒绝： firewall-cmd --query-panic 添加firewall-cmd --zone=public --add-port=80/tcp --permanent （--permanent永久生效，没有此参数重启后失效） 重新载入firewall-cmd --reload 查看firewall-cmd --zone=public --query-port=80/tcp 删除firewall-cmd --zone=public --remove-port=80/tcp --permanent 查看firewall是否运行,下面两个命令都可以systemctl status firewalld.service firewall-cmd --state 查看当前开了哪些端口其实一个服务对应一个端口，每个服务对应/usr/lib/firewalld/services下面一个xml文件。 firewall-cmd --list-services 查看还有哪些服务可以打开firewall-cmd --get-services 查看所有打开的端口：firewall-cmd --zone=public --list-ports 更新防火墙规则：firewall-cmd --reload]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>firewalld</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nodejs与Django图片信息传输]]></title>
    <url>%2F2018%2F07%2F05%2FNodejs%E4%B8%8EDjango%E5%9B%BE%E7%89%87%E4%BF%A1%E6%81%AF%E4%BC%A0%E8%BE%93%2F</url>
    <content type="text"><![CDATA[Nodejs与Django图片信息传输由于公司需要Nodejs的前端与Django的后端进行交互，其中涉及到图片信息作为二进制流传输，在此记录前后端分离中二进制图片在Django中的保存与转换。 Nodejs中的数据传输Nodejs采用Input插件读取图片 其中涉及到公司大牛写的Webship框架，但传送数据的方式没有大的改变 Django保存传输的二进制图片首先，我们分析request请求中所包含的信息： 通过分析，发现request中FILES属性是前端发给我们的包含图片信息的内容，我们取出FILES属性中的内容赋值给files，再进行分析，看我们需要的图片究竟是什么样的内容和格式，内容如下： 可以发现，files变量中包含name属性，即我提交的图片名字，还有一个file属性，其是一个bytes格式的变量入口，这个可能就是我需要的二进制图片，经过测试，读取这个file属性得到的二进制流和我以‘rb’模式read()提交的同一个图片所得到的二进制流相等。至此，就找到了requst中所包含的图片信息，然后将其保存到指定路径中： 完成！]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>Django</tag>
        <tag>Nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django文档——Model字段类型]]></title>
    <url>%2F2018%2F06%2F14%2FDjango%E6%96%87%E6%A1%A3%E2%80%94%E2%80%94Model%E5%AD%97%E6%AE%B5%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[字段类型(Field types)AutoField它是一个根据 ID 自增长的 IntegerField 字段。通常，你不必直接使用该字段。如果你没在别的字段上指定主 键，Django 就会自动添加主键字段。 BigIntegerField64位整数，类似于IntegerField，范围从-9223372036854775808 到9223372036854775807。默认的form widget 是TextInput。 BooleanField一个布尔值(true/false)字段。 默认的form widget是CheckboxInput。 如果要使用null作为空值，可使用NullBooleanField。 CharFieldclass CharField(max_length=None[, **options]) 它是一个字符串字段，对小字符串和大字符串都适用。 对于更大的文本，应该使用TextField 。 默认的form widget是TextInput。 CharField 有一个必须传入的参数：max_length,字段的最大字符数。它作用于数据库层级和 Django 的数据验证层级。 CommaSeparatedInterFieldclass CommaSeparatedIntegerField(max_length=None[, **options]) 它用来存放以逗号间隔的整数序列。和 CharField 一样，必须为它提供 max_length 参数。而且要注意不同数据库对 max_length 的限制。 DateFieldclass DateField([auto_now=False, auto_now_add=False, **options]) 该字段利用 Python 的 datetime.date 实例来表示日期。下面是它额外的可选参数： DateField.auto_now：每一次保存对象时，Django 都会自动将该字段的值设置为当前时间。一般用来表示 “最后修改” 时间。要注意使用的是当前日期，而并非默认值，所以 不能通过重写默认值的办法来改变保存时间。 DateField.auto_now_add：在第一次创建对象时，Django 自动将该字段的值设置为当前时间，一般用来表示对象创建时间。它使用的同样是当前日期，而非默认值。 默认的form widget是TextInput。 Note:当auto_now或者auto_now_add设置为True时，字段会有editable=True和blank=True的设定。 DateTimeFieldclass DateTimeField([auto_now=False, auto_now_add=False, **options]) 该字段利用 datetime.datetime 实例表示日期和时间。该字段所按受的参数和 DateField 一样。 默认的form widget是TextInput。Django 的admin使用两个带有 JavaScript 快捷选项TextInput分别表示日期和时间。 DecimalFieldclass DecimalField(max_digits=None, decimal_places=None[, **options]) 它是使用 Decimal 实例表示固定精度的十进制数的字段。它有两个必须的参数： DecimalField.max_digits：数字允许的最大位数 DecimalField.decimal_places：小数的最大位数 例如，要存储的数字最大值是999，而带有两个小数位，你可以使用： 1models.DecimalField(…, max_digits=5, decimal_places=2)要存储大约是十亿级且带有10个小数位的数字，就这样写： 1models.DecimalField(…, max_digits=19, decimal_places=10)默认的form widget是TextInput。 EmailFieldclass EmailField([max_length=75, **options]) 它是带有 email 合法性检测的A CharField 。 Note：最大长度默认为75，并不能存储所有与RFC3696/5321兼容的email地址。如果要存储所有，请设置 max_length=254。设置为75是历史遗留问题。 FileFieldclass FileField(upload_to=None[, max_length=100, **options]) 文件上传字段 Note：该字段不支持 primary_key 和 unique 参数，否则会抛出 TypeError 异常。 它有一个必须的参数： FileField.upload_to 用于保存文件的本地文件系统。它根据 MEDIA_ROOT 设置确定该文件的 url 属性。 该路径可以包含 时间格式串strftime()，可以在上传文件的时候替换成当时日期／时间(这样，就不会出现在上传文件把某个目录塞满的情况了)。 该参数也可以是一个可调用项，比如是一个函数，可以调用函数获得包含文件名的上传路径。这个可调用项必须要接受两个参数， 并且返回一个保存文件用的 Unix-Style 的路径(用 / 斜杠)。两个参数分别是： instance ：定义了当前 FileField 的 model 实例。更准确地说，就是以该文件为附件的 model 实例。 大多数情况下，在保存该文件时， model 实例对象还并没有保存到数据库，这是因为它很有可能使用默认的 AutoField，而此时它还没有从数据库中获得主键值。 filename ：上传文件的原始名称。在生成最终路径的时候，有可能会用到它。 还有一个可选的参数： FileField.storage 负责保存和获取文件的对象。 默认的form widget是FileInput。 Note：在 model 中使用 FileField 或 ImageField 要按照以下的步骤： 1.在项目settings文件中，你要定义 MEDIA_ROOT ，将它的值设为用来存放上传文件的目录的完整路径。(基于性能的考虑，Django 没有将文件保存在数据库中). 然后定义 MEDIA_URL ，将它的值设为表示该目录的网址。 要确保 web 服务器所用的帐号拥有对该目录的写权限。 2.在 model 里面添加 FileField 或 ImageField ，并且确认已定义了 upload_to 项，让 Django 知道应该用 MEDIA_ROOT 的哪个子目录来保存文件。 3.存储在数据库当中的仅仅只是文件的路径(而且是相对于 MEDIA_ROOT 的相对路径)。你可能已经想到利用 Django 提供的 url 这个方便的属性。举个例子，如果你的 ImageField 名称是 mug_shot，那么你可以在模板 中使用 1就能得到图片的完整网址。 例如，假设你的 MEDIA_ROOT 被设为 ‘/home/media’，upload_to 被设为 ‘photos/%Y/%m/%d’。 upload_to 中 的 ‘%Y/%m/%d’ 是一个strftime()， ‘%Y’ 是四位的年份，’%m’ 是两位的月份， ‘%d’ 是两位的日子。如果你 在2007年01月15号上传了一个文件，那么这个文件就保存在 /home/media/photos/2007/01/15 目录下。 如果你想得到上传文件的本地文件名称，文件网址，或是文件的大小，你可以使用 name, url 和 size 属性。 Note：在上传文件时，要警惕保存文件的位置和文件的类型，这么做的原因是为了避免安全漏洞。对每一个上传 文件都要验证，这样你才能确保上传的文件是你想要的文件。举个例子，如果你盲目地让别人上传文件，而没有 对上传文件进行验证，如果保存文件的目录处于 web 服务器的根目录下，万一有人上传了一个 CGI 或是 PHP 脚本，然后通过访问脚本网址来运行上传的脚本，那可就太危险了。千万不要让这样的事情发生！ 默认情况下，FileField 实例在数据库中的对应列是 varchar(100) ，和其他字段一样，你可以利用max_length 参数改变字段的最大长度。 FileField and FieldFile class FieldFile 当你访问一个Model的FileField字段时，会得到一个FieldFile的实例作为代理去访问底层文件。实例有几种属性和方法可以用来和文件数据进行互动。 FieldFile.url 通过只读的方式调用底层存储(Storage)类的 url() 方法，来访问该文件的相对URL。 FieldFile.open(mode=’rb’) 类似于python的open()方法。 FieldFile.close() 类似于python的close()方法。 FieldFile.save(name,content,save=True) 这种方法将filename和文件内容传递到该字段然后存储到该模型。该方法需要两个必须的参数：name， 文件的名称， content， 包含文件内容的对象。save 参数是可选的，主 要是控制文件修改后实例是否保存。默认是 True 。需要注意的是，content 参数是 django.core.files.File 的一个实例，不是Python的内置File对象。你可以使 用他从现有的Python文件对象中构建一个文件，如下所示： 1234from django.core.files import File Open an existing file using Python’s built-in open()f = open(‘/tmp/hello.world’)myfile = File(f)或者从字符串中构造： 12from django.core.files.base import ContentFilemyfile = ContentFile(“hello world”)FieldFile.delete(save=True) 删除此实例相关的文件，并清除该字段的所有属性。 Note：当delete()被调用时，如果文件正好是打开的，该方法将关闭文件。 save 参数是可选的，控制文件删除后实例是否保存。默认是 True 。 需要注意的是，当一个模型被删除时，相关文件不被删除。如果想删除这些孤立的文件，需要自己去处理（比如，可以手动运行命令清理，也可以通过cron来定期执行清理命令） FilePathFieldclass FilePathField(path=None[, match=None, recursive=False, max_length=100, **options]) 它是一个 CharField ，它用来选择文件系统下某个目录里面的某些文件。它有三个专有的参数，只有第一个参 数是必须的： FilePathField.path 这个参数是必需的。它是一个目录的绝对路径，而这个目录就是 FilePathField 用来选择文件的那个目录。比 如： “/home/images”. FilePathField.match 可选参数。它是一个正则表达式字符串， FilePathField 用它来过滤文件名称，只有符合条件的文件才出现在 文件选择列表中。要注意正则表达式只匹配文件名，而不是匹配文件路径。例如：”foo.*.txt$” 只匹配名为 foo23.txt 而不匹配 bar.txt 和 foo23.gif。 FilePathField.recursive 可选参数。它的值是 True 或 False。默认值是 False。它指定是否包含 path 下的子目录。 FilePathField.allow_files 该项属于Django1.5新增内容。可选参数，它的值是 True 或 False。默认值是 True。它指定是否包含指定位置的文件。该项与allow_folders 必须有一个是 True。 FilePathField.allow_folders Django1.5新增内容。可选参数，它的值是True或False。默认是False。它指定是否包含指定位置的目录。该项与allow_files必须有一个是 True。 前面已经提到了 match 只匹配文件名称，而不是文件路径。所以下面这个例子： 1FilePathField(path=”/home/images”, match=”foo.*”, recursive=True)将匹配 /home/images/foo.gif ，而不匹配 /home/images/foo/bar.gif。这是因为 match 只匹配文件名(foo.gif 和 bar.gif). 默认情况下， FilePathField 实例在数据库中的对应列是varchar(100) 。和其他字段一样，你可以利用 max_length 参数改变字段的最大长度。 FloatFieldclass FloatField([**options]) 该字段在 Python 中使用float 实例来表示一个浮点数。 默认的form widget是TextInput。 请注意FloatField与DecimalField的区别。 ImageFieldclass ImageField(upload_to=None[, height_field=None, width_field=None, max_length=100,**options]) 和 FileField 一样，只是会验证上传对象是不是一个合法的图象文件。 除了那些在 FileField 中有效的参数之外， ImageField 还可以使用 File.height and File.width 两个属性 。 它有两个可选参数： ImageField.height_field 保存图片高度的字段名称。在保存对象时，会根据该字段设定的高度，对图片文件进行缩放转换。 ImageField.width_field 保存图片宽度的字段名称。在保存对象时，会根据该字段设定的宽度，对图片文件进行缩放转换。 默认情况下， ImageField 实例对应着数据库中的varchar(100) 列。和其他字段一样，你可以使 用 max_length 参数来改变字段的最大长度。 IntegerFieldclass IntegerField([**options]) 整数字段。默认的form widget是TextInput。 IPAddressFieldclass IPAddressField([**options]) 以字符串形式(比如 “192.0.2.30”)表示 IP 地址字段。默认的form widget是TextInput。 GenericIPAddressFieldclass GenericIPAddressField([**options]) Django1.4新增。 以字符串形式(比如 “192.0.2.30”或者”2a02:42fe::4”)表示 IP4或者IP6 地址字段。默认的form widget是TextInput。 IPv6的地址格式遵循RFC 4291 section 2.2。比如如果这个地址实际上是IPv4的地址，后32位可以用10进制数表示，例如 “::ffff:192.0.2.0”。 2001:0::0:01可以写成2001::1,而::ffff:0a0a:0a0a可以写成::ffff:10.10.10.10。字母都为小写。 GenericIPAddressField.protocol 验证输入协议的有效性。默认值是 ‘both’ 也就是IPv4或者IPv6。该项不区分大小写。 GenericIPAddressField.unpack_ipv4 解释IPv4映射的地址，像 ::ffff:192.0.2.1 。如果启用该选项，该地址将必解释为 192.0.2.1 。默认是禁止的。只有当 protocol 被设置为 ‘both’ 时才可以启用。 NullBooleanFieldclass NullBooleanField([**options]) 与 BooleanField 相似，但多了一个 NULL 选项。建议用该字段代替使用 null=True 选项的 BooleanField 。 默认的form widget是NullBooleanSelect。 PositiveIntegerFieldclass PositiveIntegerField([**options]) 和 IntegerField 相似，但字段值必须是非负数。 PositiveSmallIntegerFieldclass PositiveSmallIntegerField([**options]) 和 PositiveIntegerField 类似，但数值的取值范围较小，受限于数据库设置。 SlugFieldclass SlugField([max_length=50, **options]) Slug 是一个新闻术语，是指某个事件的短标签。它只能由字母，数字，下划线或连字符组成。通赏情况下，它被用做网址的一部分。 和 CharField 类似，你可以指定 max_length (要注意数据库兼容性和本节提到的 max_length )。如果没有指定 max_length ，Django 会默认字段长度为50。 该字段会自动设置 Field.db_index to True。 基于其他字段的值来自动填充 Slug 字段是很有用的。你可以在 Django 的管理后台中使用prepopulated_fields 来做到这一点。 SmallIntegerFieldclass SmallIntegerField([**options]) 和 IntegerField 类似，但数值的取值范围较小，受限于数据库的限制。 TextFieldclass TextField([**options]) 大文本字段。默认的form widget是Textarea。 TimeFieldclass TimeField([auto_now=False, auto_now_add=False, **options]) 该字段使用 Python 的 datetime.time 实例来表示时间。它和 DateField 接受同样的自动填充的参数。 默认的form widget是TextInput。 URLFieldclass URLField([max_length=200, **options]) 保存 URL 的 CharField 。 和所有 CharField 子类一样，URLField 接受可选的 max_length 参数，该参数默认值是200。]]></content>
  </entry>
  <entry>
    <title><![CDATA[点滴积累]]></title>
    <url>%2F2018%2F06%2F11%2F%E7%82%B9%E6%BB%B4%E7%A7%AF%E7%B4%AF%2F</url>
    <content type="text"><![CDATA[hexo生成博文插入图片 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true 在你的hexo目录下执行这样一句话npm install hexo-asset-image –save 等待一小段时间后，再运行hexo n “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹 最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片： ! [你想输入的替代文字](xxxx/图片名.jpg) 注意： xxxx是这个md文件的名字，也是同名文件夹的名字。只需要有文件夹名字即可，不需要有什么绝对路径。你想引入的图片就只需要放入xxxx这个文件夹内就好了，很像引用相对路径。 Error: That port is already in use.的错误。即端口号已经被占用,说明servr已经在运行了(也有可能在后台运行) 那么找到该进程,kill掉即可. 或者最简单的解决方法就是： 在终端输入 sudo fuser -k 8000/tcp 这样和端口8000相关的进程就都关了。 Centos下实现word转pdflibreoffice –headless –invisible –convert-to pdf 模版123.docx –outdir /filepath 爆破大数据平台Nodejs后端 1 创建数据库 1 使用redis、mongodb 2 使用Mysql 其中有DATATIME属性 2 使用Admzip以及正则表达式实现文档的替换 3 将生成的文档转换为PDF Django后端 1 设计Model 2 前后端分离传递数据 3 算法的嵌入 涉及算法 1 图像识别 2 自动布孔 Github学习资源 https://morvanzhou.github.io/（莫烦） https://cn.wordpress.org/（博客主题） https://blog.evjang.com http://bamos.github.io/ GAN生成对抗网络 GAN多种网络分析http://nooverfit.com/wp/%E7%8B%AC%E5%AE%B6%EF%BD%9Cgan%E5%A4%A7%E7%9B%98%E7%82%B9%EF%BC%8C%E8%81%8A%E8%81%8A%E8%BF%99%E4%BA%9B%E5%B9%B4%E7%9A%84%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-lsgan-wgan-cgan-info/ DCGAN代码：https://github.com/carpedm20/DCGAN-tensorflow tensorflowhttps://github.com/jacobgil/keras-dcgan keras 论文资料：https://github.com/zhangqianhui/AdversarialNetsPapers DCGAN、WGAN、WGAN-GP、LSGAN、BEGAN原理总结及对比：https://blog.csdn.net/qq_25737169/article/details/78857788 WGAN-GP：https://github.com/caogang/wgan-gphttps://github.com/tjwei/GANotebookshttps://github.com/jalola/improved-wgan-pytorchhttps://blog.csdn.net/omnispace/article/details/54942668(博客介绍) BEGAN全称是Boundary Equilibrium GANs：https://github.com/carpedm20/BEGAN-tensorflowhttps://github.com/Heumi/BEGAN-tensorflowhttps://github.com/carpedm20/BEGAN-pytorch Keras implementation of Image OutPainting：https://github.com/bendangnuksung/Image-OutPainting WGAN-GP与WGAN及GAN的比较：https://blog.csdn.net/qq_38826019/article/details/80786061 待查询问题 基于动量的优化算法（包括momentum和Adam） RMSProp的基本概念]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>日常记录</tag>
        <tag>小技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习——EM算法]]></title>
    <url>%2F2018%2F06%2F10%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94EM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[EM算法EM简介EM(Expectation Mmaximization) 是一种迭代算法， 用于含隐变量(Latent Variable) 的概率模型参数的极大似然估计， 或极大后验概率估计 EM算法由两步组成， 求期望的E步，和求极大的M步。EM算法可以看成是特殊情况下计算极大似然的一种算法。现实的数据经常有一些比较奇怪的问题，比如缺失数据、含有隐变量等问题。当这些问题出现的时候，计算极大似然函数通常是比较困难的，而EM算法可以解决这个问题。 EM算法已经有很多应用，比如最经典的Hidden Markov模型等。经济学中，除了逐渐开始受到重视的HMM模型（例如Yin and Zhao, 2015），其他领域也有可能涉及到EM算法，比如在Train的《Discrete Choice Methods with Simulation》就给出了一个 $mixed logit$ 模型的EM算法。 EM算法的预备知识 极大似然估计 1 举例说明：经典问题——学生身高问题 我们需要调查我们学校的男生和女生的身高分布。 假设你在校园里随便找了100个男生和100个女生。他们共200个人。将他们按照性别划分为两组，然后先统计抽样得到的100个男生的身高。假设他们的身高是服从高斯分布的。但是这个分布的均值u和方差∂2我们不知道，这两个参数就是我们要估计的。记作θ=[u, ∂]T。问题：我们知道样本所服从的概率分布的模型和一些样本，而不知道该模型中的参数。我们已知的有两个：（1）样本服从的分布模型（2）随机抽取的样本 需要通过极大似然估计求出的包括：模型的参数总的来说：极大似然估计就是用来估计模型参数的统计学方法。 2 如何估计 问题数学化： (1)样本集: x={$x_1,x_2,…,x_N$}, $N=100$。 (2)概率密度：$p(x_i|\theta)$ 抽到男生$i$（的身高）的概率 100个样本之间独立同分布，所以我同时抽到这100个男生的概率就是他们各自概率的乘积。就是从分布是$p(x|\theta)$ 的总体样本中抽取到这100个样本的概率，也就是样本集X中各个样本的联合概率，用下式表示：$$L(\theta)=L(x_1,…,x_n;\theta)=\prod_{i=1}^n{p(x_i|\theta)},\theta\in\phi$$这个概率反映了，在概率密度函数的参数是$\theta$时，得到X这组样本的概率。 需要找到一个参数θ，其对应的似然函数$L(\theta)$最大，也就是说抽到这100个男生（的身高）概率最大。这个叫做$\theta$的最大似然估计量，记为$$argmaxL(\theta)$$ 3 求最大似然函数估计值的一般步骤 首先，写出似然函数：$$L(\theta)=L(x_1,…,x_n;\theta)=\prod_{i=1}^n{p(x_i|\theta)},\theta\in\phi$$其次，对似然函数取对数，并整理：$$H(\theta)=lnL(\theta)=ln\prod_{i=1}^n{p(x_i|\theta)}=\sum_{i=1}^n{lnp(x_i|\theta)}$$然后，求导数，令导数为0，得到似然方程；最后，解似然方程，得到的参数即为所求。 4 总结 多数情况下我们是根据已知条件来推算结果，而极大似然估计是已经知道了结果，然后寻求使该结果出现的可能性最大的条件，以此作为估计值。]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>EM算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS环境搭建]]></title>
    <url>%2F2018%2F06%2F05%2FCentOS%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[系统安装电脑配置 微星1080Ti 至强E5 -2620v4 技嘉的主板 踩坑记录将CentOS 7.4镜像刻到U盘之后，向服务器安装时，使用U盘启动会出现两种启动选项，一种是UEFI启动选项，一种是默认的启动选项，如果不使用UEFI方式安装，那么一般是没有问题的，如果选择UEFI方式安装系统，那么引导系统时会出现如下的提示： [sdb] No Caching mode page found [sdb] Assuming drive cache:write through Could not boot /dev/root does not exist 然后命令行就卡在这了，现在只需要耐心等待，等一会之后会不断的滚动错误警告，这个时候继续等待，那么一会就会出来命令行输入界面，这个时候输入以下命令： ls /dev/sd* 输入命令之后会列出所有的存储设备，这个时候一般情况下第一块硬盘是sda，如果有多个分区，那么依次就是sda1、sda2等等，如果有两块硬盘那么就是sdb，U盘一般是排最后的号，如果有一块硬盘，那么U盘就是sdb，如果有两块硬盘，那么U盘就是sdc，U盘一般会有sdc和sdc4两个选项，sdc属于U盘存储，sdc4就是镜像所在分区了，这样一般是没有问题的，如果出现问题，那么接下来多配置几次就好了，接下来输入命令reboot重启计算机，在安装界面，先不要选择安装，这个时候按一下e键，会进入编辑界面，移动光标进行如下修改： 在第二行默认是：vmlinuz initrd=initrd.img inst.stage2=hd:LABEL=CentOS\x207\x20x86_64 rd.live.check quiet 把这行修改为：vmlinuz initrd=initrd.img inst.stage2=hd:/dev/sdc4:/ quiet 就是把hd:和quiet之间的内容修改为U盘镜像所在位置这样就可以了，注意要写成/dev/sdc4:/ 然后根据提示按Ctrl+X键就可以开始安装了，现在就正常进入安装界面了 NVIDIA驱动安装1、在官网上http://www.geforce.cn/drivers搜索到对应型号的显卡驱动并下载，下载到的驱动文件是一个后缀名为.run的文件（例如NVIDIA-Linux-x86_64-384.98.run）； 2、安装gcc编译环境以及内核相关的包： yum install kernel-devel kernel-doc kernel-headers gcc* glibc* glibc-*注意：安装内核包时需要先检查一下当前内核版本是否与所要安装的kernel-devel/kernel-doc/kernel-headers的版本一致，请务必保持两者版本一致，否则后续的编译过程会出问题。 3、禁用系统默认安装的 nouveau 驱动，修改/etc/modprobe.d/blacklist.conf 文件： 修改配置echo -e &quot;blacklist nouveau\noptions nouveau modeset=0&quot; &gt; /etc/modprobe.d/blacklist.conf 备份原来的镜像文件mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.bak 重建新镜像文件dracut /boot/initramfs-$(uname -r).img $(uname -r) 重启reboot 在命令行界面init 5 查看nouveau是否启动，如果结果为空即为禁用成功lsmod | grep nouveau 4、安装DKMS模块 DKMS全称是DynamicKernel ModuleSupport，它可以帮我们维护内核外的驱动程序，在内核版本变动之后可以自动重新生成新的模块。 sudo yum install DKMS 5、执行显卡驱动安装脚本（如果内核版本一致，就不需要指定–kernel-source-path和-k） ./NVIDIA-Linux-x86_64-384.98.run --kernel-source-path=/usr/src/kernels/3.10.0-693.11.1.el7.x86_64/ -k $(uname -r) --dkms -s 6、若步骤5执行过程中没报错，则安装成功。重启，执行nvidia-smi可查看相关信息。如若出现重启系统驱动找不到的情况，在装完驱动后，切记，先不要重启，使用 init5 和 init 3 交替切换，几次后，会进入图形界面（其中init 5为进入图形界面的命令），之后，在图形界面，重新编译一下启动项。 CUDA&amp;&amp;CUDNN 关于cuda和cudnn的安装，这一点尤其要注意。我在CentOS7.5(更新后的版本，初始装的时候为7.4)上安装CUDA9.2时，无法与NVIDIA的驱动匹配，因此退而求其次，选择了CUDA9.1，同时CUDNN选择7.0.5版本。注意 不要下载cudnn-9.1-linux-ppc64le-v7.1.tgz，因为ppc64le并不是针对X64的电脑系统CUDA的安装我选择了使用yum安装的方式，因为之前使用命令行，出现了未知的错误，导致系统重装。]]></content>
  </entry>
  <entry>
    <title><![CDATA[学习的东西]]></title>
    <url>%2F2018%2F05%2F30%2F%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[function doDecrypt (pwd, onError) { console.log('in doDecrypt'); const txt = document.getElementById('enc_content').innerHTML; let plantext; try { const bytes = CryptoJS.AES.decrypt(txt, pwd); var plaintext = bytes.toString(CryptoJS.enc.Utf8); } catch(err) { if(onError) { onError(err); } return; } document.getElementById('enc_content').innerHTML = plaintext; document.getElementById('enc_content').style.display = 'block'; document.getElementById('enc_passwd').style.display = 'none'; if(typeof MathJax !== 'undefined') { MathJax.Hub.Queue( ['resetEquationNumbers', MathJax.InputJax.TeX], ['PreProcess', MathJax.Hub], ['Reprocess', MathJax.Hub] ); } } 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 var onError = function(error) { document.getElementById("enc_error").innerHTML = "password error!" }; function decrypt() { var passwd = document.getElementById("enc_pwd_input").value; console.log(passwd); doDecrypt(passwd, onError); }]]></content>
      <categories>
        <category>感悟</category>
      </categories>
      <tags>
        <tag>坚持</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django的网站逻辑]]></title>
    <url>%2F2018%2F05%2F29%2FDjango%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[Django实战——后台逻辑Django用户 （登录 注册 找回密码）登录 后台的操作放在对应app的views文件下，当我们在路由中添加一个url，django会自动为我们生成一个request，并添加到函数里面。首先判断请求方法，是POST还是GET。然后跳转到对应的页面进行操作。 对登录账户进行验证（采用类来做） 得到用户名和密码后，使用django.contrib.auth.authenticate进行验证，验证成功的话得到一个对象，然后进行对应后台逻辑的编写，即调用django.contrib.auth.login进行验证。 对登录成功后返回index.html文件的状态处理，需要在html文件中进行判断，用户是否登录，调用request.user.user.is_authenticated来进行判断。决定显示哪一行代码。 自定义认证方法，实现邮箱的登录方式（重定义方式） Seesion和Cookie机制 无状态请求 有状态请求 注册 准备工具 添加插件（captcha） 提交注册信息（包括注册码）发送邮件验证注册信息激活账户 找回密码 采用类似于激活账户的方式来实现]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Adm-zip工具]]></title>
    <url>%2F2018%2F05%2F29%2FAdm-zip%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[Adm-zip介绍 Adm-zip是JavaScript中的一款与压缩文件相关的插件，其功能相当的强大（我看来），我用其实现了对Word文档的内容替换。 Word 文档本质上是一个压缩 文件夹，其中的word文件夹下的document.xml文件是包含文档内容的文件，而我们需要操作的也正是这个文件。 Adm-zip这款插件则正好满足我们即对压缩文件内部条目文件的处理，同时又保证不影响压缩文件内部其余文件的要求。 我们需要的函数接口主要有四个，分别为： 读取压缩文件内指定目录里面的文件或者文件夹： Admzip.readAsText() 删除压缩文件内的指定文件或者文件夹： Admzip.deleteFile() 将指定文件写入到压缩文件夹中： Admzip.addFile() 将所做的更改重新写入文件（可以是当前文件，也可以重命名的word文档） Admzip.writeZip() 关于Adm-zip的使用方法，暂时只发现了这样一种，其还有别的Api接口，有兴趣的小伙伴可以自己再研究下^_^]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>Adm-zip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MARKDOWN语法熟悉]]></title>
    <url>%2F2018%2F05%2F28%2FMARKDOWN%E8%AF%AD%E6%B3%95%E7%86%9F%E6%82%89%2F</url>
    <content type="text"><![CDATA[基础标题测试一级标题二级标题三级标题四级标题五级标题六级标题换行和分段未换行未换行测试示例 换行后已换行测试（后有两个空格）示例 分段分段测试 分段 文本样式加粗(使用两个*号)斜体（使用一个号）*删除线(使用两个波浪线)‘底纹（单引号）’ 列表在Markdown 下，无序列表直接在文字前加 「 - 」 或者 「 * 」 即可，有序列表则直接在文字前加 「1.」「2.」「3.」 。符号要和文字之间加上一个字符的空格。 无序列表： 在文本前加 「 」 即可生成一个无序列表。快捷键：control + L （只能生成列表，不能生成子列表）在 「 」 前加两个空格键或者一个 tab 键就可以产生一个子列表。有序列表： 在文本前加 「字母.」 或 「数字.」 即可生成一个有序列表。注意，当你第一个序号使用什么作为标记的，那么同级别的列表就会自动使用其作为标记。 无序列表 1 2 2.1 2.1.1 2.1.1.1 3 有序列表 1 2 a. 2.1 b. 2.2 3 有序列表与无序列表混排 1 2 a. 2.1 b. 2.2* 2.2.1 引用只要在文本内容之前加 「 &gt; （大于号）」 即可将文本变成引用文本。 这是引用文本 图片与链接图片 链接Mou 水平线三个「 - 」或「 * 」都可以画出一条水平分割线 使用（—）的水平分割线 使用（***）的水平分割线 代码框两对「 123456代码前加四个空格键 代码前加一个 tab 键### 两对‘ ``` ’包裹```print(&apos;Hello Word!&apos;); 四个空格print(&apos;Hello Word!&apos;); 一个 tab 键print(&apos;Hello Word!&apos;); 脚注脚注总是成对出现的，「 [^1] 」作为标记，可以点击跳至末尾注解。「 [^1]: 」填写注解，不论写在什么位置，都会出现在文章的末尾。 点击右上方的小数字有注解$[^1]$ $[^1] :$这里是注解这是随机文本这是随机文本这是随机文本 注释注释是给自己看的，预览时也不会出现，当然发布出去别人也不会看见。 首行缩进关于首行缩进，网上争议很多，而技术本身并没有错，不是吗？在输入法的「全角」模式下，输入两个空格键即可。 引号在网页上写文章建议使用直角引号『「」』。 利用Markdown创建表格Markdown作为一种轻量级书写/写作语言，并没有提供很好的排版、编辑等功能。因此，如果想要利用Markdown创建表格（特别是复杂表格），其实是一项不太轻松的事情。经过笔者在简书平台上的测试与其他若干帖子的表述，Markdown应是只提供了最简单的创建表格与内容对齐方式的功能。总结而言，有如下两种最为直观的创建表格方式: 简单方式Name | Academy | score - | :-: | -: Harry Potter | Gryffindor| 90 Hermione Granger | Gryffindor | 100 Draco Malfoy | Slytherin | 90 Name Academy score Harry Potter Gryffindor 90 Hermione Granger Gryffindor 100 Draco Malfoy Slytherin 90 原生方式| Name | Academy | score | | - | :-: | -: | | Harry Potter | Gryffindor| 90 | | Hermione Granger | Gryffindor | 100 | | Draco Malfoy | Slytherin | 90 | Name Academy score Harry Potter Gryffindor 90 Hermione Granger Gryffindor 100 Draco Malfoy Slytherin 90 语法说明： 不管是哪种方式，第一行为表头，第二行分隔表头和主体部分，第三行开始每一行代表一个表格行； 列与列之间用管道符号 “|” 隔开，原生方式的表格每一行的两边也要有管道符。 可在第二行指定不同列单元格内容的对齐方式，默认为左对齐，在 “-” 右边加上 “:” 为右对齐，在 “-” 两侧同时加上 “:” 为居中对齐。 这样傻瓜的表格创建方式十分符合Markdown简小精悍的语言气质，具有上手快、即学即用的优势。但傻瓜的定义方式显然不能满足很多处女座的要求，比如文章——“Linux备忘录-Linux中文件/文件夹按照时间顺序升序/降序排列”的表格如下： | 参数 |详细解释|备注| | - | :-: | -: | | -l | use a long listing format |以长列表方式显示（显示出文件/文件夹详细信息） | | -t | sort by modification time |按照修改时间排序（默认最近被修改的文件/文件夹排在最前面） | |-r | reverse order while sorting |逆序排列| 参数 详细解释 备注 -l use a long listing format 以长列表方式显示（显示出文件/文件夹详细信息） -t sort by modification time 按照修改时间排序（默认最近被修改的文件/文件夹排在最前面） -r reverse order while sorting 逆序排列 单元格排列不齐整、第一列太窄而第三列略宽，如此不堪的视觉效果着实让强迫症患者们难以忍受。还好，利用HTML可以弥补Markdown这一缺陷，甚至可以在创建表格时其他诸多表现方面锦上添花。 Markdown 添加 MathJax 数学公式添加公式的方法行内公式 $行内公式$ 行间公式 $$行间公式$$ MathJax 数学公式语法呈现位置注意: 在公式的前一行和后一行，要注意空一行，否则公式会出错。 所有公式定义格式为 \$…$ 具体语句例如 \$\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$ 显示为： $\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$ 居中并放大显示 \$\$\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$$ 显示为： $$\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$$ 希腊字母 显示 命令 显示 命令 α \$\alpha\$ β \$\beta\$ γ \$\gamma\$ δ \$\delta\$ ϵ \$\epsilon\$ ζ \$\zeta\$ η \$\eta\$ θ \$\theta\$ ι \$\iota\$ κ \$\kappa\$ λ \$\lambda\$ μ \$\mu\$ ν \$\nu\$ ξ \$\xi\$ π \$\pi\$ ρ \$\rho\$ σ \$\sigma\$ τ \$\tau\$ υ \$\upsilon\$ ϕ \$\phi\$ χ \$\chi\$ ψ \$\psi\$ ω \$\omega\$ 如果需要大写的希腊字母，只需将命令的首字母大写即可(有的字母没有大写)，如 \$\gamma$ &amp; \$\Gamma$ $\gamma$ &amp; $\Gamma$ 若需要斜体希腊字母，在命令前加上var前缀即可(大写可斜)，如 \$\Gamma$ &amp; \$\varGamma$ $\Gamma$ &amp; $\varGamma$ 字母修饰上下标 上标：^ 下标：_ \$C_n^2$ $$C_n^2$$ 矢量 例1 \$\vec a$ $\vec a$ 例2 \$\overrightarrow a$ $\overrightarrow xy$ 字体 - Typewriter\$\mathtt {ABCDEFGHIJKLMNOPQRSTUVWXYZ}$ $\mathtt {ABCDEFGHIJKLMNOPQRSTUVWXYZ}$ \$\mathbb {ABCDEFGHIJKLMNOPQRSTUVWXYZ}$ $\mathbb {ABCDEFGHIJKLMNOPQRSTUVWXYZ}$ \$\mathsf {ABCDEFGHIJKLMNOPQRSTUVWXYZ}$ $\mathsf {ABCDEFGHIJKLMNOPQRSTUVWXYZ}$ 分组 - {}有分组功能，如\$10^{10}\$ \&amp; \$10^10\$$10^{10}$ &amp; $10^10$ 括号 小括号：\$()$呈现为 $()$ 中括号：\$[]$呈现为 $[]$ 尖括号：\$\langle\rangle$呈现为 $\langle\rangle$ - 此处为与分组符号{}相区别，使用转义字符\ 使用\left(或\right)使符号大小与邻近的公式相适应；该语句适用于所有括号类型 \$(\frac{x}{y})$呈现为 $(\frac{x}{y})$ 而\$\left(\frac{x}{y}\right)$呈现为 $\left(\frac{x}{y}\right)$ 注意: 在公式的前后，必须留有一个空格或者换行，否则无法识别。 求和、极限与积分求和：\sum 举例：\$\sum_{i=1}^n{a_i}$ $\sum_{i=1}^n{a_i}$ 极限：\$\lim_{x\to 0}$ $\lim_{x\to 0}$ 积分：\$\int$ $\int$ 举例：\$\int_0^\infty{fxdx}$ $\int_0^\infty{fxdx}$ \$\iint$ $\iint$ \$\iiint$ $\iiint$ 连乘：\$\prod$ $\prod$ 分式与根式分式(fractions)：\$\frac{公式1}{公式2}$ $\frac{公式1}{公式2}$ 根式：\$\sqrt[x]{y}$ $\sqrt[x]{y}$ 特殊符号 显示 命令 ∞ \$\infty$ ∪ \$\cup$ ∩ \$\cap$ ⊂ \$\subset$ ⊆ \$\subseteq$ ⊃ \$\supset$ ∈ \$\in$ ∉ \$\notin$ ∅ \$\varnothing$ ∀ \$\forall$ ∃ \$\exists$ ¬ \$\lnot$ ∇ \$\nabla$ ∂ \$\partial$ 空格 LaTeX语法本身会忽略空格的存在 小空格：\$a\ b$呈现为 $a\ b$ 4格空格：\$a\quad b$呈现为 $a\quad b$ 矩阵边框在起始、结束标记处用下列词替换matrix pmatrix：小括号边框 bmatrix：中括号边框 Bmatrix：大括号边框 vmatrix：单竖线边框 Vmatrix：双竖线边框 省略元素横省略号：\cdots竖省略号：\vdots斜省略号：\ddots举例 $$\begin{bmatrix}{a_{11}}&amp;{a_{12}}&amp;{\cdots}&amp;{a_{1n}}\\\ {a_{21}}&amp;{a_{22}}&amp;{\cdots}&amp;{a_{2n}}\\\ {\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\\ {a_{m1}}&amp;{a_{m2}}&amp;{\cdots}&amp;{a_{mn}}\\\ \end{bmatrix}$$ $$\begin{bmatrix}{a_{11}}&amp;{a_{12}}&amp;{\cdots}&amp;{a_{1n}}\\ {a_{21}}&amp;{a_{22}}&amp;{\cdots}&amp;{a_{2n}}\\ {\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\ {a_{m1}}&amp;{a_{m2}}&amp;{\cdots}&amp;{a_{mn}}\\ \end{bmatrix}$$ 阵列需要array环境：起始、结束处以{array}声明 对齐方式：在{array}后以{}逐行统一声明 左对齐：l；居中：c；右对齐：r 竖直线：在声明对齐方式时，插入|建立竖直线 插入水平线：\hline 方程组需要cases环境：起始、结束处以{cases}声明举例 $$\begin{cases} a_1x+b_1y+c_1z=d_1\\\ a_2x+b_2y+c_2z=d_2\\\ a_3x+b_3y+c_3z=d_3\\\ \end{cases}$$ $$\begin{cases} a_1x+b_1y+c_1z=d_1\\ a_2x+b_2y+c_2z=d_2\\ a_3x+b_3y+c_3z=d_3\\ \end{cases}$$]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具 Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一个博客！]]></title>
    <url>%2F2018%2F05%2F26%2F%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8D%9A%E5%AE%A2%EF%BC%81%2F</url>
    <content type="text"><![CDATA[第一个博客从这个博客开始，新的学习阶段开启了！]]></content>
      <categories>
        <category>记录</category>
      </categories>
      <tags>
        <tag>NoteBook</tag>
      </tags>
  </entry>
</search>
