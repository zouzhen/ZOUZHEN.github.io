<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[pandas常用方法整理]]></title>
    <url>%2F2019%2F04%2F02%2Fpandas%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[pandas dataframe的合并（append, merge, concat）创建2个DataFrame： &gt;&gt;&gt; df1 = pd.DataFrame(np.ones((4, 4))*1, columns=list(&apos;DCBA&apos;), index=list(&apos;4321&apos;)) &gt;&gt;&gt; df2 = pd.DataFrame(np.ones((4, 4))*2, columns=list(&apos;FEDC&apos;), index=list(&apos;6543&apos;)) &gt;&gt;&gt; df3 = pd.DataFrame(np.ones((4, 4))*3, columns=list(&apos;FEBA&apos;), index=list(&apos;6521&apos;)) &gt;&gt;&gt; df1 D C B A 4 1.0 1.0 1.0 1.0 3 1.0 1.0 1.0 1.0 2 1.0 1.0 1.0 1.0 1 1.0 1.0 1.0 1.0 &gt;&gt;&gt; df2 F E D C 6 2.0 2.0 2.0 2.0 5 2.0 2.0 2.0 2.0 4 2.0 2.0 2.0 2.0 3 2.0 2.0 2.0 2.0 &gt;&gt;&gt; df3 F E B A 6 3.0 3.0 3.0 3.0 5 3.0 3.0 3.0 3.0 2 3.0 3.0 3.0 3.0 1 3.0 3.0 3.0 3.0 1，concatpd.concat(objs, axis=0, join=’outer’, join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True) 示例： &gt;&gt;&gt; pd.concat([df1, df2]) A B C D E F 4 1.0 1.0 1.0 1.0 NaN NaN 3 1.0 1.0 1.0 1.0 NaN NaN 2 1.0 1.0 1.0 1.0 NaN NaN 1 1.0 1.0 1.0 1.0 NaN NaN 6 NaN NaN 2.0 2.0 2.0 2.0 5 NaN NaN 2.0 2.0 2.0 2.0 4 NaN NaN 2.0 2.0 2.0 2.0 3 NaN NaN 2.0 2.0 2.0 2.0 1.1，axis默认值：axis=0 axis=0：竖方向（index）合并，合并方向index作列表相加，非合并方向columns取并集 axis=1：横方向（columns）合并，合并方向columns作列表相加，非合并方向index取并集 axis=0： &gt;&gt;&gt; pd.concat([df1, df2], axis=0) A B C D E F 4 1.0 1.0 1.0 1.0 NaN NaN 3 1.0 1.0 1.0 1.0 NaN NaN 2 1.0 1.0 1.0 1.0 NaN NaN 1 1.0 1.0 1.0 1.0 NaN NaN 6 NaN NaN 2.0 2.0 2.0 2.0 5 NaN NaN 2.0 2.0 2.0 2.0 4 NaN NaN 2.0 2.0 2.0 2.0 3 NaN NaN 2.0 2.0 2.0 2.0 axis=1： &gt;&gt;&gt; pd.concat([df1, df2], axis=1) D C B A F E D C 1 1.0 1.0 1.0 1.0 NaN NaN NaN NaN 2 1.0 1.0 1.0 1.0 NaN NaN NaN NaN 3 1.0 1.0 1.0 1.0 2.0 2.0 2.0 2.0 4 1.0 1.0 1.0 1.0 2.0 2.0 2.0 2.0 5 NaN NaN NaN NaN 2.0 2.0 2.0 2.0 6 NaN NaN NaN NaN 2.0 2.0 2.0 2.0 备注：原df中，取并集的行/列名称不能有重复项，即axis=0时columns不能有重复项，axis=1时index不能有重复项： &gt;&gt;&gt; df1.columns = list(&apos;DDBA&apos;) &gt;&gt;&gt; pd.concat([df1, df2], axis=0) ValueError: Plan shapes are not aligned 1.2，join默认值：join=‘outer’ 非合并方向的行/列名称：取交集（inner），取并集（outer）。 axis=0时join=’inner’，columns取交集： &gt;&gt;&gt; pd.concat([df1, df2], axis=0, join=&apos;inner&apos;) D C 4 1.0 1.0 3 1.0 1.0 2 1.0 1.0 1 1.0 1.0 6 2.0 2.0 5 2.0 2.0 4 2.0 2.0 3 2.0 2.0 axis=1时join=’inner’，index取交集： &gt;&gt;&gt; pd.concat([df1, df2], axis=1, join=&apos;inner&apos;) D C B A F E D C 4 1.0 1.0 1.0 1.0 2.0 2.0 2.0 2.0 3 1.0 1.0 1.0 1.0 2.0 2.0 2.0 2.0 1.3，join_axes默认值：join_axes=None，取并集 合并后，可以设置非合并方向的行/列名称，使用某个df的行/列名称 axis=0时join_axes=[df1.columns]，合并后columns使用df1的： &gt;&gt;&gt; pd.concat([df1, df2], axis=0, join_axes=[df1.columns]) D C B A 4 1.0 1.0 1.0 1.0 3 1.0 1.0 1.0 1.0 2 1.0 1.0 1.0 1.0 1 1.0 1.0 1.0 1.0 6 2.0 2.0 NaN NaN 5 2.0 2.0 NaN NaN 4 2.0 2.0 NaN NaN 3 2.0 2.0 NaN NaN axis=1时axes=[df1.index]，合并后index使用df2的： &gt;&gt;&gt; pd.concat([df1, df2], axis=1, join_axes=[df1.index]) D C B A F E D C 4 1.0 1.0 1.0 1.0 2.0 2.0 2.0 2.0 3 1.0 1.0 1.0 1.0 2.0 2.0 2.0 2.0 2 1.0 1.0 1.0 1.0 NaN NaN NaN NaN 1 1.0 1.0 1.0 1.0 NaN NaN NaN NaN 同时设置join和join_axes的，以join_axes为准： &gt;&gt;&gt; pd.concat([df1, df2], axis=0, join=&apos;inner&apos;, join_axes=[df1.columns]) D C B A 4 1.0 1.0 1.0 1.0 3 1.0 1.0 1.0 1.0 2 1.0 1.0 1.0 1.0 1 1.0 1.0 1.0 1.0 6 2.0 2.0 NaN NaN 5 2.0 2.0 NaN NaN 4 2.0 2.0 NaN NaN 3 2.0 2.0 NaN NaN 1.4，ignore_index默认值：ignore_index=False 合并方向是否忽略原行/列名称，而采用系统默认的索引，即从0开始的int。 axis=0时ignore_index=True，index采用系统默认索引： &gt;&gt;&gt; pd.concat([df1, df2], axis=0, ignore_index=True) A B C D E F 0 1.0 1.0 1.0 1.0 NaN NaN 1 1.0 1.0 1.0 1.0 NaN NaN 2 1.0 1.0 1.0 1.0 NaN NaN 3 1.0 1.0 1.0 1.0 NaN NaN 4 NaN NaN 2.0 2.0 2.0 2.0 5 NaN NaN 2.0 2.0 2.0 2.0 6 NaN NaN 2.0 2.0 2.0 2.0 7 NaN NaN 2.0 2.0 2.0 2.0 axis=1时ignore_index=True，columns采用系统默认索引： &gt;&gt;&gt; pd.concat([df1, df2], axis=1, ignore_index=True) 0 1 2 3 4 5 6 7 1 1.0 1.0 1.0 1.0 NaN NaN NaN NaN 2 1.0 1.0 1.0 1.0 NaN NaN NaN NaN 3 1.0 1.0 1.0 1.0 2.0 2.0 2.0 2.0 4 1.0 1.0 1.0 1.0 2.0 2.0 2.0 2.0 5 NaN NaN NaN NaN 2.0 2.0 2.0 2.0 6 NaN NaN NaN NaN 2.0 2.0 2.0 2.0 1.5，keys默认值：keys=None 可以加一层标签，标识行/列名称属于原来哪个df。 axis=0时设置keys： &gt;&gt;&gt; pd.concat([df1, df2], axis=0, keys=[&apos;x&apos;, &apos;y&apos;]) A B C D E F x 4 1.0 1.0 1.0 1.0 NaN NaN 3 1.0 1.0 1.0 1.0 NaN NaN 2 1.0 1.0 1.0 1.0 NaN NaN 1 1.0 1.0 1.0 1.0 NaN NaN y 6 NaN NaN 2.0 2.0 2.0 2.0 5 NaN NaN 2.0 2.0 2.0 2.0 4 NaN NaN 2.0 2.0 2.0 2.0 3 NaN NaN 2.0 2.0 2.0 2.0 axis=1时设置keys： &gt;&gt;&gt; pd.concat([df1, df2], axis=1, keys=[&apos;x&apos;, &apos;y&apos;]) x y D C B A F E D C 1 1.0 1.0 1.0 1.0 NaN NaN NaN NaN 2 1.0 1.0 1.0 1.0 NaN NaN NaN NaN 3 1.0 1.0 1.0 1.0 2.0 2.0 2.0 2.0 4 1.0 1.0 1.0 1.0 2.0 2.0 2.0 2.0 5 NaN NaN NaN NaN 2.0 2.0 2.0 2.0 6 NaN NaN NaN NaN 2.0 2.0 2.0 2.0 也可以传字典取代keys： &gt;&gt;&gt; pd.concat({&apos;x&apos;: df1, &apos;y&apos;: df2}, axis=0) A B C D E F x 4 1.0 1.0 1.0 1.0 NaN NaN 3 1.0 1.0 1.0 1.0 NaN NaN 2 1.0 1.0 1.0 1.0 NaN NaN 1 1.0 1.0 1.0 1.0 NaN NaN y 6 NaN NaN 2.0 2.0 2.0 2.0 5 NaN NaN 2.0 2.0 2.0 2.0 4 NaN NaN 2.0 2.0 2.0 2.0 3 NaN NaN 2.0 2.0 2.0 2.0 1.6，levels默认值：levels=None 明确行/列名称取值范围： &gt;&gt;&gt; pd.concat([df1, df2], axis=0, keys=[&apos;x&apos;, &apos;y&apos;], levels=[[&apos;x&apos;, &apos;y&apos;, &apos;z&apos;, &apos;w&apos;]]) &gt;&gt;&gt; df.index.levels [[&apos;x&apos;, &apos;y&apos;, &apos;z&apos;, &apos;w&apos;], [&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;, &apos;5&apos;, &apos;6&apos;]] 1.7，sort默认值：sort=True，提示新版本会设置默认为False，并取消该参数 但0.22.0中虽然取消了，还是设置为True 非合并方向的行/列名称是否排序。例如1.1中默认axis=0时columns进行了排序，axis=1时index进行了排序。 axis=0时sort=False，columns不作排序： &gt;&gt;&gt; pd.concat([df1, df2], axis=0, sort=False) D C B A F E 4 1.0 1.0 1.0 1.0 NaN NaN 3 1.0 1.0 1.0 1.0 NaN NaN 2 1.0 1.0 1.0 1.0 NaN NaN 1 1.0 1.0 1.0 1.0 NaN NaN 6 2.0 2.0 NaN NaN 2.0 2.0 5 2.0 2.0 NaN NaN 2.0 2.0 4 2.0 2.0 NaN NaN 2.0 2.0 3 2.0 2.0 NaN NaN 2.0 2.0 axis=1时sort=False，index不作排序： &gt;&gt;&gt; pd.concat([df1, df2], axis=1, sort=False) D C B A F E D C 4 1.0 1.0 1.0 1.0 2.0 2.0 2.0 2.0 3 1.0 1.0 1.0 1.0 2.0 2.0 2.0 2.0 2 1.0 1.0 1.0 1.0 NaN NaN NaN NaN 1 1.0 1.0 1.0 1.0 NaN NaN NaN NaN 6 NaN NaN NaN NaN 2.0 2.0 2.0 2.0 5 NaN NaN NaN NaN 2.0 2.0 2.0 2.0 1.8，concat多个DataFrame&gt;&gt;&gt; pd.concat([df1, df2, df3], sort=False, join_axes=[df1.columns]) D C B A 4 1.0 1.0 1.0 1.0 3 1.0 1.0 1.0 1.0 2 1.0 1.0 1.0 1.0 1 1.0 1.0 1.0 1.0 6 2.0 2.0 NaN NaN 5 2.0 2.0 NaN NaN 4 2.0 2.0 NaN NaN 3 2.0 2.0 NaN NaN 6 NaN NaN 3.0 3.0 5 NaN NaN 3.0 3.0 2 NaN NaN 3.0 3.0 1 NaN NaN 3.0 3.0 2，appendappend(self, other, ignore_index=False, verify_integrity=False)竖方向合并df，没有axis属性 不会就地修改，而是会创建副本 示例： &gt;&gt;&gt; df1.append(df2) # 相当于pd.concat([df1, df2]) A B C D E F 4 1.0 1.0 1.0 1.0 NaN NaN 3 1.0 1.0 1.0 1.0 NaN NaN 2 1.0 1.0 1.0 1.0 NaN NaN 1 1.0 1.0 1.0 1.0 NaN NaN 6 NaN NaN 2.0 2.0 2.0 2.0 5 NaN NaN 2.0 2.0 2.0 2.0 4 NaN NaN 2.0 2.0 2.0 2.0 3 NaN NaN 2.0 2.0 2.0 2.0 2.1，ignore_index属性&gt;&gt;&gt; df1.append(df2, ignore_index=True) A B C D E F 0 1.0 1.0 1.0 1.0 NaN NaN 1 1.0 1.0 1.0 1.0 NaN NaN 2 1.0 1.0 1.0 1.0 NaN NaN 3 1.0 1.0 1.0 1.0 NaN NaN 4 NaN NaN 2.0 2.0 2.0 2.0 5 NaN NaN 2.0 2.0 2.0 2.0 6 NaN NaN 2.0 2.0 2.0 2.0 7 NaN NaN 2.0 2.0 2.0 2.0 2.2，append多个DataFrame和concat相同，append也支持append多个DataFrame &gt;&gt;&gt; df1.append([df2, df3], ignore_index=True) A B C D E F 0 1.0 1.0 1.0 1.0 NaN NaN 1 1.0 1.0 1.0 1.0 NaN NaN 2 1.0 1.0 1.0 1.0 NaN NaN 3 1.0 1.0 1.0 1.0 NaN NaN 4 NaN NaN 2.0 2.0 2.0 2.0 5 NaN NaN 2.0 2.0 2.0 2.0 6 NaN NaN 2.0 2.0 2.0 2.0 7 NaN NaN 2.0 2.0 2.0 2.0 8 3.0 3.0 NaN NaN 3.0 3.0 9 3.0 3.0 NaN NaN 3.0 3.0 10 3.0 3.0 NaN NaN 3.0 3.0 11 3.0 3.0 NaN NaN 3.0 3.0 3，mergepd.merge(left, right, how=’inner’, on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=True, suffixes=(‘_x’, ‘_y’), copy=True, indicator=False,validate=None)示例： &gt;&gt;&gt; left = pd.DataFrame({&apos;A&apos;: [&apos;a0&apos;, &apos;a1&apos;, &apos;a2&apos;, &apos;a3&apos;], &apos;B&apos;: [&apos;b0&apos;, &apos;b1&apos;, &apos;b2&apos;, &apos;b3&apos;], &apos;k1&apos;: [&apos;x&apos;, &apos;x&apos;, &apos;y&apos;, &apos;y&apos;]}) &gt;&gt;&gt; right = pd.DataFrame({&apos;C&apos;: [&apos;c1&apos;, &apos;c2&apos;, &apos;c3&apos;, &apos;c4&apos;], &apos;D&apos;: [&apos;d1&apos;, &apos;d2&apos;, &apos;d3&apos;, &apos;d4&apos;], &apos;k1&apos;: [&apos;y&apos;, &apos;y&apos;, &apos;z&apos;, &apos;z&apos;]}) &gt;&gt;&gt; left A B k1 0 a0 b0 x 1 a1 b1 x 2 a2 b2 y 3 a3 b3 y &gt;&gt;&gt; right C D k2 0 c1 d1 y 1 c2 d2 y 2 c3 d3 z 3 c4 d4 z 对df1和df2进行merge： &gt;&gt;&gt; pd.merge(left, right) A B k1 C D 0 a2 b2 y c1 d1 1 a2 b2 y c2 d2 2 a3 b3 y c1 d1 3 a3 b3 y c2 d2 可以看到只有df1和df2的key1=y的行保留了下来，即默认合并后只保留有共同列项并且值相等行（即交集）。 本例中left和right的k1=y分别有2个，最终构成了2*2=4行。 如果没有共同列会报错： &gt;&gt;&gt; del left[&apos;k1&apos;] &gt;&gt;&gt; pd.merge(left, right) pandas.errors.MergeError: No common columns to perform merge on 3.1，on属性新增一个共同列，但没有相等的值，发现合并返回是空列表，因为默认只保留所有共同列都相等的行： &gt;&gt;&gt; left[&apos;k2&apos;] = list(&apos;1234&apos;) &gt;&gt;&gt; right[&apos;k2&apos;] = list(&apos;5678&apos;) &gt;&gt;&gt; pd.merge(left, right) Empty DataFrame Columns: [B, A, k1, k2, F, E] Index: [] 可以指定on，设定合并基准列，就可以根据k1进行合并，并且left和right共同列k2会同时变换名称后保留下来： &gt;&gt;&gt; pd.merge(left, right, on=&apos;k1&apos;) A B k1 k2_x C D k2_y 0 a2 b2 y 3 c1 d1 5 1 a2 b2 y 3 c2 d2 6 2 a3 b3 y 4 c1 d1 5 3 a3 b3 y 4 c2 d2 6 默认值：on的默认值是所有共同列，本例为：on=[‘k1’, ‘k2’] 3.2，how属性 how取值范围：’inner’, ‘outer’, ‘left’, ‘right’ 默认值：how=’inner’ ‘inner’：共同列的值必须完全相等： &gt;&gt;&gt; pd.merge(left, right, on=&apos;k1&apos;, how=&apos;inner&apos;) A B k1 k2_x C D k2_y 0 a2 b2 y 3 c1 d1 5 1 a2 b2 y 3 c2 d2 6 2 a3 b3 y 4 c1 d1 5 3 a3 b3 y 4 c2 d2 6 ‘outer’：共同列的值都会保留，left或right在共同列上的差集，会对它们的缺失列项的值赋上NaN： &gt;&gt;&gt; pd.merge(left, right, on=&apos;k1&apos;, how=&apos;outer&apos;) A B k1 k2_x C D k2_y 0 a0 b0 x 1 NaN NaN NaN 1 a1 b1 x 2 NaN NaN NaN 2 a2 b2 y 3 c1 d1 5 3 a2 b2 y 3 c2 d2 6 4 a3 b3 y 4 c1 d1 5 5 a3 b3 y 4 c2 d2 6 6 NaN NaN z NaN c3 d3 7 7 NaN NaN z NaN c4 d4 8 ‘left’：根据左边的DataFrame确定共同列的保留值，右边缺失列项的值赋上NaN： pd.merge(left, right, on=&apos;k1&apos;, how=&apos;left&apos;) A B k1 k2_x C D k2_y 0 a0 b0 x 1 NaN NaN NaN 1 a1 b1 x 2 NaN NaN NaN 2 a2 b2 y 3 c1 d1 5 3 a2 b2 y 3 c2 d2 6 4 a3 b3 y 4 c1 d1 5 5 a3 b3 y 4 c2 d2 6 ‘right’：根据右边的DataFrame确定共同列的保留值，左边缺失列项的值赋上NaN： &gt;&gt;&gt; pd.merge(left, right, on=&apos;k1&apos;, how=&apos;right&apos;) A B k1 k2_x C D k2_y 0 a2 b2 y 3 c1 d1 5 1 a3 b3 y 4 c1 d1 5 2 a2 b2 y 3 c2 d2 6 3 a3 b3 y 4 c2 d2 6 4 NaN NaN z NaN c3 d3 7 5 NaN NaN z NaN c4 d4 8 3.3，indicator默认值：indicator=False，不显示合并方式 设置True表示显示合并方式，即left / right / both： &gt;&gt;&gt; pd.merge(left, right, on=&apos;k1&apos;, how=&apos;outer&apos;, indicator=True) A B k1 k2_x C D k2_y _merge 0 a0 b0 x 1 NaN NaN NaN left_only 1 a1 b1 x 2 NaN NaN NaN left_only 2 a2 b2 y 3 c1 d1 5 both 3 a2 b2 y 3 c2 d2 6 both 4 a3 b3 y 4 c1 d1 5 both 5 a3 b3 y 4 c2 d2 6 both 6 NaN NaN z NaN c3 d3 7 right_only 7 NaN NaN z NaN c4 d4 8 right_only]]></content>
      <categories>
        <category>Python第三方包</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建python沙盒环境]]></title>
    <url>%2F2019%2F04%2F01%2F%E6%90%AD%E5%BB%BApython%E6%B2%99%E7%9B%92%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[一. 先update sudo apt update二. 安装pip sudo apt install python-pip三. 安装virtualenv sudo pip install virtualenv virtualenvwrapper四. 创建目录用来存放虚拟环境 sudo mkdir -p $WORKON_HOME五. 在用户目录下中的 .bashrc 中添加以下内容并保存(通过ll 可查看到 .bashrc 文件)if [ -f /usr/local/bin/virtualenvwrapper.sh ]; then export WORKON_HOME=$HOME/.virtualenvs source /usr/local/bin/virtualenvwrapper.shfi 六. 运行 source .bashrc 重新加载到环境变量中七. 创建虚拟环境 mkvirtualenv test1 关于虚拟环境的命令如下mkvirtualenv wxhpython01：创建运行环境wxhpython01workon wxhpython01: 工作在 zqxt 环境 或 从其它环境切换到 wxhpython01环境deactivate: 退出终端环境rmvirtualenv ENV：删除运行环境ENVmkproject mic：创建mic项目和运行环境micmktmpenv：创建临时运行环境lsvirtualenv: 列出可用的运行环境lssitepackages: 列出当前环境安装了的包 八.在虚拟环境中可以通过pip安装其他的所需包 例如 pip install django==1.9.8]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>沙盒</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用的损失函数]]></title>
    <url>%2F2019%2F03%2F30%2F%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[MAE：平均绝对误差(mean absolute error)，对应位置差值的绝对值之和$$J(\theta) = \frac{1}{mn}\sum_{i=1}^m\sum_{j=1}^n|\hat y_{ij}-y_{ij}|$$ MSE：均方误差(mean squared error)，对应位置差值的平方之和$$J(\theta) = \frac{1}{mn}\sum_{i=1}^m\sum_{j=1}^n(\hat y_{ij}-y_{ij})^2$$ 两种损失函数的性质异常值MSE对异常值敏感，因为它的惩罚是平方的，所以异常值的loss会非常大。MAE对异常之不敏感， 不妨设拟合函数为常数，那么MSE就相当于所有数据的均值（列出loss对c求导即可），而MAE相当于所有数据的中位数，所以会对异常值不敏感。 优化效率MAE不可导而且所有的导数的绝对值都相同，优化时无法确定更新速度，MSE可导，有closed-form解，只需要令偏导数为0即可。 如何选择如果想要检测异常值则使用MSE，如果想学习一个预测模型则建议使用MAE，或者先进行异常值处理再使用MSE]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>损失函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh传输文件]]></title>
    <url>%2F2019%2F03%2F28%2Fssh%E4%BC%A0%E8%BE%93%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[利用ssh传输文件在linux下一般用scp这个命令来通过ssh传输文件。 1、从服务器上下载文件scp username@servername:/path/filename /var/www/local_dir（本地目录） 例如scp root@192.168.0.101:/var/www/test.txt 把192.168.0.101上的/var/www/test.txt 的文件下载到/var/www/local_dir（本地目录） 2、上传本地文件到服务器scp /path/filename username@servername:/path 例如scp /var/www/test.php root@192.168.0.101:/var/www/ 把本机/var/www/目录下的test.php文件上传到192.168.0.101这台服务器上的/var/www/目录中 3、从服务器下载整个目录scp -r username@servername:/var/www/remote_dir/（远程目录） /var/www/local_dir（本地目录） 例如:scp -r root@192.168.0.101:/var/www/test /var/www/ 4、上传目录到服务器scp -r local_dir username@servername:remote_dir例如：scp -r test root@192.168.0.101:/var/www/ 把当前目录下的test目录上传到服务器的/var/www/ 目录 注：目标服务器要开启写入权限。]]></content>
      <categories>
        <category>远程操作</category>
      </categories>
      <tags>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习框架简介]]></title>
    <url>%2F2019%2F03%2F18%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[theanotheano最初于2008年开始开发，是第一个有较大影响力的框架。theano是一个Python库，可用于定义、优化和计算数学表达式，特别是多维数组（numpy.ndarray）theano诞生于研究机构，服务于研究人员，其设计具有较浓厚的学术气息，但在工业设计上有较大的缺陷。 TensorflowTendorflow是Google于2015年11月10日推出的全新的机器学习开源框架。Tensorflow在很大程度上可以看作是Theano的后继者，都是基于计算图实现自动微分系统。Tensoflow支持多种语言，Java、Go、R和Haskell的alpha版本也被支持。但是Tensorflow也存在一些问题： 过于复杂的系统设计 频繁变动的接口 接口设计过于晦涩难懂 文档混乱脱节 直接使用Tensorflow的生产力很低下，因而出现了Keras等第三方封装库（PS：我不认为Keras等第三方封装库可以被称之为深度学习框架）虽然不完美，但是最流行的深度学习框架，社区强大，适合生产环境 Caffe/Caffe2caffe是一个清晰、高效的深度学习框架，可信语言是C++，既可以在CPU上运行，也可以在GPU上运行。其优点为简洁快速、缺点是缺少灵活性。Caffe灵活性的缺失主要是因为它的设计。caffe的作者在FAIR担任主管的时候，开发了Caffe2，Caffe2的设计追求轻量级，在保有扩展性和高性能的同时，Caffe2也强调了便携性。文档不够完善，但性能优异，几乎全平台支持Caffe2，适合生产环境。 MXNetMXNet是一个深度学习库，支持C++、Python、JavaScript等语言；支持命令和符号编程；可以运行在CPU、GPU、集群、服务器、台式机或者移动设备上。MXNet以其超强的分布式支持，明显的内存、显存优化为人所称道。文档略混乱、但分布式性能强大、语言支持最多、适合AWS平台开发。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[框架梳理——RetinaNet/keras]]></title>
    <url>%2F2019%2F01%2F17%2F%E6%A1%86%E6%9E%B6%E6%A2%B3%E7%90%86%E2%80%94%E2%80%94RetinaNet-keras%2F</url>
    <content type="text"><![CDATA[框架梳理——RetinaNet/keras代码地址：https://travis-ci.org/fizyr/keras-retinanet 文件目录 keras-retinanet │ ├── examples(例图和测试Notebook) │ ├── images(测试图片) │ ├── keras_retinanet(主要的框架代码) │ ├── snapshots(模型保存地址) │ └── tests(测试文件夹，包含对模型的各种测试) keras_retinanet(主要的框架代码) │ ├── backend(tf或者th相关的设置) │ ├── bin(包含模型转换、调试、评价、训练等文件) │ ├── callbacks(与召回相关的函数) │ ├── layers(基础网络层之上的采样层处理，过滤预选框) │ ├── callbacks(与召回相关的函数) │ ├── models(基础网络文件夹，包括densenet、mobilenet、resnet、retinanet、vgg等网络) │ ├── preprocessing(数据集相关，包括各种数据集的获取、解析等) │ ├── utils(附属工具，包括锚点框、非最大值抑制等) │ └── losses.py/initializers.py(损失和初始化等) 训练模型关于模型的训练文件，在bin文件夹下，其训练文件的流程大致如下]]></content>
      <categories>
        <category>框架实现</category>
      </categories>
      <tags>
        <tag>RetinaNet</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keras-RetinaNet问题记录]]></title>
    <url>%2F2019%2F01%2F16%2Fkeras-RetinaNet%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[框架搭建获取代码 克隆代码库。 git clone https://github.com/fizyr/keras-retinanet.git 编译支持编译Cython代码 python setup.py build_ext –inplace Retinanet训练Pascal VOC 2007train# train python3 keras_retinanet/bin/train.py pascal /path/to/VOCdevkit/VOC2007 # 使用 --backbone=xxx 选择网络结构，默认是resnet50 # xxx可以是resnet模型（`resnet50`，`resnet101`，`resnet152`） # 或`mobilenet`模型（`mobilenet128_1.0`，`mobilenet128_0.75`，`mobilenet160_1.0`等） # 也可以使用models目录下的 resnet.py，mobilenet.py等来自定义网络 test1 首先需要进行模型转换，将训练好的模型转换为测试所需模型，keras-retinanet的训练程序与训练模型一起使用。 与测试模型相比，这些是精简版本，仅包含培训所需的层（回归和分类值）。 如果您希望对模型进行测试（对图像执行对象检测），则需要将训练模型转换为测试模型。 # Running directly from the repository: keras_retinanet/bin/convert_model.py /path/to/training/model.h5 /path/to/save/inference/model.h5 # Using the installed script: retinanet-convert-model /path/to/training/model.h5 /path/to/save/inference/model.h5 2 测试代码 # import keras import keras # import keras_retinanet from keras_retinanet import models from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image from keras_retinanet.utils.visualization import draw_box, draw_caption from keras_retinanet.utils.colors import label_color # import miscellaneous modules import matplotlib.pyplot as plt import cv2 import os import numpy as np import time # set tf backend to allow memory to grow, instead of claiming everything import tensorflow as tf def get_session(): config = tf.ConfigProto() config.gpu_options.allow_growth = True return tf.Session(config=config) # use this environment flag to change which GPU to use #os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;1&quot; # set the modified tf session as backend in keras keras.backend.tensorflow_backend.set_session(get_session()) # adjust this to point to your downloaded/trained model # models can be downloaded here: https://github.com/fizyr/keras-retinanet/releases model_path = os.path.join(&apos;..&apos;, &apos;snapshots&apos;, &apos;resnet50_coco_best_v2.1.0.h5&apos;) # load retinanet model model = models.load_model(model_path, backbone_name=&apos;resnet50&apos;) # if the model is not converted to an inference model, use the line below # see: https://github.com/fizyr/keras-retinanet#converting-a-training-model-to-inference-model #model = models.convert_model(model) #print(model.summary()) # load label to names mapping for visualization purposes labels_to_names = {0: &apos;person&apos;, 1: &apos;bicycle&apos;, 2: &apos;car&apos;, 3: &apos;motorcycle&apos;, 4: &apos;airplane&apos;, 5: &apos;bus&apos;, 6: &apos;train&apos;, 7: &apos;truck&apos;, 8: &apos;boat&apos;, 9: &apos;traffic light&apos;, 10: &apos;fire hydrant&apos;, 11: &apos;stop sign&apos;, 12: &apos;parking meter&apos;, 13: &apos;bench&apos;, 14: &apos;bird&apos;, 15: &apos;cat&apos;, 16: &apos;dog&apos;, 17: &apos;horse&apos;, 18: &apos;sheep&apos;, 19: &apos;cow&apos;, 20: &apos;elephant&apos;, 21: &apos;bear&apos;, 22: &apos;zebra&apos;, 23: &apos;giraffe&apos;, 24: &apos;backpack&apos;, 25: &apos;umbrella&apos;, 26: &apos;handbag&apos;, 27: &apos;tie&apos;, 28: &apos;suitcase&apos;, 29: &apos;frisbee&apos;, 30: &apos;skis&apos;, 31: &apos;snowboard&apos;, 32: &apos;sports ball&apos;, 33: &apos;kite&apos;, 34: &apos;baseball bat&apos;, 35: &apos;baseball glove&apos;, 36: &apos;skateboard&apos;, 37: &apos;surfboard&apos;, 38: &apos;tennis racket&apos;, 39: &apos;bottle&apos;, 40: &apos;wine glass&apos;, 41: &apos;cup&apos;, 42: &apos;fork&apos;, 43: &apos;knife&apos;, 44: &apos;spoon&apos;, 45: &apos;bowl&apos;, 46: &apos;banana&apos;, 47: &apos;apple&apos;, 48: &apos;sandwich&apos;, 49: &apos;orange&apos;, 50: &apos;broccoli&apos;, 51: &apos;carrot&apos;, 52: &apos;hot dog&apos;, 53: &apos;pizza&apos;, 54: &apos;donut&apos;, 55: &apos;cake&apos;, 56: &apos;chair&apos;, 57: &apos;couch&apos;, 58: &apos;potted plant&apos;, 59: &apos;bed&apos;, 60: &apos;dining table&apos;, 61: &apos;toilet&apos;, 62: &apos;tv&apos;, 63: &apos;laptop&apos;, 64: &apos;mouse&apos;, 65: &apos;remote&apos;, 66: &apos;keyboard&apos;, 67: &apos;cell phone&apos;, 68: &apos;microwave&apos;, 69: &apos;oven&apos;, 70: &apos;toaster&apos;, 71: &apos;sink&apos;, 72: &apos;refrigerator&apos;, 73: &apos;book&apos;, 74: &apos;clock&apos;, 75: &apos;vase&apos;, 76: &apos;scissors&apos;, 77: &apos;teddy bear&apos;, 78: &apos;hair drier&apos;, 79: &apos;toothbrush&apos;} # load image image = read_image_bgr(&apos;000000008021.jpg&apos;) # copy to draw on draw = image.copy() draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB) # preprocess image for network image = preprocess_image(image) image, scale = resize_image(image) # process image start = time.time() boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0)) print(&quot;processing time: &quot;, time.time() - start) # correct for image scale boxes /= scale # visualize detections for box, score, label in zip(boxes[0], scores[0], labels[0]): # scores are sorted so we can break if score &lt; 0.5: break color = label_color(label) b = box.astype(int) draw_box(draw, b, color=color) caption = &quot;{} {:.3f}&quot;.format(labels_to_names[label], score) draw_caption(draw, b, caption) plt.figure(figsize=(15, 15)) plt.axis(&apos;off&apos;) plt.imshow(draw) plt.show()]]></content>
      <categories>
        <category>算法实现</category>
      </categories>
      <tags>
        <tag>RetinaNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FPN——CNN特征提取]]></title>
    <url>%2F2019%2F01%2F14%2FFPN%E2%80%94%E2%80%94CNN%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[转载自：https://www.jianshu.com/p/5a28ae9b365d 如有侵权，请联系删除 介绍FPN是一种利用常规CNN模型来高效提取图片中各维度特征的方法。在计算机视觉学科中，多维度的目标检测一直以来都是通过将缩小或扩大后的不同维度图片作为输入来生成出反映不同维度信息的特征组合。这种办法确实也能有效地表达出图片之上的各种维度特征，但却对硬件计算能力及内存大小有较高要求，因此只能在有限的领域内部使用。FPN通过利用常规CNN模型内部从底至上各个层对同一scale图片不同维度的特征表达结构，提出了一种可有效在单一图片视图下生成对其的多维度特征表达的方法。它可以有效地赋能常规CNN模型，从而可以生成出表达能力更强的feature maps以供下一阶段计算机视觉任务像object detection/semantic segmentation等来使用。本质上说它是一种加强主干网络CNN特征表达的方法。 Featurized image pyramid下图中描述了四种不同的得到一张图片多维度特征组合的方法。 上图(a)中的方法即为常规的生成一张图片的多维度特征组合的经典方法。即对某一输入图片我们通过压缩或放大从而形成不同维度的图片作为模型输入，使用同一模型对这些不同维度的图片分别处理后，最终再将这些分别得到的特征（feature maps）组合起来就得到了我们想要的可反映多维度信息的特征集。此种方法缺点在于需要对同一图片在更改维度后输入处理多次，因此对计算机的算力及内存大小都有较高要求。图(b)中的方法则只拿单一维度的图片做为输入，然后经CNN模型处理后，拿最终一层的feature maps作为最终的特征集。显然此种方法只能得到单一维度的信息。优点是计算简单，对计算机算力及内存大小都无过高需求。此方法为大多数R-CNN系列目标检测方法所用像R-CNN/Fast-RCNN/Faster-RCNN等。因此最终这些模型对小维度的目标检测性能不是很好。图(c)中的方法同样是拿单一维度的图片做为输入，不过最终选取用于接下来分类或检测任务时的特征组合时，此方法不只选用了最后一层的high level feature maps，同样也会选用稍靠下的反映图片low level 信息的feature maps。然后将这些不同层次（反映不同level的图片信息）的特征简单合并起来（一般为concat处理），用于最终的特征组合输出。此方法可见于SSD当中。不过SSD在选取层特征时都选用了较高层次的网络。比如在它以VGG16作为主干网络的检测模型里面所选用的最低的Convolution的层为Conv4，这样一些具有更低级别信息的层特征像Conv2/Conv3就被它给漏掉了，于是它对更小维度的目标检测效果就不大好。图(d)中的方法同图(c)中的方法有些类似，也是拿单一维度的图片作为输入，然后它会选取所有层的特征来处理然后再联合起来做为最终的特征输出组合。（作者在论文中拿Resnet为实例时并没选用Conv1层，那是为了算力及内存上的考虑，毕竟Conv1层的size还是比较大的，所包含的特征跟直接的图片像素信息也过于接近）。另外还对这些反映不同级别图片信息的各层自上向下进行了再处理以能更好地组合从而形成较好的特征表达（详细过程会在下面章节中进一步介绍）。而此方法正是我们本文中要讲的FPN CNN特征提取方法。 FPN基本架构FPN会使用CNN网络中每一层的信息来生成最后的表达特征组合。下图是它的基本架构。从中我们能看到FPN会模型每个CNN层的特征输出进行处理以生成反映此维度信息的特征。而自上至下处理后所生成出的特征之间也有个关联关系，即上层high level的特征会影响下一层次的low level特征表达。最终所有的特征一起用来作为下一步的目标检测或类别分析等任务的输入。 FPN详细介绍FPN是传统CNN网络对图片信息进行表达输出的一种增强。它目的是为了改进CNN网络的特征提取方式，从而可以使最终输出的特征更好地表示出输入图片各个维度的信息。它的基本过程有三个分别为：自下至上的通路即自下至上的不同维度特征生成；自上至下的通路即自上至下的特征补充增强；CNN网络层特征与最终输出的各维度特征之间的关联表达。我们在下图中能看出这三个过程的细粒度表示。 自下至上的通路（Bottom-top pathway）：这个没啥奇怪就是指的普通CNN特征自底至上逐层浓缩表达特征的一个过程。此过程很早即被认识到了即较底的层反映较浅层次的图片信息特征像边缘等；较高的层则反映较深层次的图片特征像物体轮廓、乃至类别等； 自上至下的通路（Top-bottome pathway）：上层的特征输出一般其feature map size比较小，但却能表示更大维度（同时也是更加high level）的图片信息。此类high level信息经实验证明能够对后续的目标检测、物体分类等任务发挥关键作用。因此我们在处理每一层信息时会参考上一层的high level信息做为其输入（这里只是在将上层feature map等比例放大后再与本层的feature maps做element wise相加）; CNN层特征与每一级别输出之间的表达关联：在这里作者实验表明使用1x1的Conv即可生成较好的输出特征，它可有效地降低中间层次的channels 数目。最终这些1x1的Convs使得我们输出不同维度的各个feature maps有着相同的channels数目（本文用到的Resnet-101主干网络中，各个层次特征的最终输出channels数目为256）。 FPN在目标检测中的实际应用以下为一个FPN特征提取方法在RCNN目标检测框架中应用的例子。从中我们可以更加详细地了解到它的具体实现。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>图像识别</tag>
        <tag>FPN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测——RetinaNet]]></title>
    <url>%2F2019%2F01%2F14%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94RetinaNet%2F</url>
    <content type="text"><![CDATA[转载自：https://www.jianshu.com/p/8e501a159b28 如有侵权，请联系删除 RetinaNet: Focal loss在目标检测网络中的应用介绍RetinaNet是2018年Facebook AI团队在目标检测领域新的贡献。它的重要作者名单中Ross Girshick与Kaiming He赫然在列。来自Microsoft的Sun Jian团队与现在Facebook的Ross/Kaiming团队在当前视觉目标分类、检测领域有着北乔峰、南慕容一般的独特地位。这两个实验室的文章多是行业里前进方向的提示牌。RetinaNet只是原来FPN网络与FCN网络的组合应用，因此在目标网络检测框架上它并无特别亮眼创新。文章中最大的创新来自于Focal loss的提出及在单阶段目标检测网络RetinaNet（实质为Resnet + FPN + FCN）的成功应用。Focal loss是一种改进了的交叉熵(cross-entropy, CE)loss，它通过在原有的CE loss上乘了个使易检测目标对模型训练贡献削弱的指数式，从而使得Focal loss成功地解决了在目标检测时，正负样本区域极不平衡而目标检测loss易被大批量负样本所左右的问题。此问题是单阶段目标检测框架（如SSD/Yolo系列）与双阶段目标检测框架（如Faster-RCNN/R-FCN等）accuracy gap的最大原因。在Focal loss提出之前，已有的目标检测网络都是通过像Boot strapping/Hard example mining等方法来解决此问题的。作者通过后续实验成功表明Focal loss可在单阶段目标检测网络中成功使用，并最终能以更快的速率实现与双阶段目标检测网络近似或更优的效果。 类别不平衡问题常规的单阶段目标检测网络像SSD一般在模型训练时会先大密度地在模型终端的系列feature maps上生成出10,000甚至100,0000个目标候选区域。然后再分别对这些候选区域进行分类与位置回归识别。而在这些生成的数万个候选区域中，绝大多数都是不包含待检测目标的图片背景，这样就造成了机器学习中经典的训练样本正负不平衡的问题。它往往会造成最终算出的training loss为占绝对多数但包含信息量却很少的负样本所支配，少样正样本提供的关键信息却不能在一般所用的training loss中发挥正常作用，从而无法得出一个能对模型训练提供正确指导的loss。常用的解决此问题的方法就是负样本挖掘。或其它更复杂的用于过滤负样本从而使正负样本数维持一定比率的样本取样方法。而在此篇文章中作者提出了可通过候选区域包含潜在目标概率进而对最终的training loss进行较正的方法。实验表明这种新提出的focal loss在单阶段目标检测任务上表现突出，有效地解决了此领域里面潜在的类别不平衡问题。 Focal loss CE(cross-entropy) loss 以下为典型的交叉熵loss，它广泛用于当下的图像分类、检测CNN网络当中。 Balanced CE loss 考虑到上节中提到的类别不平衡问题对最终training loss的不利影响，我们自然会想到可通过在loss公式中使用与目标存在概率成反比的系数对其进行较正。如下公式即是此朴素想法的体现。它也是作者最终Focus loss的baseline。 Focal loss定义 以下是作者提出的focal loss的想法。 下图为focal loss与常规CE loss的对比。从中，我们易看出focal loss所加的指数式系数可对正负样本对loss的贡献自动调节。当某样本类别比较明确些，它对整体loss的贡献就比较少；而若某样本类别不易区分，则对整体loss的贡献就相对偏大。这样得到的loss最终将集中精力去诱导模型去努力分辨那些难分的目标类别，于是就有效提升了整体的目标检测准度。不过在此focus loss计算当中，我们引入了一个新的hyper parameter即γ。一般来说新参数的引入，往往会伴随着模型使用难度的增加。在本文中，作者有试者对其进行调节，线性搜索后得出将γ设为2时，模型检测效果最好。 在最终所用的focal loss上，作者还引入了α系数，它能够使得focal loss对不同类别更加平衡。实验表明它会比原始的focal loss效果更好。 模型的初始化参数选择一般我们初始化CNN网络模型时都会使用无偏的参数对其初始化，比如Conv的kernel 参数我们会以bias 为0，variance为0.01的某分布来对其初始化。但是如果我们的模型要去处理类别极度不平衡的情况，那么就会考虑到这样对训练数据分布无任选先验假设的初始化会使得在训练过程中，我们的参数更偏向于拥有更多数量的负样本的情况去进化。作者观察下来发现它在训练时会出现极度的不稳定。于是作者在初始化模型最后一层参数时考虑了数据样本分布的不平衡性，这样使得初始训练时最终得出的loss不会对过多的负样本数量所惊讶到，从而有效地规避了初始训练时模型的震荡与不稳定 RetinaNet检测框架RetinaNet本质上是Resnet + FPN + 两个FCN子网络。以下为RetinaNet目标框架框架图。有了之前blog里面提到的FPN与FCN的知识后，我们很容易理解此框架的设计含义。 一般主干网络可选用任一有效的特征提取网络如vgg16或resnet系列，此处作者分别尝试了resnet-50与resnet-101。而FPN则是对resnet-50里面自动形成的多尺度特征进行了强化利用，从而得到了表达力更强、包含多尺度目标区域信息的feature maps集合。最后在FPN所吐出的feature maps集合上，分别使用了两个FCN子网络（它们有着相同的网络结构却各自独立，并不share参数）用来完成目标框类别分类与位置回归任务。 模型的推理与训练 模型推理 一旦我们有了训练好的模型，在正式部署时，只需对其作一次forward，然后对最终生成的目标区域进行过渡。然后只对每个FPN level上目标存在概率最高的前1000个目标框进一步地decoding处理。接下来再将所有FPN level上得到的目标框汇集起来，统一使用极大值抑制的方法进一步过渡（其中极大值抑制时所用的阈值为0.5）。这样，我们就得到了最终的目标与其位置框架。 模型训练 模型训练中主要在后端Loss计算时采用了Focal loss，另外也在模型初始化时考虑到了正负样本极度不平衡的情况进而对模型最后一个conv layer的bias参数作了有偏初始化。 训练时用了SGD，mini batch size为16，在8个GPU上一块训练，每个GPU上local batch size为2。最大iterations数目为90,000；模型初始lr为0.01,接下来随着训练进行分step wisely 降低。真正的training loss则为表达目标类别的focus loss与表达目标框位置回归信息的L1 loss的和。 下图为RetinaNet模型的检测准度与性能]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>RetinaNet</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目整理]]></title>
    <url>%2F2019%2F01%2F14%2F%E9%A1%B9%E7%9B%AE%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[渝黔高速——隧道开发工具：JavaScript + Git + Docker + PostgresqlIDE：Vscode 表结构设计： 隧道表 衬砌表 工法分区表 衬砌工程量表 分区工程量表 模型表 进度表 施工劳务记录表 劳务合同清单表 业务分析模型的创建 隧道的创建以及获取信息 衬砌的创建以及信息获取 工法分区的创建以及信息获取 衬砌工程量的录入 隧道段的创建、查询、修改 在段下面创建模型（根据衬砌类型和分区批量创建） 分区工程量参数设置（根据已经创建的模型，对每一个分区进行分区工程量参数设置）、修改 分区工程量参数 进度 根据模型获取相关的工程量，起止桩号 创建进尺 修改完成状态 查看已完成进尺 工程量 统计当前施工完成的工程量 根据时间筛选当前完成的工程量 产值 根据工程量计算得到当前施工的总产值以及单个隧道产值 分包 合同工程量清单的创建、获取 劳务成本的计算 业务逻辑]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>项目</tag>
        <tag>隧道</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Pytorch的特征图提取]]></title>
    <url>%2F2018%2F11%2F13%2F%E5%9F%BA%E4%BA%8EPytorch%E7%9A%84%E7%89%B9%E5%BE%81%E5%9B%BE%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[简述为了方便理解卷积神经网络的运行过程，需要对卷积神经网络的运行结果进行可视化的展示。 大致可分为如下步骤： 单个图片的提取 神经网络的构建 特征图的提取 可视化展示 单个图片的提取根据目标要求，需要对单个图片进行卷积运算，但是Pytorch中读取数据主要用到torch.utils.data.DataLoader类，因此我们需要编写单个图片的读取程序 1234567891011121314def get_picture(picture_dir, transform): ''' 该算法实现了读取图片，并将其类型转化为Tensor ''' tmp = [] img = skimage.io.imread(picture_dir) tmp.append(img) img = skimage.io.imread('./picture/4.jpg') tmp.append(img) img256 = [skimage.transform.resize(img, (256, 256)) for img in tmp] img256 = np.asarray(img256) img256 = img256.astype(np.float32) return transform(img256[0]) 注意： 神经网络的输入是四维形式，我们返回的图片是三维形式，需要使用unsqueeze()插入一个维度 神经网络的构建网络的基于LeNet构建，不过为了方便展示，将其中的参数按照2562563进行的参数的修正 网络构建如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class LeNet(nn.Module): ''' 该类继承了torch.nn.Modul类 构建LeNet神经网络模型 ''' def __init__(self): super(LeNet, self).__init__() # 第一层神经网络，包括卷积层、线性激活函数、池化层 self.conv1 = nn.Sequential( nn.Conv2d(3, 32, 5, 1, 2), # input_size=(3*256*256)，padding=2 nn.ReLU(), # input_size=(32*256*256) nn.MaxPool2d(kernel_size=2, stride=2), # output_size=(32*128*128) ) # 第二层神经网络，包括卷积层、线性激活函数、池化层 self.conv2 = nn.Sequential( nn.Conv2d(32, 64, 5, 1, 2), # input_size=(32*128*128) nn.ReLU(), # input_size=(64*128*128) nn.MaxPool2d(2, 2) # output_size=(64*64*64) ) # 全连接层(将神经网络的神经元的多维输出转化为一维) self.fc1 = nn.Sequential( nn.Linear(64 * 64 * 64, 128), # 进行线性变换 nn.ReLU() # 进行ReLu激活 ) # 输出层(将全连接层的一维输出进行处理) self.fc2 = nn.Sequential( nn.Linear(128, 84), nn.ReLU() ) # 将输出层的数据进行分类(输出预测值) self.fc3 = nn.Linear(84, 62) # 定义前向传播过程，输入为x def forward(self, x): x = self.conv1(x) x = self.conv2(x) # nn.Linear()的输入输出都是维度为一的值，所以要把多维度的tensor展平成一维 x = x.view(x.size()[0], -1) x = self.fc1(x) x = self.fc2(x) x = self.fc3(x) return x 特征图的提取直接上代码：123456789101112131415161718class FeatureExtractor(nn.Module): def __init__(self, submodule, extracted_layers): super(FeatureExtractor, self).__init__() self.submodule = submodule self.extracted_layers = extracted_layers def forward(self, x): outputs = [] for name, module in self.submodule._modules.items(): # 目前不展示全连接层 if "fc" in name: x = x.view(x.size(0), -1) print(module) x = module(x) print(name) if name in self.extracted_layers: outputs.append(x) return outputs 可视化展示可视化展示使用matplotlib 代码如下：1234567# 特征输出可视化for i in range(32): ax = plt.subplot(6, 6, i + 1) ax.set_title('Feature &#123;&#125;'.format(i)) ax.axis('off') plt.imshow(x[0].data.numpy()[0,i,:,:],cmap='jet')plt.plot() 完整代码在此贴上完整代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186import osimport torchimport torchvision as tvimport torchvision.transforms as transformsimport torch.nn as nnimport torch.optim as optimimport argparseimport skimage.dataimport skimage.ioimport skimage.transformimport numpy as npimport matplotlib.pyplot as plt# 定义是否使用GPUdevice = torch.device("cuda" if torch.cuda.is_available() else "cpu")# Load training and testing datasets.pic_dir = './picture/3.jpg'# 定义数据预处理方式(将输入的类似numpy中arrary形式的数据转化为pytorch中的张量（tensor）)transform = transforms.ToTensor()def get_picture(picture_dir, transform): ''' 该算法实现了读取图片，并将其类型转化为Tensor ''' img = skimage.io.imread(picture_dir) img256 = skimage.transform.resize(img, (256, 256)) img256 = np.asarray(img256) img256 = img256.astype(np.float32) return transform(img256)def get_picture_rgb(picture_dir): ''' 该函数实现了显示图片的RGB三通道颜色 ''' img = skimage.io.imread(picture_dir) img256 = skimage.transform.resize(img, (256, 256)) skimage.io.imsave('./picture/4.jpg',img256) # 取单一通道值显示 # for i in range(3): # img = img256[:,:,i] # ax = plt.subplot(1, 3, i + 1) # ax.set_title('Feature &#123;&#125;'.format(i)) # ax.axis('off') # plt.imshow(img) # r = img256.copy() # r[:,:,0:2]=0 # ax = plt.subplot(1, 4, 1) # ax.set_title('B Channel') # # ax.axis('off') # plt.imshow(r) # g = img256.copy() # g[:,:,0]=0 # g[:,:,2]=0 # ax = plt.subplot(1, 4, 2) # ax.set_title('G Channel') # # ax.axis('off') # plt.imshow(g) # b = img256.copy() # b[:,:,1:3]=0 # ax = plt.subplot(1, 4, 3) # ax.set_title('R Channel') # # ax.axis('off') # plt.imshow(b) # img = img256.copy() # ax = plt.subplot(1, 4, 4) # ax.set_title('image') # # ax.axis('off') # plt.imshow(img) img = img256.copy() ax = plt.subplot() ax.set_title('image') # ax.axis('off') plt.imshow(img) plt.show()class LeNet(nn.Module): ''' 该类继承了torch.nn.Modul类 构建LeNet神经网络模型 ''' def __init__(self): super(LeNet, self).__init__() # 第一层神经网络，包括卷积层、线性激活函数、池化层 self.conv1 = nn.Sequential( nn.Conv2d(3, 32, 5, 1, 2), # input_size=(3*256*256)，padding=2 nn.ReLU(), # input_size=(32*256*256) nn.MaxPool2d(kernel_size=2, stride=2), # output_size=(32*128*128) ) # 第二层神经网络，包括卷积层、线性激活函数、池化层 self.conv2 = nn.Sequential( nn.Conv2d(32, 64, 5, 1, 2), # input_size=(32*128*128) nn.ReLU(), # input_size=(64*128*128) nn.MaxPool2d(2, 2) # output_size=(64*64*64) ) # 全连接层(将神经网络的神经元的多维输出转化为一维) self.fc1 = nn.Sequential( nn.Linear(64 * 64 * 64, 128), # 进行线性变换 nn.ReLU() # 进行ReLu激活 ) # 输出层(将全连接层的一维输出进行处理) self.fc2 = nn.Sequential( nn.Linear(128, 84), nn.ReLU() ) # 将输出层的数据进行分类(输出预测值) self.fc3 = nn.Linear(84, 62) # 定义前向传播过程，输入为x def forward(self, x): x = self.conv1(x) x = self.conv2(x) # nn.Linear()的输入输出都是维度为一的值，所以要把多维度的tensor展平成一维 x = x.view(x.size()[0], -1) x = self.fc1(x) x = self.fc2(x) x = self.fc3(x) return x# 中间特征提取class FeatureExtractor(nn.Module): def __init__(self, submodule, extracted_layers): super(FeatureExtractor, self).__init__() self.submodule = submodule self.extracted_layers = extracted_layers def forward(self, x): outputs = [] print(self.submodule._modules.items()) for name, module in self.submodule._modules.items(): if "fc" in name: print(name) x = x.view(x.size(0), -1) print(module) x = module(x) print(name) if name in self.extracted_layers: outputs.append(x) return outputsdef get_feature(): # 输入数据 img = get_picture(pic_dir, transform) # 插入维度 img = img.unsqueeze(0) img = img.to(device) # 特征输出 net = LeNet().to(device) # net.load_state_dict(torch.load('./model/net_050.pth')) exact_list = ["conv1"，"conv2"] myexactor = FeatureExtractor(net, exact_list) x = myexactor(img) # 特征输出可视化 for i in range(32): ax = plt.subplot(6, 6, i + 1) ax.set_title('Feature &#123;&#125;'.format(i)) ax.axis('off') plt.imshow(x[0].data.numpy()[0,i,:,:],cmap='jet') plt.show()# 训练if __name__ == "__main__": get_picture_rgb(pic_dir) # get_feature()]]></content>
      <categories>
        <category>Pytorch框架</category>
      </categories>
      <tags>
        <tag>feature map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于pytorch的交通标志识别]]></title>
    <url>%2F2018%2F11%2F11%2F%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E4%BA%A4%E9%80%9A%E6%A0%87%E5%BF%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[本文是在https://www.jianshu.com/p/d8feaddc7bdf文章的基础上用Pytorch实现的 话不多说，直接上代码，具体的可以看代码中的解释 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180import osimport torchimport torchvision as tvimport torchvision.transforms as transformsimport torch.nn as nnimport torch.optim as optimimport argparseimport skimage.dataimport skimage.transformimport numpy as np# 定义是否使用GPUdevice = torch.device("cuda" if torch.cuda.is_available() else "cpu")'''使得我们能够手动输入命令行参数，就是让风格变得和Linux命令行差不多argparse是python的一个包，用来解析输入的参数如： python mnist.py --outf model （意思是将训练的模型保存到model文件夹下，当然，你也可以不加参数，那样的话代码最后一行 torch.save()就需要注释掉了） python mnist.py --net model/net_005.pth （意思是加载之前训练好的网络模型，前提是训练使用的网络和测试使用的网络是同一个网络模型，保证权重参数矩阵相等）'''parser = argparse.ArgumentParser()parser.add_argument('--outf', default='./model/', help='folder to output images and model checkpoints') # 模型保存路径parser.add_argument('--net', default='./model/net.pth', help="path to netG (to continue training)") # 模型加载路径opt = parser.parse_args() # 解析得到你在路径中输入的参数，比如 --outf 后的"model"或者 --net 后的"model/net_005.pth"，是作为字符串形式保存的# Load training and testing datasets.ROOT_PATH = "./traffic"train_data_dir = os.path.join(ROOT_PATH, "datasets/BelgiumTS/Training")test_data_dir = os.path.join(ROOT_PATH, "datasets/BelgiumTS/Testing")'''定义LeNet神经网络，进一步的理解可查看Pytorch入门，里面很详细，代码本质上是一样的，这里做了一些封装'''class LeNet(nn.Module): ''' 该类继承了torch.nn.Modul类 构建LeNet神经网络模型 ''' def __init__(self): super(LeNet, self).__init__() # 这一个是python中的调用父类LeNet的方法，因为LeNet继承了nn.Module，如果不加这一句，无法使用导入的torch.nn中的方法，这涉及到python的类继承问题，你暂时不用深究 # 第一层神经网络，包括卷积层、线性激活函数、池化层 self.conv1 = nn.Sequential( # input_size=(1*28*28)：输入层图片的输入尺寸，我看了那个文档，发现不需要天，会自动适配维度 nn.Conv2d(3, 32, 5, 1, 2), # padding=2保证输入输出尺寸相同：采用的是两个像素点进行填充，用尺寸为5的卷积核，保证了输入和输出尺寸的相同 nn.ReLU(), # input_size=(6*28*28)：同上，其中的6是卷积后得到的通道个数，或者叫特征个数，进行ReLu激活 nn.MaxPool2d(kernel_size=2, stride=2), # output_size=(6*14*14)：经过池化层后的输出 ) # 第二层神经网络，包括卷积层、线性激活函数、池化层 self.conv2 = nn.Sequential( nn.Conv2d(32, 64, 5), # input_size=(6*14*14)： 经过上一层池化层后的输出,作为第二层卷积层的输入，不采用填充方式进行卷积 nn.ReLU(), # input_size=(16*10*10)： 对卷积神经网络的输出进行ReLu激活 nn.MaxPool2d(2, 2) # output_size=(16*5*5)： 池化层后的输出结果 ) # 全连接层(将神经网络的神经元的多维输出转化为一维) self.fc1 = nn.Sequential( nn.Linear(64 * 5 * 5, 128), # 进行线性变换 nn.ReLU() # 进行ReLu激活 ) # 输出层(将全连接层的一维输出进行处理) self.fc2 = nn.Sequential( nn.Linear(128, 84), nn.ReLU() ) # 将输出层的数据进行分类(输出预测值) self.fc3 = nn.Linear(84, 62) # 定义前向传播过程，输入为x def forward(self, x): x = self.conv1(x) x = self.conv2(x) # nn.Linear()的输入输出都是维度为一的值，所以要把多维度的tensor展平成一维 x = x.view(x.size()[0], -1) x = self.fc1(x) x = self.fc2(x) x = self.fc3(x) return x# 超参数设置EPOCH = 20 # 遍历数据集次数(训练模型的轮数)BATCH_SIZE = 3 # 批处理尺寸(batch_size)：关于为何进行批处理，文档中有不错的介绍LR = 0.001 # 学习率：模型训练过程中每次优化的幅度# 定义数据预处理方式(将输入的类似numpy中arrary形式的数据转化为pytorch中的张量（tensor）)transform = transforms.ToTensor()# transform = torch.FloatTensor# 定义训练数据集(此处是加载MNIST手写数据集)trainset = tv.datasets.Traffic( root=train_data_dir, # 如果从本地加载数据集，对应的加载路径 train=True, # 训练模型 download=True, # 是否从网络下载训练数据集 transform=transform # 数据的转换形式)# 定义训练批处理数据trainloader = torch.utils.data.DataLoader( trainset, # 加载测试集 batch_size=BATCH_SIZE, # 最小批处理尺寸 shuffle=True, # 标识进行数据迭代时候将数据打乱)# 定义测试数据集testset = tv.datasets.Traffic( root=test_data_dir, # 如果从本地加载数据集，对应的加载路径 train=True, # 训练模型 download=True, # 是否从网络下载训练数据集 transform=transform # 数据的转换形式)# 定义测试批处理数据testloader = torch.utils.data.DataLoader( testset, # 加载测试集 batch_size=BATCH_SIZE, # 最小批处理尺寸 shuffle=False, # 标识进行数据迭代时候不将数据打乱)def model_train(): # 定义损失函数loss function 和优化方式（采用SGD） net = LeNet().to(device) criterion = nn.CrossEntropyLoss() # 交叉熵损失函数，通常用于多分类问题上 optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9) # 优化函数 for epoch in range(EPOCH): sum_loss = 0.0 # 数据读取（采用python的枚举方法获得标签和数据，这一部分可能和numpy相关） for i, data in enumerate(trainloader): inputs, labels = data # labels = [torch.LongTensor(label) for label in labels] # 将输入数据和标签放入构建的图中 注：图的概念可在pytorch入门中查 inputs, labels = inputs.to(device), labels.to(device) # 梯度清零 optimizer.zero_grad() # forward + backward 注: 这一部分是训练神经网络的核心 outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() # 反向自动求导 optimizer.step() # 进行优化 # 每训练100个batch打印一次平均loss sum_loss += loss.item() if i % 48 == 0: print('[%d, %d] loss: %.03f' % (epoch + 1, i + 1, sum_loss / 100)) sum_loss = 0.0 # 每跑完一次epoch测试一下准确率 with torch.no_grad(): correct = 0 total = 0 # for i, data in enumerate(testloader): for data in testloader: images, labels = data images, labels = images.to(device), labels.to(device) outputs = net(images) # 取得分最高的那个类 _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum() print('第%d个epoch的识别准确率为：%d%%' % (epoch + 1, (100 * correct / total))) torch.save(net.state_dict(), '%s/net_%03d.pth' % (opt.outf, epoch + 1))# 训练if __name__ == "__main__": model_train() 主要问题——数据读取PyTorch中数据读取的一个重要接口是torch.utils.data.DataLoader，该接口定义在dataloader.py脚本中，只要是用PyTorch来训练模型基本都会用到该接口，为了满足pytorch的数据读取要求，写了一个tv.datasets.Traffic的读取文件，是基于mnist数据集的读取进行编写的： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788from __future__ import print_functionimport torch.utils.data as datafrom PIL import Imageimport osimport os.pathimport errnoimport numpy as npimport torchimport codecsimport skimage.dataimport skimage.transformdef load_data(data_dir, train=True): """Loads a data set and returns two lists: images: a list of Numpy arrays, each representing an image. labels: a list of numbers that represent the images labels. 仅仅只是加载图片到数组，并没有对图片进行缩放比例 """ if train: # Get all subdirectories of data_dir. Each represents a label. directories = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))] # Loop through the label directories and collect the data in # two lists, labels and images. labels = [] images = [] for d in directories: label_dir = os.path.join(data_dir, d) file_names = [os.path.join(label_dir, f) for f in os.listdir(label_dir) if f.endswith(".ppm")] # For each label, load it's images and add them to the images list. # And add the label number (i.e. directory name) to the labels list. for f in file_names: images.append(skimage.data.imread(f)) labels.append(int(d)) # 为每一个图片加上标签 images28 = [skimage.transform.resize(image, (28, 28)) for image in images] labels_a = np.asarray(labels) images_a = np.asarray(images28) return images_a, labels_aclass Traffic(data.Dataset): ''' Traffic Dataset. ''' def __init__(self, root, train=True, transform=None, target_transform=None, download=False): self.root = os.path.expanduser(root) self.transform = transform self.target_transform = target_transform self.train = train # training set or test set if self.train: self.train_data, self.train_labels = load_data(root,train) else: self.train_data, self.train_labels = load_data(root,train) def __getitem__(self, index): """ Args: index (int): Index Returns: tuple: (image, target) where target is index of the target class. """ if self.train: img, target = self.train_data[index], self.train_labels[index] else: img, target = self.test_data[index], self.test_labels[index] # doing this so that it is consistent with all other datasets img = img.astype(np.float32) target = torch.LongTensor([target])[0] if self.transform is not None: img = self.transform(img) if self.target_transform is not None: target = self.target_transform(target) return img, target def __len__(self): if self.train: return len(self.train_data) else: return len(self.test_data) 注意: 在返回标签的时候，由于数据格式的问题，需要将标签放入一个list中，之后再转换为LongTensor，并取其第一个数据。]]></content>
      <categories>
        <category>Pytorch框架</category>
      </categories>
      <tags>
        <tag>交通标志识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习的基本概念]]></title>
    <url>%2F2018%2F10%2F29%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[机器学习和深度学习的关系在此不做赘述，本文主要说明机器学习的基本知识 学习算法机器学习算法是一种能够从数据中学习的算法，Mitchell（1997）提供了一个简洁的定义：“对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。” 任务T通常机器学习任务定义为机器学习系统应该如何处理样本（example）。样本是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的特征（feature）的集合。 一些非常常见的机器学习的任务列举如下： 分类 输入缺失分类 回归 转录 机器翻译 结构化输出 异常检测 合成和采样 确实值填补 去噪 密度估计或概率质量函数估计 当然，还有很多其他同类型或其他类型的任务，这里我们列举的任务类型只是用来介绍机器学习可以做哪些任务。 性能度量P为了评估机器学习算法的能力，引入了对其性能的定量度量。通常性能度量P是特定与系统执行的任务T而言的。 这就需要我们针对不同的的任务来寻找相对应的性能度量标准。 经验E根据学习过程中的不同经验，机器学习算法可以大致分类为无监督（unsupervised）算法和监督（supervised）算法。 无监督学习算法（unsupervised learning algorithm）：训练包含有很多特征的数据集，然后学习出这个数据集上有用的结构性质。 监督学习算法（supervised learning algorithm）：训练含有很多特征的数据集，不过数据集中的样本都有一个标签（label）或者目标（target）。 大部分学习算法可以被理解为在整个 数据集（dataset） 上获取经验。数据集可以用很多不同方式来表示，在所有的情况下，数据集都是样本的集合，而样本是特征的集合。 表示数据集的常用方法是 设计矩阵（design matrix） 容量、过拟合和欠拟合机器学习的主要挑战是我们的算法必须能够在先前未观测到的新输入上表现良好，而不只是在训练集上表现良好。在先前未观测到的输入上表现良好的能力被称为 泛化（generalization）。 训练误差（training error）：使用训练集，进行性能度量的一个量。来优化模型 泛化误差（generalization errot）：（也被称为测试误差（test error）），使用测试集，进行性能度量的一个量。 训练集和测试集通过数据集上被称为数据生成过程（data generating process）的概率分部生成。在生成数据集时，不会提前固定参数，然后采样得到两个数据集，一般是先采样得到训练集，然后挑选参数去降低训练集误差，然后采样得到测试集。在这个过程中，测试误差会大于或等于训练误差期望。以下是决定机器学习算法效果是否好的因素： 降低训练误差 缩小训练误差和测试误差的差距 这两个因素对应机器学习的两个主要挑战：欠拟合（underfitting）和过拟合（overfitting）。 通过调整模型的容量（capacity）可以控制模型是否偏向于过拟合或者欠拟合。通俗来讲，模型的容量是指其拟合各种函数的能力。容量低的模型可能很难拟合训练集，容量高的模型可能会过拟合，因为记住了不适用于测试集的训练集性质。一种控制训练算法容量的方法是选择假设空间（hypothesis space），即学习算法可以选择为解决方法的函数集。容量不仅取决与模型的选择，模型规定了调整参数降低训练目标时，学习算法可以从哪些函数中选择函数。这被称为模型的表示容量（representational capacity）。在很多情况下，由于额外的限制因素，比如优化算法的不完美，意味着学习算法的有效容量（effective capacity）可能小于模型的表示容量。 没有免费午餐定理机器学习的没有免费午餐定理（no free lunch theorem）表明（Wolpert，1996），在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率。换言之，在某种意义上，没有一个机器学习算法总是比其他的要好。 正则化正则化是指修改学习算法，使其降低泛化误差而非训练误差。正则化是机器学习领域领域的中心问题之一，只有优化能够与其重要性想提并论。 超参数和验证集大多数机器学习算法都有超参数，可以设置来控制算法行为。超参数的值不是通过学习算法本身学习出来的（尽管我们可以设置一个嵌套的学习过程，一个学习算法为另一个学习算法学习出最有超参数）用于挑选超参数的数据子集被称为验证集 交叉验证当数据集太小时，也有替代方法允许我们使用所有的样本估计平均测试误差，代价是增加了计算量。最常用的方法是K-折交叉验证过程。 K-折交叉验证：将数据集分成k个不重合的子集，测试误差可以估计为k次计算后的平均测试误差。在第i次测试时，数据的第i个自己用于测试集，其他的数据用于训练集，但是带来的一个问题是不存在平均误差方差的无偏估计（Bengio and Grandvalet，2004） 估计、偏差和方差统计领域为我们提供了很多工具来实现机器学习目标，不仅可以解决训练集上的任务，还可以泛化。基本的概念，例如参数估计、偏差和方差，对于正式的刻画泛化、欠拟合和过拟合都非常有帮助。 点估计点估计试图为一些感兴趣的量提供单个“最优”预测。一般地，感兴趣的量可以是单个参数，或是某些参数模型中的一个向量参数。 函数估计：有时我们会关注函数估计（或函数近似）。 偏差估计的偏差有 无偏（unbiased）和渐近无偏（asymptotically unbiased）。 伯努利分布 均值的高斯分布估计 高斯分布方差估计 样本方差 无偏样本方差 方差和标准差我们有时会考虑估计量的另一个性质是它作为数据样本的函数，期望的变化程度是多少。正如我们可以计算估计量的期望来决定它的偏差，我们也可以计算它的方差。估计量的方差就是一个方差，另外，方差的平方根被称为标准差（standard error）。 权衡偏差和方差以最小化均方误差偏差和方差度量着估计量的两个不同误差来源。偏差度量着偏离真实函数或参数的误差期望，而方差度量这数据上任意特定采样可能导致的估计期望的偏差。当面对一个偏差更大的估计和一个方差更大的估计时，我们如何进行选择。判断这种权衡最常用的方法是交叉验证，我们还可以比较这些估计的均方误差（mean squared error， MSE）。MSE度量着估计和真实值之间平方误差的总体期望偏差。 一致性一致性保证了估计量的偏差会随着样本数目的增多而减少。然而，反过来是不正确的——渐进无偏并不意味着一致性。 最大似然估计之前，我们已经看到过常用估计的定义，并分析了它们的性质。但是这些估计是从哪里来的呢？我们希望有些准则可以让我们从不同模型中得到特定函数作为好的估计，而不是猜测某些函数可能是好的估计，然后分析其偏差和方差。最常用的准则是最大似然估计。 条件对数似然和均方误差 线性回归作为最大似然 最大似然的性质最大似然估计最吸引人的地方在于，它被证明当样本数目趋向于无穷大时，就收敛率而言是最好的渐进估计。在合适的条件下，最大似然估计具有一致性，意味着训练样本数目趋向于无穷大时，参数的最大似然估计会收敛到参数的真实值。 贝叶斯统计前面讨论的属于频率派统计（frequentist statistics） 方法和基于估计单一值的方法，然后基于该估计作所有的预测。另一种方法是在做预测时会考虑所有可能的值。后者属于贝叶斯统计（Bayesian statistics）的范畴。 贝叶斯线性回归 最大后验（MAP）估计 MAP贝叶斯推断提供了一个直观的方法来设计复杂但可解释的正则化项。例如，更复杂的惩罚项可以通过混合高斯分部作为先验得到，而不是一个单独的高斯分布（Nowlan and Hinton，1992） 监督学习算法粗略的说，监督学习算法是给定一组输入x和输出y的训练集，学习如何关联输入和输出。在许多情况下，输出y很难自动收集，必须由人来提供“监督”，不过该术语仍然适用于训练集目标可以被自动收集的情况。 概率监督学习支持向量机支持向量机（support vector machine，SVM）是监督学习中最有影响力的方法之一。类似于逻辑回归，但支持向量机不输出概率，只输出类别。支持向量机的一个重要创新是核技巧（kernal trick）。核技巧观察到许多机器学习算法都可以写成样本间点积的形式。最常用的核函数是高斯核（Gaussian kernel），这个核也被称为径向基函数（radial basis function，RBF）核。支持向量机不是唯一可以使用核技巧来增强的算法，许多其他的线性模型也可以通过这种方式来增强。使用核技巧的算法类别被称为和机器（kernel machine）或核方法（kernel method）。核机器的一个主要缺点是计算决策函数的成本关于训练样本的数目是线性的。而训练的样本则被称做支持向量（support vector）。 其他简单的机器学习算法 决策树 无监督学习算法无监督算法只处理“特征”，不操作监督信号。监督和无监督算法之间的区别没有规范严格的定义，因为没有客观的判断来区分监督者提供的值是特征还是目标。通俗的说，无监督学习的大多数尝试是指从不需要人为注释的样本的分布中抽取信息。该术语通常与密度估计相关，学习从分布中采样、学习从分布中去噪、寻找数据分布的流形或是将数据中相关的样本聚类。 主成分分析k-均值聚类随机梯度下降几乎所有的深度学习算法都用到了一个非常重要的算法：梯度下降算法（stochastic gradient,SGD）。机器学习中反复出现的一个问题是好的泛化需要大的训练集，但大的计算集的计算代价也更大。机器学习算法中的代价函数通常可以分解成每个样本的代价函数的总和。随机梯度下降的核心是，梯度是期望。期望可使用小规模的样本近似估计。 构建机器学习算法构建机器学习算法促使深度学习发展的挑战 维度灾难 局部不变性和平滑正则化 流形学习]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch--线性回归和逻辑回归]]></title>
    <url>%2F2018%2F10%2F20%2FPytorch-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[代码如下利用torch中的线性回归和逻辑回归模块实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768'''torch 一维线性回归算法'''import numpy as npimport torchfrom torch import nn, optimfrom torch.autograd import Variable import matplotlib.pyplot as plt # 生成训练数据np.random.seed(10)x = np.linspace(0, 30, 20)y = x * 3 + np.random.normal(0, 5, 20)x = np.array(x, dtype=np.float32).reshape([20, 1])y = np.array(y, dtype=np.float32).reshape([20, 1])# 将数据转换为torch中的张量形式x_train = torch.from_numpy(x)y_train = torch.from_numpy(y)class LinearRegression(nn.Module): ''' 线性回归模型：一维线性回归 ''' def __init__(self): super(LinearRegression, self).__init__() self.linear = nn.Linear(1, 1) def forward(self, x): out = self.linear(x) return outif torch.cuda.is_available(): model = LinearRegression().cuda()else: model = LinearRegression()criterion = nn.MSELoss()optimizer = optim.SGD(model.parameters(), lr=0.001)num_epochs = 1000for epoch in range(num_epochs): if torch.cuda.is_available(): inputs = Variable(x_train).cuda() target = Variable(y_train).cuda() else: inputs = Variable(x_train) target = Variable(y_train) optimizer.zero_grad() out = model(inputs) loss = criterion(out, target) loss.backward() optimizer.step() if (epoch+1) % 20 == 0: print('Epoch[&#123;&#125;/&#123;&#125;], loss:&#123;:.6f&#125;'.format(epoch+1, num_epochs, loss.data[0]))model.eval()predict = model(Variable(x_train))predict = predict.data.numpy()plt.plot(x_train.numpy(), y_train.numpy(), 'ro', label='Original data')plt.plot(x_train.numpy(), predict, label='predict data')plt.legend()plt.show() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970'''torch 一维线性回归算法(多项式回归)'''# 生成训练数据def make_features(x): ''' 建立多项式特征 ''' x = x.unsqueeze(1) return torch.cat([x ** i for i in range(1, 4)], 1)W_target = torch.FloatTensor([0.5, 3, 2.4]).unsqueeze(1)b_target = torch.FloatTensor([0.9])def f(x): ''' 实际函数 ''' return x.mm(W_target) + b_target[0]def get_batch(batch_size=32): ''' 生成训练数据 ''' random = torch.randn(batch_size) x = make_features(random) y = f(x) # print(random,x) if torch.cuda.is_available(): return Variable(x).cuda(), Variable(y).cuda() else: return Variable(random), Variable(x), Variable(y)class poly_model(nn.Module): ''' 多项式线性回归（三维） ''' def __init__(self): super(poly_model, self).__init__() self.poly = nn.Linear(3, 1) def forward(self, x): out = self.poly(x) return outif torch.cuda.is_available(): model = poly_model().cuda()else: model = poly_model()criterion = nn.MSELoss()optimizer = optim.SGD(model.parameters(), lr=0.001)epoch = 0while True: _, batch_x, batch_y = get_batch() optimizer.zero_grad() out = model(batch_x) loss = criterion(out, batch_y) print_loss = loss.data[0] loss.backward() optimizer.step() print(print_loss) epoch += 1 if print_loss &lt; 0.001: break 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364'''逻辑回归'''import numpy as npimport torchfrom torch import nn, optimfrom torch.autograd import Variable import matplotlib.pyplot as plt class LogisticRegression(nn.Module): def __init__(self): super(LogisticRegression, self).__init__() self.lr = nn.Linear(2,1) self.sm = nn.Sigmoid() def forward(self, x): x = self.lr(x) x = self.sm(x) return xlogistic_model = LogisticRegression()if torch.cuda.is_available(): logistic_model.cuda()criterion = nn.BCELoss()optimezer = torch.optim.SGD(logistic_model.parameters(), lr=0.001, momentum=0.9)for epoch in range(50000): if torch.cuda.is_available(): x = Variable(x_data).cuda() y = Variable(y_data).cuda() else: x = Variable(x_data) y = Variable(y_data) out = logistic_model(x) loss = criterion(out, y) print_loss = loss.data[0] mask = out.ge(0.5).float() correct = (mask == y).sum() acc = correct.data[0] / x.size(0) optimizer.zero_grad() loss.backward() optimizer.step() if (epoch+1) % 1000 == 0: print('*'*10) print('epoch &#123;&#125;'.format(epoch+1)) print('loss is &#123;:.4f&#125;'.format(print_loss)) print('acc is &#123;:.4f&#125;'.format(acc)) 0w0, w1 = logistic_model.lr.weight[0]w0 = w0.data[0]w1 = w1.data[0]b = logistic_model.lr.bias.data[0]plot_x = np.arrange(30,100,0.1)plot_y = (-w0 * plot_x -b)/ w1plot.plot(plot_x, plot_y)plt.show()]]></content>
      <categories>
        <category>Pytorch框架</category>
      </categories>
      <tags>
        <tag>线性回归</tag>
        <tag>逻辑回归</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习——线性回归和逻辑回归]]></title>
    <url>%2F2018%2F10%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[线性回归简述在统计学中，线性回归（Linear Regression）是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合（自变量都是一次方）。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。简单来说，就是找到一条直线去拟合数据点。如下图： 优点：结果易于理解，计算上不复杂。缺点：对非线性数据拟合不好。适用数据类型：数值型和标称型数据。算法类型：回归算法 线性回归的模型函数如下： $$h_\theta = \theta^Tx$$ 它的损失函数如下： $$J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2$$ 通过训练数据集寻找参数的最优解，即求解可以得到$minJ(θ)$的参数向量$θ$,其中这里的参数向量也可以分为参数和$w$和$b$,分别表示权重和偏置值。求解最优解的方法有最小二乘法和梯度下降法。 梯度下降法 梯度下降算法的思想如下(这里以一元线性回归为例)： 首先，我们有一个代价函数，假设是$J(θ_0,θ_1)$，我们的目标是$minθ_0,θ_1 J(θ_0,θ_1)$。 接下来的做法是： 首先是随机选择一个参数的组合$(θ_0,θ_1)$,一般是设$θ_0=0,θ_1=0$; 然后是不断改变$(θ_0,θ_1)$，并计算代价函数，直到一个局部最小值。之所以是局部最小值，是因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值，选择不同的初始参数组合，可能会找到不同的局部最小值。下面给出梯度下降算法的公式：repeat until convergence{ $$\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(θ_0,θ_1)(for\ j =0\ and\ j=1)$$}也就是在梯度下降中，不断重复上述公式直到收敛，也就是找到局部最小值局部最小值。其中符号$:=$是赋值符号的意思。 而应用梯度下降法到线性回归，则公式如下： $$\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{i})-y^i)$$$$\theta_1 := \theta_1 - \alpha\frac{1}{m}\sum_{i=1}^m((h_\theta(x^i)-y^i)\cdot x^i)$$ 公式中的$\alpha$称为学习率(learning rate)，它决定了我们沿着能让代价函数下降程度最大的方向向下迈进的步子有多大。在梯度下降中，还涉及都一个参数更新的问题，即更新$(\theta_0,\theta_1)$,一般我们的做法是同步更新.最后，上述梯度下降算法公式实际上是一个叫批量梯度下降(batch gradient descent)，即它在每次梯度下降中都是使用整个训练集的数据，所以公式中是带有$ \sum_{i=1}^m $. 岭回归（ridge regression） 岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价，获得回归系数更为符合实际、更可靠的回归方法，对病态数据的耐受性远远强于最小二乘法。 岭回归分析法是从根本上消除复共线性影响的统计方法。岭回归模型通过在相关矩阵中引入一个很小的岭参数$K（1&gt;K&gt;0）$，并将它加到主对角线元素上，从而降低参数的最小二乘估计中复共线特征向量的影响，减小复共线变量系数最小二乘估计的方法，以保证参数估计更接近真实情况。岭回归分析将所有的变量引入模型中，比逐步回归分析提供更多的信息。 代码实现线性回归的相关数据及代码点此 使用sklearn包中的线性回归算法 123456789101112131415161718192021222324252627282930313233343536373839404142434445import matplotlib.pyplot as pltimport numpy as npfrom sklearn import datasets, linear_modelfrom sklearn.metrics import mean_squared_error, r2_score# Load the diabetes datasetdiabetes = datasets.load_diabetes()# Use only one featurediabetes_X = diabetes.data[:, np.newaxis, 2]# Split the data into training/testing setsdiabetes_X_train = diabetes_X[:-20]diabetes_X_test = diabetes_X[-20:]# Split the targets into training/testing setsdiabetes_y_train = diabetes.target[:-20]diabetes_y_test = diabetes.target[-20:]# Create linear regression objectregr = linear_model.LinearRegression()# Train the model using the training setsregr.fit(diabetes_X_train, diabetes_y_train)# Make predictions using the testing setdiabetes_y_pred = regr.predict(diabetes_X_test)# The coefficientsprint('Coefficients: \n', regr.coef_)# The mean squared errorprint("Mean squared error: %.2f" % mean_squared_error(diabetes_y_test, diabetes_y_pred))# Explained variance score: 1 is perfect predictionprint('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))# Plot outputsplt.scatter(diabetes_X_test, diabetes_y_test, color='black')plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)plt.xticks(())plt.yticks(())plt.show() 使用代码实现算法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import osimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltpath = os.path.dirname(os.getcwd()) + '\data\ex1data1.txt'data = pd.read_csv(path, header=None, names=['Population', 'Profit'])def computeCost(X, y, theta): ''' 损失函数 X: 自变量 y: 因变量 theta: 参数向量 ''' inner = np.power(((X * theta.T) - y), 2) return np.sum(inner) / (2 * len(X))def gradientDescent(X, y, theta, alpha, iters): ''' 梯度下降算法 X: 自变量 y: 因变量 theta: 参数向量 alpha: 学习率 iters: 计算次数 ''' # 暂存参数向量 temp = np.matrix(np.zeros(theta.shape)) # 将参数向量降为一维，返回视图，可以修改原始的参数向量 parameters = int(theta.ravel().shape[1]) # 损失值消耗记录 cost = np.zeros(iters) # 梯度下降的计算 for i in range(iters): error = (X * theta.T) - y for j in range(parameters): term = np.multiply(error, X[:, j]) temp[0, j] = theta[0, j] - ((alpha / len(X)) * np.sum(term)) theta = temp cost[i] = computeCost(X, y, theta) return theta, cost# append a ones column to the front of the data setdata.insert(0, 'Ones', 1)# set X (training data) and y (target variable)cols = data.shape[1]X = data.iloc[:, 0:cols - 1]y = data.iloc[:, cols - 1:cols]# convert from data frames to numpy matricesX = np.matrix(X.values)y = np.matrix(y.values)theta = np.matrix(np.array([0, 0]))# initialize variables for learning rate and iterationsalpha = 0.01iters = 1000# perform gradient descent to "fit" the model parametersg, cost = gradientDescent(X, y, theta, alpha, iters)x = np.linspace(data.Population.min(), data.Population.max(), 100)f = g[0, 0] + (g[0, 1] * x)fig, ax = plt.subplots(figsize=(8, 6))ax.plot(x, f, 'r', label='Prediction')ax.scatter(data.Population, data.Profit, label='Traning Data')ax.legend(loc=2)ax.set_xlabel('Population')ax.set_ylabel('Profit')ax.set_title('Predicted Profit vs. Population Size')plt.show()# 查看损失值的变化# fig, ax = plt.subplots(figsize=(12,8))# ax.plot(np.arange(iters), cost, 'r')# ax.set_xlabel('Iterations')# ax.set_ylabel('Cost')# ax.set_title('Error vs. Training Epoch') 逻辑回归简述Logistic回归算法基于$Sigmoid$函数，或者说$Sigmoid$就是逻辑回归函数。$Sigmoid$函数定义如下： $\frac{1}{1+e^{-z}}$。函数值域范围$(0,1)$。因此逻辑回归函数的表达式如下： $$h_\theta = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}}$$$$其中，g(z) = \frac{1}{1+e^{-z}}$$ 其导数形式为： $$g\prime(z) = \frac{d}{dz}\frac{1}{1+e^{-z}}$$$$=\frac{1}{(1+e^{-z})^2}(e^{-z})$$$$=\frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}})$$$$ = g(z)(1-g(z))$$ 代价函数逻辑回归方法主要是用最大似然估计来学习的，所以单个样本的后验概率为： $$p(y|x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}$$ 到整个样本的后验概率就是: $$L(\theta) = p(y|X;\theta)$$$$ = \prod_{i=1}^{m}p(y^i|x^i;\theta)$$$$ = \prod_{i=1}^{m}(h_\theta(x^i))^{y^i}(1-h_\theta(x^i))^{1-y^i}$$$$其中，P(y=1|x;\theta)=h_\theta(x),P(y=0|x;\theta)=1-h_\theta(x)$$$$通过对数进一步简化有：l(\theta) = \log L(\theta) = \sum_{i=1}^m(y^i\log h(x^i)+(1-y^i)\log(1-h(x^i)))$$ 而逻辑回归的代价函数就是$−l(\theta)$。也就是如下所示： $$J(\theta) = \frac{1}{m}\left[\sum_{i=1}^{m}y^i\log h_\theta(x^i)+(1-y^i)\log(1-h_\theta(x^i))\right]$$ 同样可以使用梯度下降算法来求解使得代价函数最小的参数。其梯度下降法公式为： $$\frac{\partial}{\partial\theta_j}l(\theta) = \left(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)}\right)\frac{\partial}{\partial\theta_j}g(\theta^Tx)$$$$= \left(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)}\right)\left(1-g(\theta^Tx)\frac{\partial}{\partial\theta_j}(\theta^Tx)\right)g(\theta^Tx)$$$$= (y(1-g(\theta^Tx))-(1-y)g(\theta^Tx))x_j$$$$= (y-h_\theta(x))x_j$$ $$\theta_j := \theta_j + \alpha(y^i-h_\theta(x^i)x_j^i$$ 总结 优点： 1、实现简单； 2、分类时计算量非常小，速度很快，存储资源低； 缺点： 1、容易欠拟合，一般准确度不太高 2、只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分； 适用数据类型：数值型和标称型数据。 类别：分类算法。 试用场景：解决二分类问题。 如下图： 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061'''Plot multinomial and One-vs-Rest Logistic Regression'''import numpy as npimport matplotlib.pyplot as pltfrom sklearn.datasets import make_blobsfrom sklearn.linear_model import LogisticRegression# make 3-class dataset for classificationcenters = [[-5, 0], [0, 1.5], [5, -1]]X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)transformation = [[0.4, 0.2], [-0.4, 1.2]]X = np.dot(X, transformation)for multi_class in ('multinomial', 'ovr'): clf = LogisticRegression(solver='sag', max_iter=100, random_state=42, multi_class=multi_class).fit(X, y) # print the training scores print("training score : %.3f (%s)" % (clf.score(X, y), multi_class)) # create a mesh to plot in h = .02 # step size in the mesh x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) # Put the result into a color plot Z = Z.reshape(xx.shape) plt.figure() plt.contourf(xx, yy, Z, cmap=plt.cm.Paired) plt.title("Decision surface of LogisticRegression (%s)" % multi_class) plt.axis('tight') # Plot also the training points colors = "bry" for i, color in zip(clf.classes_, colors): idx = np.where(y == i) plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired, edgecolor='black', s=20) # Plot the three one-against-all classifiers xmin, xmax = plt.xlim() ymin, ymax = plt.ylim() coef = clf.coef_ intercept = clf.intercept_ def plot_hyperplane(c, color): def line(x0): return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1] plt.plot([xmin, xmax], [line(xmin), line(xmax)], ls="--", color=color) for i, color in zip(clf.classes_, colors): plot_hyperplane(i, color)plt.show() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687'''代码实现(加入正则化)'''import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport scipy.optimize as optimport ospath = os.path.dirname(os.getcwd()) + '\data\ex2data1.txt'data2 = pd.read_csv(path, header=None, names=['Test 1', 'Test 2', 'Accepted'])positive = data2[data2['Accepted'].isin([1])]negative = data2[data2['Accepted'].isin([0])]def sigmoid(z): return 1 / (1 + np.exp(-z))def costReg(theta, X, y, learningRate): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(-y, np.log(sigmoid(X * theta.T))) second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T))) reg = (learningRate / 2 * len(X)) * np.sum(np.power(theta[:, 1:theta.shape[1]], 2)) return np.sum(first - second) / (len(X)) + regdef gradientReg(theta, X, y, learningRate): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) parameters = int(theta.ravel().shape[1]) grad = np.zeros(parameters) error = sigmoid(X * theta.T) - y for i in range(parameters): term = np.multiply(error, X[:,i]) if (i == 0): grad[i] = np.sum(term) / len(X) else: grad[i] = (np.sum(term) / len(X)) + ((learningRate / len(X)) * theta[:,i]) return graddef predict(theta, X): probability = sigmoid(X * theta.T) return [1 if x &gt;= 0.5 else 0 for x in probability]degree = 5x1 = data2['Test 1']x2 = data2['Test 2']data2.insert(3, 'Ones', 1)for i in range(1, degree): for j in range(0, i): data2['F' + str(i) + str(j)] = np.power(x1, i-j) * np.power(x2, j)data2.drop('Test 1', axis=1, inplace=True)data2.drop('Test 2', axis=1, inplace=True)# set X and y (remember from above that we moved the label to column 0)cols = data2.shape[1]X2 = data2.iloc[:,1:cols]y2 = data2.iloc[:,0:1]# convert to numpy arrays and initalize the parameter array thetaX2 = np.array(X2.values)y2 = np.array(y2.values)theta2 = np.zeros(11)learningRate = 0.1result2 = opt.fmin_tnc(func=costReg, x0=theta2, fprime=gradientReg, args=(X2, y2, learningRate))# print(costReg(theta2, X2, y2, learningRate))theta_min = np.matrix(result2[0])predictions = predict(theta_min, X2)correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y2)]accuracy = (sum(map(int, correct)) % len(correct))print('accuracy = &#123;0&#125;%'.format(accuracy))]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>线性回归</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scikit-learn整理]]></title>
    <url>%2F2018%2F10%2F17%2Fscikit-learn%E6%95%B4%E7%90%86%2F</url>
    <content type="text"></content>
      <categories>
        <category>Python第三方包</category>
      </categories>
      <tags>
        <tag>scikit-learn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask整理]]></title>
    <url>%2F2018%2F10%2F17%2FFlask%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[使用工具Flask后端 + Postgresql数据库 + JS前端（我未使用）Flask搭建 确定确定目录结构 app/algorithms: 用来存放相关的算法文件 app/models: 用来存放数据库的操作 app/web: 用来存放路由和视图函数 manage: flask的启动文件 确定路由注册方式 使用蓝图形式来注册路由 确定数据库操作方式 使用sqlalchemy及psycopg2来控制Postgresql数据库 由于主要是用来进行数据读取的，所以采用非ORM方式构建的表结构，这种方式方便进行查询过滤操作 基于sqlalchemy的Postgresql数据库访问操作 创建表结构 12345678910111213141516171819from sqlalchemy.engine import create_enginefrom sqlalchemy.schema import MetaData, Table, Column, ForeignKey, Sequencefrom sqlalchemy.types import *from sqlalchemy.sql.expression import select,and_from datetime import datetimeengine = create_engine('postgres://user:password@hosts/builder', echo=True)metadata = MetaData()metadata.bind = engine # 创建桥梁索引表bridges_table = Table('bridges', metadata, Column('id', Integer, primary_key=True), Column('org_id', Integer, nullable=False), Column('user_id', Integer, nullable=False), Column('name', VARCHAR(length=255), nullable=False), Column('created_date', TIMESTAMP, nullable=False), Column('finished_date', TIMESTAMP, nullable=True), # autoload=True, ) 这种方式，有助于进行表查询，具体的相关API介绍及使用放那格式可点此查看。 相关操作 1234567891011121314151617181920212223# 添加数据def add(): s = book_table.insert().values(title='测试写入2',time=datetime.now()) c = engine.execute(s) c.close() return c.inserted_primary_key# 查询数据def query_code(id): info = &#123;'id': '', 'title': ''&#125; s = select([bridge_jobs_table.c.id.label('name')]).where(and_(bridge_jobs_table.c.kind=='桩基',bridge_jobs_table.c.name=='起钻')) codename_query = engine.execute(s) print(codename_query.keys()) for row in codename_query: print(row[0]) codename_query.close() return info# 更新数据def updata(id, title): s = book_table.update().where(book_table.c.id == id).values(title=title, id=id) c = engine.execute(s) c.close() Flask相关知识 路由操作 静态路由 123@web.route('/hello', methods=['POST', 'GET'])def hello(): return 'hello world!' 参数路由 123@web.route('/hello/&lt;string:name&gt;', methods=['POST', 'GET'])def hello(name): return 'hello %d '% name JSON返回 123@web.route('/hello/&lt;string:name&gt;', methods=['POST', 'GET'])def hello(name): return jsonify('hello %d '% name) 使用蓝图方式注册路由 12345678910111213141516from flask import Flaskdef create_app(): app = Flask(__name__) app.config.from_object('app.setting') register_blueprint(app) # db.init_app(app) # db.create_app(app=app) return appdef register_blueprint(app): from app.web.view import web app.register_blueprint(web)]]></content>
      <categories>
        <category>Python后端</category>
      </categories>
      <tags>
        <tag>flask框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自省]]></title>
    <url>%2F2018%2F10%2F06%2F%E8%87%AA%E7%9C%81%2F</url>
    <content type="text"><![CDATA[function doDecrypt (pwd, onError) { console.log('in doDecrypt'); const txt = document.getElementById('enc_content').innerHTML; let plantext; try { const bytes = CryptoJS.AES.decrypt(txt, pwd); var plaintext = bytes.toString(CryptoJS.enc.Utf8); } catch(err) { if(onError) { onError(err); } return; } document.getElementById('enc_content').innerHTML = plaintext; document.getElementById('enc_content').style.display = 'block'; document.getElementById('enc_passwd').style.display = 'none'; if(typeof MathJax !== 'undefined') { MathJax.Hub.Queue( ['resetEquationNumbers', MathJax.InputJax.TeX], ['PreProcess', MathJax.Hub], ['Reprocess', MathJax.Hub] ); } } U2FsdGVkX1+zrouZUOMQaRT6NOMR2n0rLcHCoCnVMgPcisyzGsy7MzMKQskomGx7vmiytICm+rXsFIe0ibIWlEWzFmIb0fpf4zmTd6CprmrBdt6AtOnsgnALm1rwAQjKZ9dMQN1Pd46UcWEz8sZim4cn5veo6EILSW0lZ6GzXl6aul+zOJ7Hq6wGEc1e06sgG2ahdsSiWIgZjU2Gqf5pr0EkQBJH8tk4zGoARPCYt7Q9H43JbKV/OWSJtHzfAxmkBUlBmMXHpIHTK2c3QtXDfKhtDAHJriEgRkNhMb6cNXEx8mwCXkbJ37r5tq9+ExHi2xqKDrNT7aEZehzsTUxGa+hI/gZgZdRSyqZSr5tsOwwf/+A0Ex1DHZJivMk/QqDHqeNGV77MqEps8HT7F9TFcGCV9Y2hq8qsYD0xauExa1RjOH8FJUlPvVgKqyQCFgk5ZzdVRxzXciHLrcfs9pdcrYs2DgJwuq70EhtkjuDQL7YwZAPM/PKv1VJPMH4klIJ4 var onError = function(error) { document.getElementById("enc_error").innerHTML = "password error!" }; function decrypt() { var passwd = document.getElementById("enc_pwd_input").value; console.log(passwd); doDecrypt(passwd, onError); }]]></content>
      <categories>
        <category>感悟</category>
      </categories>
      <tags>
        <tag>自我反省</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PostgreSQL学习记录]]></title>
    <url>%2F2018%2F09%2F26%2FPostgreSQL%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[selectfetchall()fetchmany()fetchone()]]></content>
  </entry>
  <entry>
    <title><![CDATA[docker基础]]></title>
    <url>%2F2018%2F09%2F03%2Fdocker%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[tensorflow入门整理]]></title>
    <url>%2F2018%2F08%2F05%2Ftensorflow%E5%85%A5%E9%97%A8%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[基本概念 tf.placeholder() 是tensorflow的一种特殊变量，这种变量并非在初始化时定义好内容，而是在训练的时候才将数据填入其中。]]></content>
      <categories>
        <category>框架</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim基本使用方法]]></title>
    <url>%2F2018%2F08%2F01%2Fvim%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[vi的基本概念基本上vi可以分为三种状态，分别是命令模式（command mode）、插入模式（Insert mode）和底行模式（last line mode），各模式的功能区分如下： 命令行模式command mode）控制屏幕光标的移动，字符、字或行的删除，移动复制某区段及进入Insert mode下，或者到 last line mode。 插入模式（Insert mode）只有在Insert mode下，才可以做文字输入，按「ESC」键可回到命令行模式。 底行模式（last line mode）将文件保存或退出vi，也可以设置编辑环境，如寻找字符串、列出行号……等。 不过一般我们在使用时把vi简化成两个模式，就是将底行模式（last line mode）也算入命令行模式command mode）。 vi的基本操作 进入vi在系统提示符号输入vi及文件名称后，就进入vi全屏幕编辑画面：$ vi myfile。不过有一点要特别注意，就是您进入vi之后，是处于「命令行模式（command mode）」，您要切换到「插入模式（Insert mode）」才能够输入文字。初次使用vi的人都会想先用上下左右键移动光标，结果电脑一直哔哔叫，把自己气个半死，所以进入vi后，先不要乱动，转换到「插入模式（Insert mode）」再说吧！ 切换至插入模式（Insert mode）编辑文件在「命令行模式（command mode）」下按一下字母「i」就可以进入「插入模式（Insert mode）」，这时候你就可以开始输入文字了。 Insert 的切换您目前处于「插入模式（Insert mode）」，您就只能一直输入文字，如果您发现输错了字！想用光标键往回移动，将该字删除，就要先按一下「ESC」键转到「命令行模式（command mode）」再删除文字。 退出vi及保存文件在「命令行模式（command mode）」下，按一下「：」冒号键进入「Last line mode」，例如：: w filename （输入 「w filename」将文章以指定的文件名filename保存）: wq (输入「wq」，存盘并退出vi): q! (输入q!， 不存盘强制退出vi) 命令行模式（command mode）功能键 插入模式按「i」切换进入插入模式「insert mode」，按“i”进入插入模式后是从光标当前位置开始输入文件；按「a」进入插入模式后，是从目前光标所在位置的下一个位置开始输入文字；按「o」进入插入模式后，是插入新的一行，从行首开始输入文字。 从插入模式切换为命令行模式按「ESC」键。 移动光标vi可以直接用键盘上的光标来上下左右移动，但正规的vi是用小写英文字母「h」、「j」、「k」、「l」，分别控制光标左、下、上、右移一格。按「ctrl」+「b」：屏幕往“后”移动一页。按「ctrl」+「f」：屏幕往“前”移动一页。按「ctrl」+「u」：屏幕往“后”移动半页。按「ctrl」+「d」：屏幕往“前”移动半页。按数字「0」：移到文章的开头。按「G」：移动到文章的最后。按「$」：移动到光标所在行的“行尾”。按「^」：移动到光标所在行的“行首”按「w」：光标跳到下个字的开头按「e」：光标跳到下个字的字尾按「b」：光标回到上个字的开头按「#l」：光标移到该行的第#个位置，如：5l,56l。 删除文字「x」：每按一次，删除光标所在位置的“后面”一个字符。「#x」：例如，「6x」表示删除光标所在位置的“后面”6个字符。「X」：大写的X，每按一次，删除光标所在位置的“前面”一个字符。「#X」：例如，「20X」表示删除光标所在位置的“前面”20个字符。「dd」：删除光标所在行。「#dd」：从光标所在行开始删除#行 复制「yw」：将光标所在之处到字尾的字符复制到缓冲区中。「#yw」：复制#个字到缓冲区「yy」：复制光标所在行到缓冲区。「#yy」：例如，「6yy」表示拷贝从光标所在的该行“往下数”6行文字。「p」：将缓冲区内的字符贴到光标所在位置。注意：所有与“y”有关的复制命令都必须与“p”配合才能完成复制与粘贴功能。 替换「r」：替换光标所在处的字符。「R」：替换光标所到之处的字符，直到按下「ESC」键为止。 回复上一次操作「u」：如果您误执行一个命令，可以马上按下「u」，回到上一个操作。按多次“u”可以执行多次回复。 更改「cw」：更改光标所在处的字到字尾处「c#w」：例如，「c3w」表示更改3个字 跳至指定的行「ctrl」+「g」列出光标所在行的行号。「#G」：例如，「15G」，表示移动光标至文章的第15行行首。 Last line mode下命令简介 在使用「last line mode」之前，请记住先按「ESC」键确定您已经处于「command mode」下后，再按「：」冒号即可进入「last line mode」。 列出行号「set nu」：输入「set nu」后，会在文件中的每一行前面列出行号。 跳到文件中的某一行「#」：「#」号表示一个数字，在冒号后输入一个数字，再按回车键就会跳到该行了，如输入数字15，再回车，就会跳到文章的第15行。 查找字符「/关键字」：先按「/」键，再输入您想寻找的字符，如果第一次找的关键字不是您想要的，可以一直按「n」会往后寻找到您要的关键字为止。「?关键字」：先按「?」键，再输入您想寻找的字符，如果第一次找的关键字不是您想要的，可以一直按「n」会往前寻找到您要的关键字为止。 保存文件「w」：在冒号输入字母「w」就可以将文件保存起来。 离开vi「q」：按「q」就是退出，如果无法离开vi，可以在「q」后跟一个「!」强制离开vi。「qw」：一般建议离开时，搭配「w」一起使用，这样在退出的时候还可以保存文件。 vi命令列表 下表列出命令模式下的一些键的功能： h左移光标一个字符 l右移光标一个字符 k光标上移一行 j光标下移一行 ^光标移动至行首 0数字“0”，光标移至文章的开头 G光标移至文章的最后 $光标移动至行尾 Ctrl+f向前翻屏 Ctrl+b向后翻屏 Ctrl+d向前翻半屏 Ctrl+u向后翻半屏 i在光标位置前插入字符 a在光标所在位置的后一个字符开始增加 o插入新的一行，从行首开始输入 ESC从输入状态退至命令状态 x删除光标后面的字符 #x删除光标后的＃个字符 X(大写X)，删除光标前面的字符 #X删除光标前面的#个字符 dd删除光标所在的行 #dd删除从光标所在行数的#行 yw复制光标所在位置的一个字 #yw复制光标所在位置的#个字 yy复制光标所在位置的一行 #yy复制从光标所在行数的#行 p粘贴 u取消操作 cw更改光标所在位置的一个字 #cw更改光标所在位置的#个字 下表列出行命令模式下的一些指令 w filename储存正在编辑的文件为filename wq filename储存正在编辑的文件为filename，并退出vi q!放弃所有修改，退出vi set nu显示行号 /或?查找，在/后输入要查找的内容 n与/或?一起使用，如果查找的内容不是想要找的关键字，按n或向后（与/联用）或向前（与?联用）继续查找，直到找到为止。 高手总结的图：]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch入门]]></title>
    <url>%2F2018%2F08%2F01%2FPyTorch%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[基本概念 张量 张量类似于NumPy的ndarray，另外还有Tensors也可用于GPU以加速计算。 from __future__ import print_function import torch 构造一个未初始化的5x3矩阵： x = torch.empty(5, 3) print(x) tensor([[ 3.2401e+18, 0.0000e+00, 1.3474e-08], [ 4.5586e-41, 1.3476e-08, 4.5586e-41], [ 1.3476e-08, 4.5586e-41, 1.3474e-08], [ 4.5586e-41, 1.3475e-08, 4.5586e-41], [ 1.3476e-08, 4.5586e-41, 1.3476e-08]]) 构造一个矩阵填充的零和dtype long： x = torch.zeros(5, 3, dtype=torch.long) print(x) tensor([[ 0, 0, 0], [ 0, 0, 0], [ 0, 0, 0], [ 0, 0, 0], [ 0, 0, 0]]) 直接从数据构造张量： x = torch.tensor([5.5, 3]) print(x) tensor([ 5.5000, 3.0000]) 或者根据现有的张量创建张量。除非用户提供新值，否则这些方法将重用输入张量的属性，例如dtype x = x.new_ones(5, 3, dtype=torch.double) # new_* methods take in sizes print(x) x = torch.randn_like(x, dtype=torch.float) # override dtype! print(x) # result has the same size tensor([[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.]], dtype=torch.float64) tensor([[ 0.2641, 0.0149, 0.7355], [ 0.6106, -1.2480, 1.0592], [ 2.6305, 0.5582, 0.3042], [-1.4410, 2.4951, -0.0818], [ 0.8605, 0.0001, -0.7220]]) 得到它的大小： print(x.size()) torch.Size([5, 3]) 注意 torch.Size 实际上是一个元组，因此它支持所有元组操作。 操作 操作有多种语法。在下面的示例中，我们将查看添加操作。 增加：语法1 y = torch.rand(5, 3) print(x + y) tensor([[ 0.7355, 0.2798, 0.9392], [ 1.0300, -0.6085, 1.7991], [ 2.8120, 1.2438, 1.2999], [-1.0534, 2.8053, 0.0163], [ 1.4088, 0.9000, -0.1172]]) 增加：语法2 print(torch.add(x, y)) tensor([[ 0.7355, 0.2798, 0.9392], [ 1.0300, -0.6085, 1.7991], [ 2.8120, 1.2438, 1.2999], [-1.0534, 2.8053, 0.0163], [ 1.4088, 0.9000, -0.1172]]) 增加：提供输出张量作为参数 result = torch.empty(5, 3) torch.add(x, y, out=result) print(result) tensor([[ 0.7355, 0.2798, 0.9392], [ 1.0300, -0.6085, 1.7991], [ 2.8120, 1.2438, 1.2999], [-1.0534, 2.8053, 0.0163], [ 1.4088, 0.9000, -0.1172]]) 增加：就地 # adds x to y y.add_(x) print(y) tensor([[ 0.7355, 0.2798, 0.9392], [ 1.0300, -0.6085, 1.7991], [ 2.8120, 1.2438, 1.2999], [-1.0534, 2.8053, 0.0163], [ 1.4088, 0.9000, -0.1172]]) 注意 任何使原位张量变形的操作都是用。后固定的。例如：x.copy(y)，x.t_()，将改变x。 你可以使用标准的NumPy索引与所有的铃声和​​口哨！ print(x[:, 1]) tensor([ 0.0149, -1.2480, 0.5582, 2.4951, 0.0001]) 调整大小：如果要调整大小/重塑张量，可以使用torch.view： x = torch.randn(4, 4) y = x.view(16) z = x.view(-1, 8) # the size -1 is inferred from other dimensions print(x.size(), y.size(), z.size()) torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) 如果你有一个元素张量，用于.item()获取值作为Python数字 x = torch.randn(1) print(x) print(x.item()) tensor([ 1.3159]) 1.3159412145614624 NumPy Bridge将Torch Tensor转换为NumPy阵列（反之亦然）是一件轻而易举的事。Torch Tensor和NumPy阵列将共享其底层内存位置，更改一个将改变另一个。 将Torch Tensor转换为NumPy数组 a = torch.ones(5) print(a) tensor([ 1., 1., 1., 1., 1.]) b = a.numpy() print(b) [1. 1. 1. 1. 1.] 了解numpy数组的值如何变化。 a.add_(1) print(a) print(b) tensor([ 2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.] 将NumPy数组转换为Torch Tensor 了解更改np阵列如何自动更改Torch Tensor import numpy as np a = np.ones(5) b = torch.from_numpy(a) np.add(a, 1, out=a) print(a) print(b) [2. 2. 2. 2. 2.] tensor([ 2., 2., 2., 2., 2.], dtype=torch.float64) 除了CharTensor之外，CPU上的所有Tensors都支持转换为NumPy并返回。 CUDA Tensors 可以使用该.to方法将张量移动到任何设备上。 # let us run this cell only if CUDA is available # We will use ``torch.device`` objects to move tensors in and out of GPU if torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) # a CUDA device object y = torch.ones_like(x, device=device) # directly create a tensor on GPU x = x.to(device) # or just use strings ``.to(&quot;cuda&quot;)`` z = x + y print(z) print(z.to(&quot;cpu&quot;, torch.double)) # ``.to`` can also change dtype together! tensor([ 2.3159], device=&apos;cuda:0&apos;) tensor([ 2.3159], dtype=torch.float64) torch torch.eye torch.eye(n, m=None, out=None)返回一个2维张量，对角线位置全1，其它位置全0 参数: n (int ) – 行数 m (int, optional) – 列数.如果为None,则默认为n out (Tensor, optinal) - Output tensor 返回值: 对角线位置全1，其它位置全0的2维张量 返回值类型: Tensor 例子: &gt;&gt;&gt; torch.eye(3) 1 0 0 0 1 0 0 0 1 [torch.FloatTensor of size 3x3] from_numpy torch.from_numpy(ndarray) → TensorNumpy桥，将numpy.ndarray 转换为pytorch的 Tensor。 返回的张量tensor和numpy的ndarray共享同一内存空间。修改一个会导致另外一个也被修改。返回的张量不能改变大小。 例子: &gt;&gt;&gt; a = numpy.array([1, 2, 3]) &gt;&gt;&gt; t = torch.from_numpy(a) &gt;&gt;&gt; t torch.LongTensor([1, 2, 3]) &gt;&gt;&gt; t[0] = -1 &gt;&gt;&gt; a array([-1, 2, 3]) torch.linspace torch.linspace(start, end, steps=100, out=None) → Tensor返回一个1维张量，包含在区间start 和 end 上均匀间隔的steps个点。 输出1维张量的长度为steps。 参数: start (float) – 序列的起始点 end (float) – 序列的最终值 steps (int) – 在start 和 end间生成的样本数 out (Tensor, optional) – 结果张量 例子: &gt;&gt;&gt; torch.linspace(3, 10, steps=5) 3.0000 4.7500 6.5000 8.2500 10.0000 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.linspace(-10, 10, steps=5) -10 -5 0 5 10 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5) -10 -5 0 5 10 [torch.FloatTensor of size 5] torch.logspace torch.logspace(start, end, steps=100, out=None) → Tensor返回一个1维张量，包含在区间 10start 和 10end上以对数刻度均匀间隔的steps个点。 输出1维张量的长度为steps。 参数: start (float) – 序列的起始点 end (float) – 序列的最终值 steps (int) – 在start 和 end间生成的样本数 out (Tensor, optional) – 结果张量 例子: &gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5) 1.0000e-10 1.0000e-05 1.0000e+00 1.0000e+05 1.0000e+10 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5) 1.2589 2.1135 3.5481 5.9566 10.0000 [torch.FloatTensor of size 5] torch.ones torch.ones(*sizes, out=None) → Tensor返回一个全为1 的张量，形状由可变参数sizes定义。 参数: sizes (int…) – 整数序列，定义了输出形状 out (Tensor, optional) – 结果张量 例子: &gt;&gt;&gt; torch.ones(2, 3) 1 1 1 1 1 1 [torch.FloatTensor of size 2x3] &gt;&gt;&gt; torch.ones(5) 1 1 1 1 1 [torch.FloatTensor of size 5] torch.rand torch.rand(*sizes, out=None) → Tensor返回一个张量，包含了从区间[0,1)的均匀分布中抽取的一组随机数，形状由可变参数sizes 定义。 参数: sizes (int…) – 整数序列，定义了输出形状 out (Tensor, optinal) - 结果张量 例子： &gt;&gt;&gt; torch.rand(4) 0.9193 0.3347 0.3232 0.7715 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.rand(2, 3) 0.5010 0.5140 0.0719 0.1435 0.5636 0.0538 [torch.FloatTensor of size 2x3] torch.randn torch.randn(*sizes, out=None) → Tensor返回一个张量，包含了从标准正态分布(均值为0，方差为 1，即高斯白噪声)中抽取一组随机数，形状由可变参数sizes定义。 参数: sizes (int…) – 整数序列，定义了输出形状 out (Tensor, optinal) - 结果张量 例子： &gt;&gt;&gt; torch.randn(4) -0.1145 0.0094 -1.1717 0.9846 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.randn(2, 3) 1.4339 0.3351 -1.0999 1.5458 -0.9643 -0.3558 [torch.FloatTensor of size 2x3] torch.randperm torch.randperm(n, out=None) → LongTensor给定参数n，返回一个从0 到n -1 的随机整数排列。 参数: n (int) – 上边界(不包含) 例子： &gt;&gt;&gt; torch.randperm(4) 2 1 3 0 [torch.LongTensor of size 4] torch.arange torch.arange(start, end, step=1, out=None) → Tensor返回一个1维张量，长度为 floor((end−start)/step)。包含从start到end，以step为步长的一组序列值(默认步长为1)。 参数: start (float) – 序列的起始点 end (float) – 序列的终止点 step (float) – 相邻点的间隔大小 out (Tensor, optional) – 结果张量例子： &gt;&gt;&gt; torch.arange(1, 4) 1 2 3 [torch.FloatTensor of size 3] &gt;&gt;&gt; torch.arange(1, 2.5, 0.5) 1.0000 1.5000 2.0000 [torch.FloatTensor of size 3] torch.range torch.range(start, end, step=1, out=None) → Tensor返回一个1维张量，有 floor((end−start)/step)+1 个元素。包含在半开区间[start, end）从start开始，以step为步长的一组值。 step 是两个值之间的间隔，即 xi+1=xi+step 警告：建议使用函数 torch.arange()参数: start (float) – 序列的起始点 end (float) – 序列的最终值 step (int) – 相邻点的间隔大小out (Tensor, optional) – 结果张量例子： &gt;&gt;&gt; torch.range(1, 4) 1 2 3 4 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.range(1, 4, 0.5) 1.0000 1.5000 2.0000 2.5000 3.0000 3.5000 4.0000 [torch.FloatTensor of size 7]&gt; torch.zerostorch.zeros(sizes, out=None) → Tensor返回一个全为标量 0 的张量，形状由可变参数sizes 定义。参数: sizes (int…) – 整数序列，定义了输出形状( out (Tensor, optional) – 结果张量例子： &gt;&gt;&gt; torch.zeros(2, 3) 0 0 0 0 0 0 [torch.FloatTensor of size 2x3] &gt;&gt;&gt; torch.zeros(5) 0 0 0 0 0 [torch.FloatTensor of size 5]#### 索引,切片,连接,换位Indexing, Slicing, Joining, Mutating Ops—&gt; torch.cattorch.cat(inputs, dimension=0) → Tensor在给定维度上对输入的张量序列seq 进行连接操作。torch.cat()可以看做 torch.split() 和 torch.chunk()的反操作。 cat() 函数可以通过下面例子更好的理解。参数: inputs (sequence of Tensors) – 可以是任意相同Tensor 类型的python 序列 dimension (int, optional) – 沿着此维连接张量序列。例子： &gt;&gt;&gt; x = torch.randn(2, 3) &gt;&gt;&gt; x 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 2x3] &gt;&gt;&gt; torch.cat((x, x, x), 0) 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 6x3] &gt;&gt;&gt; torch.cat((x, x, x), 1) 0.5983 -0.0341 2.4918 0.5983 -0.0341 2.4918 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 1.5981 -0.5265 -0.8735 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 2x9]&gt;torch.chunktorch.chunk(tensor, chunks, dim=0)在给定维度(轴)上将输入张量进行分块儿。参数: tensor (Tensor) – 待分块的输入张量 chunks (int) – 分块的个数 dim (int) – 沿着此维度进行分块&gt; torch.gathertorch.gather(input, dim, index, out=None) → Tensor沿给定轴dim，将输入索引张量index指定位置的值进行聚合。对一个3维张量，输出可以定义为： out[i][j][k] = tensor[index[i][j][k]][j][k] # dim=0 out[i][j][k] = tensor[i][index[i][j][k]][k] # dim=1 out[i][j][k] = tensor[i][j][index[i][j][k]] # dim=3例子： &gt;&gt;&gt; t = torch.Tensor([[1,2],[3,4]]) &gt;&gt;&gt; torch.gather(t, 1, torch.LongTensor([[0,0],[1,0]])) 1 1 4 3 [torch.FloatTensor of size 2x2]参数: input (Tensor) – 源张量 dim (int) – 索引的轴 index (LongTensor) – 聚合元素的下标 out (Tensor, optional) – 目标张量&gt; torch.index_selecttorch.index_select(input, dim, index, out=None) → Tensor沿着指定维度对输入进行切片，取index中指定的相应项(index为一个LongTensor)，然后返回到一个新的张量， 返回的张量与原始张量Tensor有相同的维度(在指定轴上)。注意： 返回的张量不与原始张量共享内存空间。 参数: input (Tensor) – 输入张量 dim (int) – 索引的轴 index (LongTensor) – 包含索引下标的一维张量 out (Tensor, optional) – 目标张量 例子： &gt;&gt;&gt; x = torch.randn(3, 4) &gt;&gt;&gt; x 1.2045 2.4084 0.4001 1.1372 0.5596 1.5677 0.6219 -0.7954 1.3635 -1.2313 -0.5414 -1.8478 [torch.FloatTensor of size 3x4] &gt;&gt;&gt; indices = torch.LongTensor([0, 2]) &gt;&gt;&gt; torch.index_select(x, 0, indices) 1.2045 2.4084 0.4001 1.1372 1.3635 -1.2313 -0.5414 -1.8478 [torch.FloatTensor of size 2x4] &gt;&gt;&gt; torch.index_select(x, 1, indices) 1.2045 0.4001 0.5596 0.6219 1.3635 -0.5414 [torch.FloatTensor of size 3x2] torch.masked_select torch.masked_select(input, mask, out=None) → Tensor根据掩码张量mask中的二元值，取输入张量中的指定项( mask为一个 ByteTensor)，将取值返回到一个新的1D张量， 张量 mask须跟input张量有相同数量的元素数目，但形状或维度不需要相同。 注意： 返回的张量不与原始张量共享内存空间。 参数: input (Tensor) – 输入张量 mask (ByteTensor) – 掩码张量，包含了二元索引值 out (Tensor, optional) – 目标张量 例子： &gt;&gt;&gt; x = torch.randn(3, 4) &gt;&gt;&gt; x 1.2045 2.4084 0.4001 1.1372 0.5596 1.5677 0.6219 -0.7954 1.3635 -1.2313 -0.5414 -1.8478 [torch.FloatTensor of size 3x4] &gt;&gt;&gt; indices = torch.LongTensor([0, 2]) &gt;&gt;&gt; torch.index_select(x, 0, indices) 1.2045 2.4084 0.4001 1.1372 1.3635 -1.2313 -0.5414 -1.8478 [torch.FloatTensor of size 2x4] &gt;&gt;&gt; torch.index_select(x, 1, indices) 1.2045 0.4001 0.5596 0.6219 1.3635 -0.5414 [torch.FloatTensor of size 3x2] torch.nonzero torch.nonzero(input, out=None) → LongTensor返回一个包含输入input中非零元素索引的张量。输出张量中的每行包含输入中非零元素的索引。 如果输入input有n维，则输出的索引张量output的形状为 z x n, 这里 z 是输入张量input中所有非零元素的个数。 参数: input (Tensor) – 源张量 out (LongTensor, optional) – 包含索引值的结果张量 例子： &gt;&gt;&gt; torch.nonzero(torch.Tensor([1, 1, 1, 0, 1])) 0 1 2 4 [torch.LongTensor of size 4x1] &gt;&gt;&gt; torch.nonzero(torch.Tensor([[0.6, 0.0, 0.0, 0.0], ... [0.0, 0.4, 0.0, 0.0], ... [0.0, 0.0, 1.2, 0.0], ... [0.0, 0.0, 0.0,-0.4]])) 0 0 1 1 2 2 3 3 [torch.LongTensor of size 4x2] torch.split torch.split(tensor, split_size, dim=0)将输入张量分割成相等形状的chunks（如果可分）。 如果沿指定维的张量形状大小不能被split_size 整分， 则最后一个分块会小于其它分块。 参数: tensor (Tensor) – 待分割张量 split_size (int) – 单个分块的形状大小 dim (int) – 沿着此维进行分割 torch.squeeze torch.squeeze(input, dim=None, out=None)将输入张量形状中的1 去除并返回。 如果输入是形如(A×1×B×1×C×1×D)，那么输出形状就为： (A×B×C×D)当给定dim时，那么挤压操作只在给定维度上。例如，输入形状为: (A×1×B), squeeze(input, 0) 将会保持张量不变，只有用 squeeze(input, 1)，形状会变成 (A×B)。 注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。 参数: input (Tensor) – 输入张量 dim (int, optional) – 如果给定，则input只会在给定维度挤压 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; x = torch.zeros(2,1,2,1,2) &gt;&gt;&gt; x.size() (2L, 1L, 2L, 1L, 2L) &gt;&gt;&gt; y = torch.squeeze(x) &gt;&gt;&gt; y.size() (2L, 2L, 2L) &gt;&gt;&gt; y = torch.squeeze(x, 0) &gt;&gt;&gt; y.size() (2L, 1L, 2L, 1L, 2L) &gt;&gt;&gt; y = torch.squeeze(x, 1) &gt;&gt;&gt; y.size() (2L, 2L, 1L, 2L) torch.stack[source] torch.stack(sequence, dim=0)沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。 参数: sqequence (Sequence) – 待连接的张量序列 dim (int) – 插入的维度。必须介于 0 与 待连接的张量序列数之间。 torch.t torch.t(input, out=None) → Tensor输入一个矩阵（2维张量），并转置0, 1维。 可以被视为函数transpose(input, 0, 1)的简写函数。 参数: input (Tensor) – 输入张量 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; x = torch.randn(2, 3) &gt;&gt;&gt; x 0.4834 0.6907 1.3417 -0.1300 0.5295 0.2321 [torch.FloatTensor of size 2x3] &gt;&gt;&gt; torch.t(x) 0.4834 -0.1300 0.6907 0.5295 1.3417 0.2321 [torch.FloatTensor of size 3x2] torch.transpose torch.transpose(input, dim0, dim1, out=None) → Tensor返回输入矩阵input的转置。交换维度dim0和dim1。 输出张量与输入张量共享内存，所以改变其中一个会导致另外一个也被修改。 参数: input (Tensor) – 输入张量 dim0 (int) – 转置的第一维 dim1 (int) – 转置的第二维 &gt;&gt;&gt; x = torch.randn(2, 3) &gt;&gt;&gt; x 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 2x3] &gt;&gt;&gt; torch.transpose(x, 0, 1) 0.5983 1.5981 -0.0341 -0.5265 2.4918 -0.8735 [torch.FloatTensor of size 3x2] torch.unbind torch.unbind(tensor, dim=0)[source]移除指定维后，返回一个元组，包含了沿着指定维切片后的各个切片 参数: tensor (Tensor) – 输入张量 dim (int) – 删除的维度 torch.unsqueeze torch.unsqueeze(input, dim, out=None)返回一个新的张量，对输入的制定位置插入维度 1 注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。 如果dim为负，则将会被转化dim+input.dim()+1 参数: tensor (Tensor) – 输入张量 dim (int) – 插入维度的索引 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; x = torch.Tensor([1, 2, 3, 4]) &gt;&gt;&gt; torch.unsqueeze(x, 0) 1 2 3 4 [torch.FloatTensor of size 1x4] &gt;&gt;&gt; torch.unsqueeze(x, 1) 1 2 3 4 [torch.FloatTensor of size 4x1] 随机抽样 Random sampling torch.manual_seed torch.manual_seed(seed)设定生成随机数的种子，并返回一个 torch._C.Generator 对象. 参数: seed (int or long) – 种子. torch.initial_seed torch.initial_seed()返回生成随机数的原始种子值（python long）。 torch.get_rng_state torch.get_rng_state()[source]返回随机生成器状态(ByteTensor) torch.set_rng_state torch.set_rng_state(new_state)[source]设定随机生成器状态 参数: new_state (torch.ByteTensor) – 期望的状态 torch.default_generator torch.default_generator = &lt;torch._C.Generator object&gt; torch.bernoulli torch.bernoulli(input, out=None) → Tensor从伯努利分布中抽取二元随机数(0 或者 1)。 输入张量须包含用于抽取上述二元随机值的概率。 因此，输入中的所有值都必须在［0,1］区间，即 0&lt;=inputi&lt;=1输出张量的第i个元素值， 将会以输入张量的第i个概率值等于1。 返回值将会是与输入相同大小的张量，每个值为0或者1 参数: input (Tensor) – 输入为伯努利分布的概率值 out (Tensor, optional) – 输出张量(可选) 例子： &gt;&gt;&gt; a = torch.Tensor(3, 3).uniform_(0, 1) # generate a uniform random matrix with range [0, 1] &gt;&gt;&gt; a 0.7544 0.8140 0.9842 0.5282 0.0595 0.6445 0.1925 0.9553 0.9732 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.bernoulli(a) 1 1 1 0 0 1 0 1 1 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing &quot;1&quot; is 1 &gt;&gt;&gt; torch.bernoulli(a) 1 1 1 1 1 1 1 1 1 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing &quot;1&quot; is 0 &gt;&gt;&gt; torch.bernoulli(a) 0 0 0 0 0 0 0 0 0 [torch.FloatTensor of size 3x3] torch.multinomial torch.multinomial(input, num_samples,replacement=False, out=None) → LongTensor返回一个张量，每行包含从input相应行中定义的多项分布中抽取的num_samples个样本。 [注意]:输入input每行的值不需要总和为1 (这里我们用来做权重)，但是必须非负且总和不能为0。 当抽取样本时，依次从左到右排列(第一个样本对应第一列)。 如果输入input是一个向量，输出out也是一个相同长度num_samples的向量。如果输入input是有 m行的矩阵，输出out是形如m×n的矩阵。 如果参数replacement 为 True, 则样本抽取可以重复。否则，一个样本在每行不能被重复抽取。 参数num_samples必须小于input长度(即，input的列数，如果是input是一个矩阵)。 参数: input (Tensor) – 包含概率值的张量 num_samples (int) – 抽取的样本数 replacement (bool, optional) – 布尔值，决定是否能重复抽取 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; weights = torch.Tensor([0, 10, 3, 0]) # create a Tensor of weights &gt;&gt;&gt; torch.multinomial(weights, 4) 1 2 0 0 [torch.LongTensor of size 4] &gt;&gt;&gt; torch.multinomial(weights, 4, replacement=True) 1 2 1 2 [torch.LongTensor of size 4] torch.normal() torch.normal(means, std, out=None)返回一个张量，包含从给定参数means,std的离散正态分布中抽取随机数。 均值means是一个张量，包含每个输出元素相关的正态分布的均值。 std是一个张量，包含每个输出元素相关的正态分布的标准差。 均值和标准差的形状不须匹配，但每个张量的元素个数须相同。 参数: means (Tensor) – 均值 std (Tensor) – 标准差 out (Tensor) – 可选的输出张量 例子： &gt;&gt;&gt; torch.normal(means=torch.arange(1, 11), std=torch.arange(1, 0, -0.1)) 1.5104 1.6955 2.4895 4.9185 4.9895 6.9155 7.3683 8.1836 8.7164 9.8916 [torch.FloatTensor of size 10] torch.normal(mean=0.0, std, out=None) 与上面函数类似，所有抽取的样本共享均值。 参数: means (Tensor,optional) – 所有分布均值 std (Tensor) – 每个元素的标准差 out (Tensor) – 可选的输出张量 例子: &gt;&gt;&gt; torch.normal(mean=0.5, std=torch.arange(1, 6)) 0.5723 0.0871 -0.3783 -2.5689 10.7893 [torch.FloatTensor of size 5] torch.normal(means, std=1.0, out=None)与上面函数类似，所有抽取的样本共享标准差。 参数: means (Tensor) – 每个元素的均值 std (float, optional) – 所有分布的标准差 out (Tensor) – 可选的输出张量 例子: &gt;&gt;&gt; torch.normal(means=torch.arange(1, 6)) 1.1681 2.8884 3.7718 2.5616 4.2500 [torch.FloatTensor of size 5] 序列化 Serialization torch.saves[source] torch.save(obj, f, pickle_module=, pickle_protocol=2)保存一个对象到一个硬盘文件上 参考: Recommended approach for saving a model 参数： obj – 保存对象 f － 类文件对象 (返回文件描述符)或一个保存文件名的字符串 pickle_module – 用于pickling元数据和对象的模块 pickle_protocol – 指定pickle protocal 可以覆盖默认参数 torch.load[source] torch.load(f, map_location=None, pickle_module=)从磁盘文件中读取一个通过torch.save()保存的对象。torch.load() 可通过参数map_location 动态地进行内存重映射，使其能从不动设备中读取文件。一般调用时，需两个参数: storage 和 location tag. 返回不同地址中的storage，或着返回None (此时地址可以通过默认方法进行解析). 如果这个参数是字典的话，意味着其是从文件的地址标记到当前系统的地址标记的映射。 默认情况下， location tags中 “cpu”对应host tensors，‘cuda:device_id’ (e.g. ‘cuda:2’) 对应cuda tensors。 用户可以通过register_package进行扩展，使用自己定义的标记和反序列化方法。 参数: f – 类文件对象 (返回文件描述符)或一个保存文件名的字符串 map_location – 一个函数或字典规定如何remap存储位置 pickle_module – 用于unpickling元数据和对象的模块 (必须匹配序列化文件时的pickle_module ) 例子: &gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;) # Load all tensors onto the CPU &gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location=lambda storage, loc: storage) # Map tensors from GPU 1 to GPU 0 &gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location={&apos;cuda:1&apos;:&apos;cuda:0&apos;}) 并行化 Parallelism torch.get_num_threads torch.get_num_threads() → int获得用于并行化CPU操作的OpenMP线程数 torch.set_num_threads torch.set_num_threads(int)设定用于并行化CPU操作的OpenMP线程数 数学操作Math operations Pointwise Ops torch.abs torch.abs(input, out=None) → Tensor计算输入张量的每个元素绝对值 例子： &gt;&gt;&gt; torch.abs(torch.FloatTensor([-1, -2, 3])) FloatTensor([1, 2, 3]) torch.acos(input, out=None) → Tensortorch.acos(input, out=None) → Tensor 返回一个新张量，包含输入张量每个元素的反余弦。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.acos(a) 2.2608 1.2956 1.1075 nan [torch.FloatTensor of size 4] torch.add() torch.add(input, value, out=None)对输入张量input逐元素加上标量值value，并返回结果到一个新的张量out，即 out=tensor+value。 如果输入input是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】 input (Tensor) – 输入张量 value (Number) – 添加到输入每个元素的数 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 0.4050 -1.2227 1.8688 -0.4185 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.add(a, 20) 20.4050 18.7773 21.8688 19.5815 [torch.FloatTensor of size 4] torch.add(input, value=1, other, out=None) other 张量的每个元素乘以一个标量值value，并加到iput 张量上。返回结果到输出张量out。即，out=input+(other∗value) 两个张量 input and other的尺寸不需要匹配，但元素总数必须一样。 注意 :当两个张量形状不匹配时，输入张量的形状会作为输出张量的尺寸。 如果other是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】 参数: input (Tensor) – 第一个输入张量 value (Number) – 用于第二个张量的尺寸因子 other (Tensor) – 第二个输入张量 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; import torch &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.9310 2.0330 0.0852 -0.2941 [torch.FloatTensor of size 4] &gt;&gt;&gt; b = torch.randn(2, 2) &gt;&gt;&gt; b 1.0663 0.2544 -0.1513 0.0749 [torch.FloatTensor of size 2x2] &gt;&gt;&gt; torch.add(a, 10, b) 9.7322 4.5770 -1.4279 0.4552 [torch.FloatTensor of size 4] torch.addcdiv torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None) → Tensor用tensor2对tensor1逐元素相除，然后乘以标量值value 并加到tensor。 张量的形状不需要匹配，但元素数量必须一致。 如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。 参数： tensor (Tensor) – 张量，对 tensor1 ./ tensor 进行相加 value (Number, optional) – 标量，对 tensor1 ./ tensor2 进行相乘 tensor1 (Tensor) – 张量，作为被除数(分子) tensor2 (Tensor) –张量，作为除数(分母) out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; t = torch.randn(2, 3) &gt;&gt;&gt; t1 = torch.randn(1, 6) &gt;&gt;&gt; t2 = torch.randn(6, 1) &gt;&gt;&gt; torch.addcdiv(t, 0.1, t1, t2) 0.0122 -0.0188 -0.2354 0.7396 -1.5721 1.2878 [torch.FloatTensor of size 2x3] torch.addcmul torch.addcmul(tensor, value=1, tensor1, tensor2, out=None) → Tensor用tensor2对tensor1逐元素相乘，并对结果乘以标量值value然后加到tensor。 张量的形状不需要匹配，但元素数量必须一致。 如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。 参数： tensor (Tensor) – 张量，对tensor1 ./ tensor 进行相加 value (Number, optional) – 标量，对 tensor1 . tensor2 进行相乘 tensor1 (Tensor) – 张量，作为乘子1 tensor2 (Tensor) –张量，作为乘子2 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; t = torch.randn(2, 3) &gt;&gt;&gt; t1 = torch.randn(1, 6) &gt;&gt;&gt; t2 = torch.randn(6, 1) &gt;&gt;&gt; torch.addcmul(t, 0.1, t1, t2) 0.0122 -0.0188 -0.2354 0.7396 -1.5721 1.2878 [torch.FloatTensor of size 2x3] torch.asin torch.asin(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的反正弦函数 参数： tensor (Tensor) – 输入张量 nout (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.asin(a) -0.6900 0.2752 0.4633 nan [torch.FloatTensor of size 4] torch.atan torch.atan(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的反正切函数 参数： tensor (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.atan(a) -0.5669 0.2653 0.4203 0.9196 [torch.FloatTensor of size 4] torch.atan2 torch.atan2(input1, input2, out=None) → Tensor返回一个新张量，包含两个输入张量input1和input2的反正切函数 参数： input1 (Tensor) – 第一个输入张量 input2 (Tensor) – 第二个输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.atan2(a, torch.randn(4)) -2.4167 2.9755 0.9363 1.6613 [torch.FloatTensor of size 4] torch.ceil torch.ceil(input, out=None) → Tensor天井函数，对输入input张量每个元素向上取整, 即取不小于每个元素的最小整数，并返回结果到输出。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.ceil(a) 2 1 -0 -0 [torch.FloatTensor of size 4] torch.clamp torch.clamp(input, min, max, out=None) → Tensor将输入input张量每个元素的夹紧到区间 $[min,max]$，并返回结果到一个新张量。 操作定义如下： | min, if x_i &lt; min y_i = | x_i, if min &lt;= x_i &lt;= max | max, if x_i &gt; max 如果输入是FloatTensor or DoubleTensor类型，则参数min max 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，min， max取整数、实数皆可。】 参数： input (Tensor) – 输入张量 min (Number) – 限制范围下限 max (Number) – 限制范围上限 Nout (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.clamp(a, min=-0.5, max=0.5) 0.5000 0.3912 -0.5000 -0.5000 [torch.FloatTensor of size 4] torch.clamp(input, *, min, out=None) → Tensor 将输入input张量每个元素的限制到不小于min ，并返回结果到一个新张量。 如果输入是FloatTensor or DoubleTensor类型，则参数 min 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，min取整数、实数皆可。】 参数： input (Tensor) – 输入张量 value (Number) – 限制范围下限 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.clamp(a, min=0.5) 1.3869 0.5000 0.5000 0.5000 [torch.FloatTensor of size 4] torch.clamp(input, *, max, out=None) → Tensor 将输入input张量每个元素的限制到不大于max ，并返回结果到一个新张量。 如果输入是FloatTensor or DoubleTensor类型，则参数 max 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，max取整数、实数皆可。】 参数： input (Tensor) – 输入张量 value (Number) – 限制范围上限 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.clamp(a, max=0.5) 0.5000 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] torch.cos torch.cos(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的余弦。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.cos(a) 0.8041 0.9633 0.9018 0.2557 [torch.FloatTensor of size 4] torch.cosh torch.cosh(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的双曲余弦。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.cosh(a) 1.2095 1.0372 1.1015 1.9917 [torch.FloatTensor of size 4] torch.div() torch.div(input, value, out=None)将input逐元素除以标量值value，并返回结果到输出张量out。 即 out=tensor/value如果输入是FloatTensor or DoubleTensor类型，则参数 value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】 参数： input (Tensor) – 输入张量 value (Number) – 除数 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(5) &gt;&gt;&gt; a -0.6147 -1.1237 -0.1604 -0.6853 0.1063 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.div(a, 0.5) -1.2294 -2.2474 -0.3208 -1.3706 0.2126 [torch.FloatTensor of size 5] torch.div(input, other, out=None) 两张量input和other逐元素相除，并将结果返回到输出。即， outi=inputi/otheri两张量形状不须匹配，但元素数须一致。 注意：当形状不匹配时，input的形状作为输出张量的形状。 参数： input (Tensor) – 张量(分子) other (Tensor) – 张量(分母) out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4,4) &gt;&gt;&gt; a -0.1810 0.4017 0.2863 -0.1013 0.6183 2.0696 0.9012 -1.5933 0.5679 0.4743 -0.0117 -0.1266 -0.1213 0.9629 0.2682 1.5968 [torch.FloatTensor of size 4x4] &gt;&gt;&gt; b = torch.randn(8, 2) &gt;&gt;&gt; b 0.8774 0.7650 0.8866 1.4805 -0.6490 1.1172 1.4259 -0.8146 1.4633 -0.1228 0.4643 -0.6029 0.3492 1.5270 1.6103 -0.6291 [torch.FloatTensor of size 8x2] &gt;&gt;&gt; torch.div(a, b) -0.2062 0.5251 0.3229 -0.0684 -0.9528 1.8525 0.6320 1.9559 0.3881 -3.8625 -0.0253 0.2099 -0.3473 0.6306 0.1666 -2.5381 [torch.FloatTensor of size 4x4] torch.exp torch.exp(tensor, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的指数。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 &gt;&gt;&gt; torch.exp(torch.Tensor([0, math.log(2)])) torch.FloatTensor([1, 2]) torch.floor torch.floor(input, out=None) → Tensor床函数: 返回一个新张量，包含输入input张量每个元素的floor，即不小于元素的最大整数。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.floor(a) 1 0 -1 -1 [torch.FloatTensor of size 4] torch.fmod torch.fmod(input, divisor, out=None) → Tensor计算除法余数。 除数与被除数可能同时含有整数和浮点数。此时，余数的正负与被除数相同。 参数： input (Tensor) – 被除数 divisor (Tensor or float) – 除数，一个数或与被除数相同类型的张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; torch.fmod(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2) torch.FloatTensor([-1, -0, -1, 1, 0, 1]) &gt;&gt;&gt; torch.fmod(torch.Tensor([1, 2, 3, 4, 5]), 1.5) torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5]) 参考: torch.remainder(), 计算逐元素余数， 相当于python 中的 % 操作符。 torch.frac torch.frac(tensor, out=None) → Tensor返回每个元素的分数部分。 例子： &gt;&gt;&gt; torch.frac(torch.Tensor([1, 2.5, -3.2]) torch.FloatTensor([0, 0.5, -0.2]) torch.lerp torch.lerp(start, end, weight, out=None)对两个张量以start，end做线性插值， 将结果返回到输出张量。即，outi=starti+weight∗(endi−starti) 参数： start (Tensor) – 起始点张量 end (Tensor) – 终止点张量 weight (float) – 插值公式的weight out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; start = torch.arange(1, 5) &gt;&gt;&gt; end = torch.Tensor(4).fill_(10) &gt;&gt;&gt; start 1 2 3 4 [torch.FloatTensor of size 4] &gt;&gt;&gt; end 10 10 10 10 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.lerp(start, end, 0.5) 5.5000 6.0000 6.5000 7.0000 [torch.FloatTensor of size 4] torch.log torch.log(input, out=None) → Tensor计算input 的自然对数 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(5) &gt;&gt;&gt; a -0.4183 0.3722 -0.3091 0.4149 0.5857 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.log(a) nan -0.9883 nan -0.8797 -0.5349 [torch.FloatTensor of size 5] torch.log1p torch.log1p(input, out=None) → Tensor计算 input+1的自然对数 yi=log(xi+1) 注意：对值比较小的输入，此函数比torch.log()更准确。 如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(5) &gt;&gt;&gt; a -0.4183 0.3722 -0.3091 0.4149 0.5857 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.log1p(a) -0.5418 0.3164 -0.3697 0.3471 0.4611 [torch.FloatTensor of size 5] torch.mul torch.mul(input, value, out=None)用标量值value乘以输入input的每个元素，并返回一个新的结果张量。 out=tensor∗value如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】 参数： input (Tensor) – 输入张量 value (Number) – 乘到每个元素的数 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(3) &gt;&gt;&gt; a -0.9374 -0.5254 -0.6069 [torch.FloatTensor of size 3] &gt;&gt;&gt; torch.mul(a, 100) -93.7411 -52.5374 -60.6908 [torch.FloatTensor of size 3] torch.mul(input, other, out=None) 两个张量input,other按元素进行相乘，并返回到输出张量。即计算outi=inputi∗otheri两计算张量形状不须匹配，但总元素数须一致。 注意：当形状不匹配时，input的形状作为输入张量的形状。 参数： input (Tensor) – 第一个相乘张量 other (Tensor) – 第二个相乘张量 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4,4) &gt;&gt;&gt; a -0.7280 0.0598 -1.4327 -0.5825 -0.1427 -0.0690 0.0821 -0.3270 -0.9241 0.5110 0.4070 -1.1188 -0.8308 0.7426 -0.6240 -1.1582 [torch.FloatTensor of size 4x4] &gt;&gt;&gt; b = torch.randn(2, 8) &gt;&gt;&gt; b 0.0430 -1.0775 0.6015 1.1647 -0.6549 0.0308 -0.1670 1.0742 -1.2593 0.0292 -0.0849 0.4530 1.2404 -0.4659 -0.1840 0.5974 [torch.FloatTensor of size 2x8] &gt;&gt;&gt; torch.mul(a, b) -0.0313 -0.0645 -0.8618 -0.6784 0.0934 -0.0021 -0.0137 -0.3513 1.1638 0.0149 -0.0346 -0.5068 -1.0304 -0.3460 0.1148 -0.6919 [torch.FloatTensor of size 4x4] torch.neg torch.neg(input, out=None) → Tensor返回一个新张量，包含输入input 张量按元素取负。 即， out=−1∗input 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(5) &gt;&gt;&gt; a -0.4430 1.1690 -0.8836 -0.4565 0.2968 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.neg(a) 0.4430 -1.1690 0.8836 0.4565 -0.2968 [torch.FloatTensor of size 5] torch.pow torch.pow(input, exponent, out=None)对输入input的按元素求exponent次幂值，并返回结果张量。 幂值exponent 可以为单一 float 数或者与input相同元素数的张量。 当幂值为标量时，执行操作：outi=xexponent 当幂值为张量时，执行操作：outi=xexponenti 参数： input (Tensor) – 输入张量 exponent (float or Tensor) – 幂值 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.5274 -0.8232 -2.1128 1.7558 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.pow(a, 2) 0.2781 0.6776 4.4640 3.0829 [torch.FloatTensor of size 4] &gt;&gt;&gt; exp = torch.arange(1, 5) &gt;&gt;&gt; a = torch.arange(1, 5) &gt;&gt;&gt; a 1 2 3 4 [torch.FloatTensor of size 4] &gt;&gt;&gt; exp 1 2 3 4 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.pow(a, exp) 1 4 27 256 [torch.FloatTensor of size 4] torch.pow(base, input, out=None) base 为标量浮点值,input为张量， 返回的输出张量 out 与输入张量相同形状。 执行操作为:outi=baseinputi 参数： base (float) – 标量值，指数的底 input ( Tensor) – 幂值 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; exp = torch.arange(1, 5) &gt;&gt;&gt; base = 2 &gt;&gt;&gt; torch.pow(base, exp) 2 4 8 16 [torch.FloatTensor of size 4] torch.reciprocal torch.reciprocal(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的倒数，即 1.0/x。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.reciprocal(a) 0.7210 2.5565 -1.1583 -1.8289 [torch.FloatTensor of size 4] torch.remainder torch.remainder(input, divisor, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的除法余数。 除数与被除数可能同时包含整数或浮点数。余数与除数有相同的符号。 参数： input (Tensor) – 被除数 divisor (Tensor or float) – 除数，一个数或者与除数相同大小的张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; torch.remainder(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2) torch.FloatTensor([1, 0, 1, 1, 0, 1]) &gt;&gt;&gt; torch.remainder(torch.Tensor([1, 2, 3, 4, 5]), 1.5) torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5]) 参考: 函数torch.fmod() 同样可以计算除法余数，相当于 C 的 库函数fmod() torch.round torch.round(input, out=None) → Tensor返回一个新张量，将输入input张量每个元素舍入到最近的整数。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.2290 1.3409 -0.5662 -0.0899 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.round(a) 1 1 -1 -0 [torch.FloatTensor of size 4] torch.rsqrt torch.rsqrt(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的平方根倒数。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.2290 1.3409 -0.5662 -0.0899 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.rsqrt(a) 0.9020 0.8636 nan nan [torch.FloatTensor of size 4] torch.sigmoid torch.sigmoid(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的sigmoid值。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.4972 1.3512 0.1056 -0.2650 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.sigmoid(a) 0.3782 0.7943 0.5264 0.4341 [torch.FloatTensor of size 4] torch.sign torch.sign(input, out=None) → Tensor符号函数：返回一个新张量，包含输入input张量每个元素的正负。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.sign(a) -1 1 1 1 [torch.FloatTensor of size 4] torch.sin torch.sin(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的正弦。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.sin(a) -0.5944 0.2684 0.4322 0.9667 [torch.FloatTensor of size 4] torch.sinh torch.sinh(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的双曲正弦。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.sinh(a) -0.6804 0.2751 0.4619 1.7225 [torch.FloatTensor of size 4] torch.sqrt torch.sqrt(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的平方根。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.2290 1.3409 -0.5662 -0.0899 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.sqrt(a) 1.1086 1.1580 nan nan [torch.FloatTensor of size 4] torch.tan torch.tan(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的正切。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.tan(a) -0.7392 0.2786 0.4792 3.7801 [torch.FloatTensor of size 4] torch.tanh torch.tanh(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的双曲正切。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.tanh(a) -0.5625 0.2653 0.4193 0.8648 [torch.FloatTensor of size 4] torch.trunc torch.trunc(input, out=None) → Tensor返回一个新张量，包含输入input张量每个元素的截断值(标量x的截断值是最接近其的整数，其比x更接近零。简而言之，有符号数的小数部分被舍弃)。 参数： input (Tensor) – 输入张量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a -0.4972 1.3512 0.1056 -0.2650 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.trunc(a) -0 1 0 -0 [torch.FloatTensor of size 4] Reduction Ops torch.cumprod torch.cumprod(input, dim, out=None) → Tensor返回输入沿指定维度的累积积。例如，如果输入是一个N 元向量，则结果也是一个N 元向量，第i 个输出元素值为$ yi=x1∗x2∗x3∗…∗xi $ 参数： input (Tensor) – 输入张量 dim (int) – 累积积操作的维度 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(10) &gt;&gt;&gt; a 1.1148 1.8423 1.4143 -0.4403 1.2859 -1.2514 -0.4748 1.1735 -1.6332 -0.4272 [torch.FloatTensor of size 10] &gt;&gt;&gt; torch.cumprod(a, dim=0) 1.1148 2.0537 2.9045 -1.2788 -1.6444 2.0578 -0.9770 -1.1466 1.8726 -0.8000 [torch.FloatTensor of size 10] &gt;&gt;&gt; a[5] = 0.0 &gt;&gt;&gt; torch.cumprod(a, dim=0) 1.1148 2.0537 2.9045 -1.2788 -1.6444 -0.0000 0.0000 0.0000 -0.0000 0.0000 [torch.FloatTensor of size 10] torch.cumsum torch.cumsum(input, dim, out=None) → Tensor返回输入沿指定维度的累积和。例如，如果输入是一个N元向量，则结果也是一个N元向量，第i 个输出元素值为 yi=x1+x2+x3+…+xi 参数： input (Tensor) – 输入张量 dim (int) – 累积和操作的维度 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(10) &gt;&gt;&gt; a -0.6039 -0.2214 -0.3705 -0.0169 1.3415 -0.1230 0.9719 0.6081 -0.1286 1.0947 [torch.FloatTensor of size 10] &gt;&gt;&gt; torch.cumsum(a, dim=0) -0.6039 -0.8253 -1.1958 -1.2127 0.1288 0.0058 0.9777 1.5858 1.4572 2.5519 [torch.FloatTensor of size 10] torch.dist torch.dist(input, other, p=2, out=None) → Tensor返回 (input - other) 的 p范数 。 参数： input (Tensor) – 输入张量 other (Tensor) – 右侧输入张量 p (float, optional) – 所计算的范数 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; x = torch.randn(4) &gt;&gt;&gt; x 0.2505 -0.4571 -0.3733 0.7807 [torch.FloatTensor of size 4] &gt;&gt;&gt; y = torch.randn(4) &gt;&gt;&gt; y 0.7782 -0.5185 1.4106 -2.4063 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.dist(x, y, 3.5) 3.302832063224223 &gt;&gt;&gt; torch.dist(x, y, 3) 3.3677282206393286 &gt;&gt;&gt; torch.dist(x, y, 0) inf &gt;&gt;&gt; torch.dist(x, y, 1) 5.560028076171875 torch.mean torch.mean(input) → float返回输入张量所有元素的均值。 参数： input (Tensor) – 输入张量 例子： &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a -0.2946 -0.9143 2.1809 [torch.FloatTensor of size 1x3] &gt;&gt;&gt; torch.mean(a) 0.32398951053619385 torch.mean(input, dim, out=None) → Tensor 返回输入张量给定维度dim上每行的均值。 输出形状与输入相同，除了给定维度上为1. 参数： input (Tensor) – 输入张量 dim (int) – the dimension to reduce out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a -1.2738 -0.3058 0.1230 -1.9615 0.8771 -0.5430 -0.9233 0.9879 1.4107 0.0317 -0.6823 0.2255 -1.3854 0.4953 -0.2160 0.2435 [torch.FloatTensor of size 4x4] &gt;&gt;&gt; torch.mean(a, 1) -0.8545 0.0997 0.2464 -0.2157 [torch.FloatTensor of size 4x1] torch.median torch.median(input, dim=-1, values=None, indices=None) -&gt; (Tensor, LongTensor)返回输入张量给定维度每行的中位数，同时返回一个包含中位数的索引的LongTensor。 dim值默认为输入张量的最后一维。 输出形状与输入相同，除了给定维度上为1. 注意: 这个函数还没有在torch.cuda.Tensor中定义 参数： input (Tensor) – 输入张量 dim (int) – 缩减的维度 values (Tensor, optional) – 结果张量 indices (Tensor, optional) – 返回的索引结果张量 例子： &gt;&gt;&gt; a -0.6891 -0.6662 0.2697 0.7412 0.5254 -0.7402 0.5528 -0.2399 [torch.FloatTensor of size 4x2] &gt;&gt;&gt; a = torch.randn(4, 5) &gt;&gt;&gt; a 0.4056 -0.3372 1.0973 -2.4884 0.4334 2.1336 0.3841 0.1404 -0.1821 -0.7646 -0.2403 1.3975 -2.0068 0.1298 0.0212 -1.5371 -0.7257 -0.4871 -0.2359 -1.1724 [torch.FloatTensor of size 4x5] &gt;&gt;&gt; torch.median(a, 1) ( 0.4056 0.1404 0.0212 -0.7257 [torch.FloatTensor of size 4x1] , 0 2 4 1 [torch.LongTensor of size 4x1] ) torch.mode torch.mode(input, dim=-1, values=None, indices=None) -&gt; (Tensor, LongTensor)返回给定维dim上，每行的众数值。 同时返回一个LongTensor，包含众数职的索引。dim值默认为输入张量的最后一维。 输出形状与输入相同，除了给定维度上为1. 注意: 这个函数还没有在torch.cuda.Tensor中定义 参数： input (Tensor) – 输入张量 dim (int) – 缩减的维度 values (Tensor, optional) – 结果张量 indices (Tensor, optional) – 返回的索引张量 例子： &gt;&gt;&gt; a -0.6891 -0.6662 0.2697 0.7412 0.5254 -0.7402 0.5528 -0.2399 [torch.FloatTensor of size 4x2] &gt;&gt;&gt; a = torch.randn(4, 5) &gt;&gt;&gt; a 0.4056 -0.3372 1.0973 -2.4884 0.4334 2.1336 0.3841 0.1404 -0.1821 -0.7646 -0.2403 1.3975 -2.0068 0.1298 0.0212 -1.5371 -0.7257 -0.4871 -0.2359 -1.1724 [torch.FloatTensor of size 4x5] &gt;&gt;&gt; torch.mode(a, 1) ( -2.4884 -0.7646 -2.0068 -1.5371 [torch.FloatTensor of size 4x1] , 3 4 2 0 [torch.LongTensor of size 4x1] ) torch.norm torch.norm(input, p=2) → float返回输入张量input 的p 范数。 参数： input (Tensor) – 输入张量 p (float,optional) – 范数计算中的幂指数值 例子： &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a -0.4376 -0.5328 0.9547 [torch.FloatTensor of size 1x3] &gt;&gt;&gt; torch.norm(a, 3) 1.0338925067372466 torch.norm(input, p, dim, out=None) → Tensor 返回输入张量给定维dim 上每行的p 范数。 输出形状与输入相同，除了给定维度上为1. 参数： input (Tensor) – 输入张量 p (float) – 范数计算中的幂指数值 dim (int) – 缩减的维度 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4, 2) &gt;&gt;&gt; a -0.6891 -0.6662 0.2697 0.7412 0.5254 -0.7402 0.5528 -0.2399 [torch.FloatTensor of size 4x2] &gt;&gt;&gt; torch.norm(a, 2, 1) 0.9585 0.7888 0.9077 0.6026 [torch.FloatTensor of size 4x1] &gt;&gt;&gt; torch.norm(a, 0, 1) 2 2 2 2 [torch.FloatTensor of size 4x1] torch.prod torch.prod(input) → float返回输入张量input 所有元素的积。 参数： input (Tensor) – 输入张量 例子： &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a 0.6170 0.3546 0.0253 [torch.FloatTensor of size 1x3] &gt;&gt;&gt; torch.prod(a) 0.005537458061418483 orch.prod(input, dim, out=None) → Tensor 返回输入张量给定维度上每行的积。 输出形状与输入相同，除了给定维度上为1. 参数： input (Tensor) – 输入张量 dim (int) – 缩减的维度 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4, 2) &gt;&gt;&gt; a 0.1598 -0.6884 -0.1831 -0.4412 -0.9925 -0.6244 -0.2416 -0.8080 [torch.FloatTensor of size 4x2] &gt;&gt;&gt; torch.prod(a, 1) -0.1100 0.0808 0.6197 0.1952 [torch.FloatTensor of size 4x1] torch.std torch.std(input) → float返回输入张量input 所有元素的标准差。 参数： input (Tensor) – 输入张量 例子： &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a -1.3063 1.4182 -0.3061 [torch.FloatTensor of size 1x3] &gt;&gt;&gt; torch.std(a) 1.3782334731508061 torch.std(input, dim, out=None) → Tensor 返回输入张量给定维度上每行的标准差。 输出形状与输入相同，除了给定维度上为1. 参数： input (Tensor) – 输入张量 dim (int) – 缩减的维度 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a 0.1889 -2.4856 0.0043 1.8169 -0.7701 -0.4682 -2.2410 0.4098 0.1919 -1.1856 -1.0361 0.9085 0.0173 1.0662 0.2143 -0.5576 [torch.FloatTensor of size 4x4] &gt;&gt;&gt; torch.std(a, dim=1) 1.7756 1.1025 1.0045 0.6725 [torch.FloatTensor of size 4x1] torch.sum torch.sum(input) → float返回输入张量input 所有元素的和。 输出形状与输入相同，除了给定维度上为1. 参数： input (Tensor) – 输入张量 例子： &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a 0.6170 0.3546 0.0253 [torch.FloatTensor of size 1x3] &gt;&gt;&gt; torch.sum(a) 0.9969287421554327 torch.sum(input, dim, out=None) → Tensor 返回输入张量给定维度上每行的和。 输出形状与输入相同，除了给定维度上为1. 参数： input (Tensor) – 输入张量 dim (int) – 缩减的维度 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a -0.4640 0.0609 0.1122 0.4784 -1.3063 1.6443 0.4714 -0.7396 -1.3561 -0.1959 1.0609 -1.9855 2.6833 0.5746 -0.5709 -0.4430 [torch.FloatTensor of size 4x4] &gt;&gt;&gt; torch.sum(a, 1) 0.1874 0.0698 -2.4767 2.2440 [torch.FloatTensor of size 4x1] torch.var torch.var(input) → float返回输入张量所有元素的方差 输出形状与输入相同，除了给定维度上为1. 参数： input (Tensor) – 输入张量 例子： &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a -1.3063 1.4182 -0.3061 [torch.FloatTensor of size 1x3] &gt;&gt;&gt; torch.var(a) 1.899527506513334 torch.var(input, dim, out=None) → Tensor 返回输入张量给定维度上每行的方差。 输出形状与输入相同，除了给定维度上为1. 参数： input (Tensor) – 输入张量 dim (int) – the dimension to reduce out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a -1.2738 -0.3058 0.1230 -1.9615 0.8771 -0.5430 -0.9233 0.9879 1.4107 0.0317 -0.6823 0.2255 -1.3854 0.4953 -0.2160 0.2435 [torch.FloatTensor of size 4x4] &gt;&gt;&gt; torch.var(a, 1) 0.8859 0.9509 0.7548 0.6949 [torch.FloatTensor of size 4x1] 比较操作 Comparison Ops torch.eq torch.eq(input, other, out=None) → Tensor比较元素相等性。第二个参数可为一个数或与第一个参数同类型形状的张量。 参数： input (Tensor) – 待比较张量 other (Tensor or float) – 比较张量或数 out (Tensor, optional) – 输出张量，须为 ByteTensor类型 or 与input同类型 返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(相等为1，不等为0 ) 返回类型： Tensor 例子： &gt;&gt;&gt; torch.eq(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 1 0 0 1 [torch.ByteTensor of size 2x2] torch.equal torch.equal(tensor1, tensor2) → bool如果两个张量有相同的形状和元素值，则返回True ，否则 False。 例子： &gt;&gt;&gt; torch.equal(torch.Tensor([1, 2]), torch.Tensor([1, 2])) True torch.ge torch.ge(input, other, out=None) → Tensor逐元素比较input和other，即是否 input&gt;=other。 如果两个张量有相同的形状和元素值，则返回True ，否则 False。 第二个参数可以为一个数或与第一个参数相同形状和类型的张量 参数: input (Tensor) – 待对比的张量 other (Tensor or float) – 对比的张量或float值 out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;= other )。 返回类型： Tensor 例子： &gt;&gt;&gt; torch.ge(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 1 1 0 1 [torch.ByteTensor of size 2x2] torch.gt torch.gt(input, other, out=None) → Tensor逐元素比较input和other ， 即是否input&gt;other 如果两个张量有相同的形状和元素值，则返回True ，否则 False。 第二个参数可以为一个数或与第一个参数相同形状和类型的张量 参数: input (Tensor) – 要对比的张量 other (Tensor or float) – 要对比的张量或float值 out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;= other )。 返回类型： Tensor 例子： &gt;&gt;&gt; torch.gt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 0 1 0 0 [torch.ByteTensor of size 2x2] torch.kthvalue torch.kthvalue(input, k, dim=None, out=None) -&gt; (Tensor, LongTensor)取输入张量input指定维上第k 个最小值。如果不指定dim，则默认为input的最后一维。 返回一个元组 (values,indices)，其中indices是原始输入张量input中沿dim维的第 k 个最小值下标。 参数: input (Tensor) – 要对比的张量 k (int) – 第 k 个最小值 dim (int, optional) – 沿着此维进行排序 out (tuple, optional) – 输出元组 (Tensor, LongTensor) 可选地给定作为 输出 buffers 例子： &gt;&gt;&gt; x = torch.arange(1, 6) &gt;&gt;&gt; x 1 2 3 4 5 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.kthvalue(x, 4) ( 4 [torch.FloatTensor of size 1] , 3 [torch.LongTensor of size 1] ) torch.le torch.le(input, other, out=None) → Tensor逐元素比较input和other ， 即是否input&lt;=other 第二个参数可以为一个数或与第一个参数相同形状和类型的张量 参数: input (Tensor) – 要对比的张量 other (Tensor or float ) – 对比的张量或float值 out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;= other )。 返回类型： Tensor 例子： &gt;&gt;&gt; torch.le(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 1 0 1 1 [torch.ByteTensor of size 2x2] torch.lt torch.lt(input, other, out=None) → Tensor逐元素比较input和other ， 即是否 input&lt;other 第二个参数可以为一个数或与第一个参数相同形状和类型的张量 参数: input (Tensor) – 要对比的张量 other (Tensor or float ) – 对比的张量或float值 out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。 input： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 tensor &gt;= other )。 返回类型： Tensor 例子： &gt;&gt;&gt; torch.lt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 0 0 1 0 [torch.ByteTensor of size 2x2] torch.max torch.max()返回输入张量所有元素的最大值。 参数: input (Tensor) – 输入张量 例子： &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a 0.4729 -0.2266 -0.2085 [torch.FloatTensor of size 1x3] &gt;&gt;&gt; torch.max(a) 0.4729 torch.max(input, dim, max=None, max_indices=None) -&gt; (Tensor, LongTensor) 返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引。 输出形状中，将dim维设定为1，其它与输入形状保持一致。 参数: input (Tensor) – 输入张量 dim (int) – 指定的维度 max (Tensor, optional) – 结果张量，包含给定维度上的最大值 max_indices (LongTensor, optional) – 结果张量，包含给定维度上每个最大值的位置索引 例子： &gt;&gt; a = torch.randn(4, 4) &gt;&gt; a 0.0692 0.3142 1.2513 -0.5428 0.9288 0.8552 -0.2073 0.6409 1.0695 -0.0101 -2.4507 -1.2230 0.7426 -0.7666 0.4862 -0.6628 torch.FloatTensor of size 4x4] &gt;&gt;&gt; torch.max(a, 1) ( 1.2513 0.9288 1.0695 0.7426 [torch.FloatTensor of size 4x1] , 2 0 0 0 [torch.LongTensor of size 4x1] ) torch.max(input, other, out=None) → Tensor 返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引。 即，outi=max(inputi,otheri)输出形状中，将dim维设定为1，其它与输入形状保持一致。 参数: input (Tensor) – 输入张量 other (Tensor) – 输出张量 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] &gt;&gt;&gt; b = torch.randn(4) &gt;&gt;&gt; b 1.0067 -0.8010 0.6258 0.3627 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.max(a, b) 1.3869 0.3912 0.6258 0.3627 [torch.FloatTensor of size 4] torch.min torch.min(input) → float返回输入张量所有元素的最小值。 参数: input (Tensor) – 输入张量 例子： &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a 0.4729 -0.2266 -0.2085 [torch.FloatTensor of size 1x3] &gt;&gt;&gt; torch.min(a) -0.22663167119026184 torch.min(input, dim, min=None, min_indices=None) -&gt; (Tensor, LongTensor) 返回输入张量给定维度上每行的最小值，并同时返回每个最小值的位置索引。 输出形状中，将dim维设定为1，其它与输入形状保持一致。 参数: input (Tensor) – 输入张量 dim (int) – 指定的维度 min (Tensor, optional) – 结果张量，包含给定维度上的最小值 min_indices (LongTensor, optional) – 结果张量，包含给定维度上每个最小值的位置索引 例子： &gt;&gt; a = torch.randn(4, 4) &gt;&gt; a 0.0692 0.3142 1.2513 -0.5428 0.9288 0.8552 -0.2073 0.6409 1.0695 -0.0101 -2.4507 -1.2230 0.7426 -0.7666 0.4862 -0.6628 torch.FloatTensor of size 4x4] &gt;&gt; torch.min(a, 1) 0.5428 0.2073 2.4507 0.7666 torch.FloatTensor of size 4x1] 3 2 2 1 torch.LongTensor of size 4x1] torch.min(input, other, out=None) → Tensor input中逐元素与other相应位置的元素对比，返回最小值到输出张量。即，outi=min(tensori,otheri)两张量形状不需匹配，但元素数须相同。 注意：当形状不匹配时，input的形状作为返回张量的形状。 参数: input (Tensor) – 输入张量 other (Tensor) – 第二个输入张量 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] &gt;&gt;&gt; b = torch.randn(4) &gt;&gt;&gt; b 1.0067 -0.8010 0.6258 0.3627 [torch.FloatTensor of size 4] &gt;&gt;&gt; torch.min(a, b) 1.0067 -0.8010 -0.8634 -0.5468 [torch.FloatTensor of size 4] torch.ne torch.ne(input, other, out=None) → Tensor逐元素比较input和other ， 即是否 input!=other。 第二个参数可以为一个数或与第一个参数相同形状和类型的张量 参数: input (Tensor) – 待对比的张量 other (Tensor or float) – 对比的张量或float值 out (Tensor, optional) – 输出张量。必须为ByteTensor或者与input相同类型。 返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果 (如果 tensor != other 为True ，返回1)。 返回类型： Tensor 例子： &gt;&gt;&gt; torch.ne(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 0 1 1 0 [torch.ByteTensor of size 2x2] torch.sort torch.sort(input, dim=None, descending=False, out=None) -&gt; (Tensor, LongTensor)对输入张量input沿着指定维按升序排序。如果不给定dim，则默认为输入的最后一维。如果指定参数descending为True，则按降序排序 返回元组 (sorted_tensor, sorted_indices) ， sorted_indices 为原始输入中的下标。 参数: input (Tensor) – 要对比的张量 dim (int, optional) – 沿着此维排序 descending (bool, optional) – 布尔值，控制升降排序 out (tuple, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。 例子： &gt;&gt;&gt; x = torch.randn(3, 4) &gt;&gt;&gt; sorted, indices = torch.sort(x) &gt;&gt;&gt; sorted -1.6747 0.0610 0.1190 1.4137 -1.4782 0.7159 1.0341 1.3678 -0.3324 -0.0782 0.3518 0.4763 [torch.FloatTensor of size 3x4] &gt;&gt;&gt; indices 0 1 3 2 2 1 0 3 3 1 0 2 [torch.LongTensor of size 3x4] &gt;&gt;&gt; sorted, indices = torch.sort(x, 0) &gt;&gt;&gt; sorted -1.6747 -0.0782 -1.4782 -0.3324 0.3518 0.0610 0.4763 0.1190 1.0341 0.7159 1.4137 1.3678 [torch.FloatTensor of size 3x4] &gt;&gt;&gt; indices 0 2 1 2 2 0 2 0 1 1 0 1 [torch.LongTensor of size 3x4] torch.topk torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor)沿给定dim维度返回输入张量input中 k 个最大值。 如果不指定dim，则默认为input的最后一维。 如果为largest为 False ，则返回最小的 k 个值。 返回一个元组 (values,indices)，其中indices是原始输入张量input中测元素下标。 如果设定布尔值sorted 为True，将会确保返回的 k 个值被排序。 参数: input (Tensor) – 输入张量 k (int) – “top-k”中的k dim (int, optional) – 排序的维 largest (bool, optional) – 布尔值，控制返回最大或最小值 sorted (bool, optional) – 布尔值，控制返回值是否排序 out (tuple, optional) – 可选输出张量 (Tensor, LongTensor) output buffers 例子： &gt;&gt;&gt; x = torch.arange(1, 6) &gt;&gt;&gt; x 1 2 3 4 5 [torch.FloatTensor of size 5] &gt;&gt;&gt; torch.topk(x, 3) ( 5 4 3 [torch.FloatTensor of size 3] , 4 3 2 [torch.LongTensor of size 3] ) &gt;&gt;&gt; torch.topk(x, 3, 0, largest=False) ( 1 2 3 [torch.FloatTensor of size 3] , 0 1 2 [torch.LongTensor of size 3] ) 其它操作 Other Operations torch.cross torch.cross(input, other, dim=-1, out=None) → Tensor返回沿着维度dim上，两个张量input和other的向量积（叉积）。 input和other 必须有相同的形状，且指定的dim维上size必须为3。 如果不指定dim，则默认为第一个尺度为3的维。 参数： input (Tensor) – 输入张量 other (Tensor) – 第二个输入张量 dim (int, optional) – 沿着此维进行叉积操作 out (Tensor,optional) – 结果张量 例子： &gt;&gt;&gt; a = torch.randn(4, 3) &gt;&gt;&gt; a -0.6652 -1.0116 -0.6857 0.2286 0.4446 -0.5272 0.0476 0.2321 1.9991 0.6199 1.1924 -0.9397 [torch.FloatTensor of size 4x3] &gt;&gt;&gt; b = torch.randn(4, 3) &gt;&gt;&gt; b -0.1042 -1.1156 0.1947 0.9947 0.1149 0.4701 -1.0108 0.8319 -0.0750 0.9045 -1.3754 1.0976 [torch.FloatTensor of size 4x3] &gt;&gt;&gt; torch.cross(a, b, dim=1) -0.9619 0.2009 0.6367 0.2696 -0.6318 -0.4160 -1.6805 -2.0171 0.2741 0.0163 -1.5304 -1.9311 [torch.FloatTensor of size 4x3] &gt;&gt;&gt; torch.cross(a, b) -0.9619 0.2009 0.6367 0.2696 -0.6318 -0.4160 -1.6805 -2.0171 0.2741 0.0163 -1.5304 -1.9311 [torch.FloatTensor of size 4x3] torch.diag torch.diag(input, diagonal=0, out=None) → Tensor如果输入是一个向量(1D 张量)，则返回一个以input为对角线元素的2D方阵如果输入是一个矩阵(2D 张量)，则返回一个包含input对角线元素的1D张量参数diagonal指定对角线: diagonal = 0, 主对角线diagonal &gt; 0, 主对角线之上diagonal &lt; 0, 主对角线之下 参数： input (Tensor) – 输入张量 diagonal (int, optional) – 指定对角线 out (Tensor, optional) – 输出张量 例子： 取得以input为对角线的方阵： &gt;&gt;&gt; a = torch.randn(3) &gt;&gt;&gt; a 1.0480 -2.3405 -1.1138 [torch.FloatTensor of size 3] &gt;&gt;&gt; torch.diag(a) 1.0480 0.0000 0.0000 0.0000 -2.3405 0.0000 0.0000 0.0000 -1.1138 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.diag(a, 1) 0.0000 1.0480 0.0000 0.0000 0.0000 0.0000 -2.3405 0.0000 0.0000 0.0000 0.0000 -1.1138 0.0000 0.0000 0.0000 0.0000 [torch.FloatTensor of size 4x4] 取得给定矩阵第k个对角线: &gt;&gt;&gt; a = torch.randn(3, 3) &gt;&gt;&gt; a -1.5328 -1.3210 -1.5204 0.8596 0.0471 -0.2239 -0.6617 0.0146 -1.0817 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.diag(a, 0) -1.5328 0.0471 -1.0817 [torch.FloatTensor of size 3] &gt;&gt;&gt; torch.diag(a, 1) -1.3210 -0.2239 [torch.FloatTensor of size 2] torch.histc torch.histc(input, bins=100, min=0, max=0, out=None) → Tensor计算输入张量的直方图。以min和max为range边界，将其均分成bins个直条，然后将排序好的数据划分到各个直条(bins)中。如果min和max都为0, 则利用数据中的最大最小值作为边界。 参数： input (Tensor) – 输入张量 bins (int) – 直方图 bins(直条)的个数(默认100个) min (int) – range的下边界(包含) max (int) – range的上边界(包含) out (Tensor, optional) – 结果张量 返回： 直方图 返回类型：张量 例子： &gt;&gt;&gt; torch.histc(torch.FloatTensor([1, 2, 1]), bins=4, min=0, max=3) FloatTensor([0, 2, 1, 0]) torch.renorm torch.renorm(input, p, dim, maxnorm, out=None) → Tensor返回一个张量，包含规范化后的各个子张量，使得沿着dim维划分的各子张量的p范数小于maxnorm。 注意 如果p范数的值小于maxnorm，则当前子张量不需要修改。 注意: 更详细解释参考torch7 以及Hinton et al. 2012, p. 2 参数： input (Tensor) – 输入张量 p (float) – 范数的p dim (int) – 沿着此维切片，得到张量子集 maxnorm (float) – 每个子张量的范数的最大值 out (Tensor, optional) – 结果张量 例子： &gt;&gt;&gt; x = torch.ones(3, 3) &gt;&gt;&gt; x[1].fill_(2) &gt;&gt;&gt; x[2].fill_(3) &gt;&gt;&gt; x 1 1 1 2 2 2 3 3 3 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.renorm(x, 1, 0, 5) 1.0000 1.0000 1.0000 1.6667 1.6667 1.6667 1.6667 1.6667 1.6667 [torch.FloatTensor of size 3x3] torch.trace torch.trace(input) → float返回输入2维矩阵对角线元素的和(迹) 例子： &gt;&gt;&gt; x = torch.arange(1, 10).view(3, 3) &gt;&gt;&gt; x 1 2 3 4 5 6 7 8 9 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.trace(x) 15.0 torch.tril torch.tril(input, k=0, out=None) → Tensor返回一个张量out，包含输入矩阵(2D张量)的下三角部分，out其余部分被设为0。这里所说的下三角部分为矩阵指定对角线diagonal之上的元素。 参数k控制对角线: k = 0, 主对角线 k &gt; 0, 主对角线之上 k &lt; 0, 主对角线之下 参数： input (Tensor) – 输入张量 k (int, optional) – 指定对角线 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(3,3) &gt;&gt;&gt; a 1.3225 1.7304 1.4573 -0.3052 -0.3111 -0.1809 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.tril(a) 1.3225 0.0000 0.0000 -0.3052 -0.3111 0.0000 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.tril(a, k=1) 1.3225 1.7304 0.0000 -0.3052 -0.3111 -0.1809 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.tril(a, k=-1) 0.0000 0.0000 0.0000 -0.3052 0.0000 0.0000 1.2469 0.0064 0.0000 [torch.FloatTensor of size 3x3] torch.triu torch.triu(input, k=0, out=None) → Tensor返回一个张量，包含输入矩阵(2D张量)的上三角部分，其余部分被设为0。这里所说的上三角部分为矩阵指定对角线diagonal之上的元素。 参数k控制对角线: k = 0, 主对角线 k &gt; 0, 主对角线之上 k &lt; 0, 主对角线之下 参数： input (Tensor) – 输入张量 k (int, optional) – 指定对角线 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; a = torch.randn(3,3) &gt;&gt;&gt; a 1.3225 1.7304 1.4573 -0.3052 -0.3111 -0.1809 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.triu(a) 1.3225 1.7304 1.4573 0.0000 -0.3111 -0.1809 0.0000 0.0000 -1.6250 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.triu(a, k=1) 0.0000 1.7304 1.4573 0.0000 0.0000 -0.1809 0.0000 0.0000 0.0000 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.triu(a, k=-1) 1.3225 1.7304 1.4573 -0.3052 -0.3111 -0.1809 0.0000 0.0064 -1.6250 [torch.FloatTensor of size 3x3] BLAS and LAPACK Operations torch.addbmm torch.addbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) → Tensor 对两个批batch1和batch2内存储的矩阵进行批矩阵乘操作，附带reduced add 步骤( 所有矩阵乘结果沿着第一维相加)。矩阵mat加到最终结果。 batch1和 batch2都为包含相同数量矩阵的3维张量。 如果batch1是形为b×n×m的张量，batch1是形为b×m×p的张量，则out和mat的形状都是n×p，即 res=(beta∗M)+(alpha∗sum(batch1i@batch2i,i=0,b))对类型为 FloatTensor 或 DoubleTensor 的输入，alphaand beta必须为实数，否则两个参数须为整数。 参数： beta (Number, optional) – 用于mat的乘子 mat (Tensor) – 相加矩阵 alpha (Number, optional) – 用于batch1@batch2的乘子 batch1 (Tensor) – 第一批相乘矩阵 batch2 (Tensor) – 第二批相乘矩阵 out (Tensor, optional) – 输出张量 例子: &gt;&gt;&gt; M = torch.randn(3, 5) &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) &gt;&gt;&gt; torch.addbmm(M, batch1, batch2) -3.1162 11.0071 7.3102 0.1824 -7.6892 1.8265 6.0739 0.4589 -0.5641 -5.4283 -9.3387 -0.1794 -1.2318 -6.8841 -4.7239 [torch.FloatTensor of size 3x5] torch.addmm torch.addmm(beta=1, mat, alpha=1, mat1, mat2, out=None) → Tensor 对矩阵mat1和mat2进行矩阵乘操作。矩阵mat加到最终结果。如果mat1 是一个 n×m张量，mat2 是一个 m×p张量，那么out和mat的形状为n×p。 alpha 和 beta 分别是两个矩阵 mat1@mat2和mat的比例因子，即， out=(beta∗M)+(alpha∗mat1@mat2) 对类型为 FloatTensor 或 DoubleTensor 的输入，betaand alpha必须为实数，否则两个参数须为整数。 参数 ： beta (Number, optional) – 用于mat的乘子 mat (Tensor) – 相加矩阵 alpha (Number, optional) – 用于mat1@mat2的乘子 mat1 (Tensor) – 第一个相乘矩阵 mat2 (Tensor) – 第二个相乘矩阵 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; M = torch.randn(2, 3) &gt;&gt;&gt; mat1 = torch.randn(2, 3) &gt;&gt;&gt; mat2 = torch.randn(3, 3) &gt;&gt;&gt; torch.addmm(M, mat1, mat2) -0.4095 -1.9703 1.3561 5.7674 -4.9760 2.7378 [torch.FloatTensor of size 2x3] torch.addmv torch.addmv(beta=1, tensor, alpha=1, mat, vec, out=None) → Tensor 对矩阵mat和向量vec对进行相乘操作。向量tensor加到最终结果。如果mat 是一个 n×m维矩阵，vec 是一个 m维向量，那么out和mat的为n元向量。 可选参数alpha 和 beta 分别是 mat∗vec和mat的比例因子，即， out=(beta∗tensor)+(alpha∗(mat@vec)) 对类型为FloatTensor或DoubleTensor的输入，alphaand beta必须为实数，否则两个参数须为整数。 参数 ： beta (Number, optional) – 用于mat的乘子 mat (Tensor) – 相加矩阵 alpha (Number, optional) – 用于mat1@vec的乘子 mat (Tensor) – 相乘矩阵 vec (Tensor) – 相乘向量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; M = torch.randn(2) &gt;&gt;&gt; mat = torch.randn(2, 3) &gt;&gt;&gt; vec = torch.randn(3) &gt;&gt;&gt; torch.addmv(M, mat, vec) -2.0939 -2.2950 [torch.FloatTensor of size 2] torch.addr torch.addr(beta=1, mat, alpha=1, vec1, vec2, out=None) → Tensor对向量vec1和vec2对进行张量积操作。矩阵mat加到最终结果。如果vec1 是一个 n维向量，vec2 是一个 m维向量，那么矩阵mat的形状须为n×m。 可选参数beta 和 alpha 分别是两个矩阵 mat和 vec1@vec2的比例因子，即，resi=(beta∗Mi)+(alpha∗batch1i×batch2i) 对类型为FloatTensor或DoubleTensor的输入，alphaand beta必须为实数，否则两个参数须为整数。 参数 ： beta (Number, optional) – 用于mat的乘子 mat (Tensor) – 相加矩阵 alpha (Number, optional) – 用于两向量vec1，vec2外积的乘子 vec1 (Tensor) – 第一个相乘向量 vec2 (Tensor) – 第二个相乘向量 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; vec1 = torch.arange(1, 4) &gt;&gt;&gt; vec2 = torch.arange(1, 3) &gt;&gt;&gt; M = torch.zeros(3, 2) &gt;&gt;&gt; torch.addr(M, vec1, vec2) 1 2 2 4 3 6 [torch.FloatTensor of size 3x2] torch.baddbmm torch.baddbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) → Tensor 对两个批batch1和batch2内存储的矩阵进行批矩阵乘操作，矩阵mat加到最终结果。 batch1和 batch2都为包含相同数量矩阵的3维张量。 如果batch1是形为b×n×m的张量，batch1是形为b×m×p的张量，则out和mat的形状都是n×p，即 resi=(beta∗Mi)+(alpha∗batch1i×batch2i)对类型为FloatTensor或DoubleTensor的输入，alphaand beta必须为实数，否则两个参数须为整数。 参数： beta (Number, optional) – 用于mat的乘子 mat (Tensor) – 相加矩阵 alpha (Number, optional) – 用于batch1@batch2的乘子 batch1 (Tensor) – 第一批相乘矩阵 batch2 (Tensor) – 第二批相乘矩阵 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; M = torch.randn(10, 3, 5) &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) &gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size() torch.Size([10, 3, 5]) torch.bmm torch.bmm(batch1, batch2, out=None) → Tensor对存储在两个批batch1和batch2内的矩阵进行批矩阵乘操作。batch1和 batch2都为包含相同数量矩阵的3维张量。 如果batch1是形为b×n×m的张量，batch1是形为b×m×p的张量，则out和mat的形状都是n×p，即 res=(beta∗M)+(alpha∗sum(batch1i@batch2i,i=0,b))对类型为 FloatTensor 或 DoubleTensor 的输入，alphaand beta必须为实数，否则两个参数须为整数。 参数： batch1 (Tensor) – 第一批相乘矩阵 batch2 (Tensor) – 第二批相乘矩阵 out (Tensor, optional) – 输出张量 例子： &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) &gt;&gt;&gt; res = torch.bmm(batch1, batch2) &gt;&gt;&gt; res.size() torch.Size([10, 3, 5]) torch.btrifact torch.btrifact(A, info=None) → Tensor, IntTensor返回一个元组，包含LU 分解和pivots 。 可选参数info决定是否对每个minibatch样本进行分解。info are from dgetrf and a non-zero value indicates an error occurred. 如果用CUDA的话，这个值来自于CUBLAS，否则来自LAPACK。 参数： A (Tensor) – 待分解张量 例子： &gt;&gt;&gt; A = torch.randn(2, 3, 3) &gt;&gt;&gt; A_LU = A.btrifact() torch.btrisolve torch.btrisolve(b, LU_data, LU_pivots) → Tensor返回线性方程组Ax=b的LU解。 参数： b (Tensor) – RHS 张量. LU_data (Tensor) – Pivoted LU factorization of A from btrifact. LU_pivots (IntTensor) – LU 分解的Pivots. 例子： &gt;&gt;&gt; A = torch.randn(2, 3, 3) &gt;&gt;&gt; b = torch.randn(2, 3) &gt;&gt;&gt; A_LU = torch.btrifact(A) &gt;&gt;&gt; x = b.btrisolve(*A_LU) &gt;&gt;&gt; torch.norm(A.bmm(x.unsqueeze(2)) - b) 6.664001874625056e-08 torch.dot torch.dot(tensor1, tensor2) → float计算两个张量的点乘(内乘),两个张量都为1-D 向量. 例子： &gt;&gt;&gt; torch.dot(torch.Tensor([2, 3]), torch.Tensor([2, 1])) 7.0 torch.eig torch.eig(a, eigenvectors=False, out=None) -&gt; (Tensor, Tensor)计算实方阵a 的特征值和特征向量 参数： a (Tensor) – 方阵，待计算其特征值和特征向量 eigenvectors (bool) – 布尔值，如果True，则同时计算特征值和特征向量，否则只计算特征值。 out (tuple, optional) – 输出元组 返回值：元组，包括： e (Tensor): a 的右特征向量 v (Tensor): 如果eigenvectors为True，则为包含特征向量的张量; 否则为空张量 返回值类型： (Tensor, Tensor) torch.gels torch.gels(B, A, out=None) → Tensor对形如m×n的满秩矩阵a计算其最小二乘和最小范数问题的解。 如果m&gt;=n,gels对最小二乘问题进行求解，即：minimize∥AX−B∥F如果m&lt;n,gels求解最小范数问题，即：minimize∥X∥Fsubject toabAX=B返回矩阵X的前n 行包含解。余下的行包含以下残差信息: 相应列从第n 行开始计算的每列的欧式距离。 注意： 返回矩阵总是被转置，无论输入矩阵的原始布局如何，总会被转置；即，总是有 stride (1, m) 而不是 (m, 1). 参数： B (Tensor) – 矩阵B A (Tensor) – m×n矩阵 out (tuple, optional) – 输出元组 返回值： 元组，包括： X (Tensor): 最小二乘解 qr (Tensor): QR 分解的细节 返回值类型： (Tensor, Tensor) 例子： &gt;&gt;&gt; A = torch.Tensor([[1, 1, 1], ... [2, 3, 4], ... [3, 5, 2], ... [4, 2, 5], ... [5, 4, 3]]) &gt;&gt;&gt; B = torch.Tensor([[-10, -3], [ 12, 14], [ 14, 12], [ 16, 16], [ 18, 16]]) &gt;&gt;&gt; X, _ = torch.gels(B, A) &gt;&gt;&gt; X 2.0000 1.0000 1.0000 1.0000 1.0000 2.0000 [torch.FloatTensor of size 3x2] torch.geqrf torch.geqrf(input, out=None) -&gt; (Tensor, Tensor) 这是一个直接调用LAPACK的底层函数。 一般使用torch.qr() 计算输入的QR 分解，但是并不会分别创建Q,R两个矩阵，而是直接调用LAPACK 函数 Rather, this directly calls the underlying LAPACK function ?geqrf which produces a sequence of ‘elementary reflectors’. 参考 LAPACK文档获取更详细信息。 参数: input (Tensor) – 输入矩阵 out (tuple, optional) – 元组，包含输出张量 (Tensor, Tensor) torch.ger torch.ger(vec1, vec2, out=None) → Tensor计算两向量vec1,vec2的张量积。如果vec1的长度为n,vec2长度为m，则输出out应为形如n x m的矩阵。 参数: vec1 (Tensor) – 1D 输入向量 vec2 (Tensor) – 1D 输入向量 out (tuple, optional) – 输出张量 例子： &gt;&gt;&gt; v1 = torch.arange(1, 5) &gt;&gt;&gt; v2 = torch.arange(1, 4) &gt;&gt;&gt; torch.ger(v1, v2) 1 2 3 2 4 6 3 6 9 4 8 12 [torch.FloatTensor of size 4x3] torch.gesv torch.gesv(B, A, out=None) -&gt; (Tensor, Tensor) X,LU=torch.gesv(B,A)，返回线性方程组AX=B的解。 LU 包含两个矩阵L，U。A须为非奇异方阵，如果A是一个m×m矩阵，B 是m×k矩阵，则LU 是m×m矩阵， X为m×k矩阵 参数： B (Tensor) – m×k矩阵 A (Tensor) – m×m矩阵 out (Tensor, optional) – 可选地输出矩阵X 例子: &gt;&gt;&gt; A = torch.Tensor([[6.80, -2.11, 5.66, 5.97, 8.23], ... [-6.05, -3.30, 5.36, -4.44, 1.08], ... [-0.45, 2.58, -2.70, 0.27, 9.04], ... [8.32, 2.71, 4.35, -7.17, 2.14], ... [-9.67, -5.14, -7.26, 6.08, -6.87]]).t() &gt;&gt;&gt; B = torch.Tensor([[4.02, 6.19, -8.22, -7.57, -3.03], ... [-1.56, 4.00, -8.67, 1.75, 2.86], ... [9.81, -4.09, -4.57, -8.61, 8.99]]).t() &gt;&gt;&gt; X, LU = torch.gesv(B, A) &gt;&gt;&gt; torch.dist(B, torch.mm(A, X)) 9.250057093890353e-06 torch.inverse torch.inverse(input, out=None) → Tensor 对方阵输入input 取逆。 注意： 返回矩阵总是被转置，无论输入矩阵的原始布局如何，总会被转置；即，总是有 stride (1, m) 而不是 (m, 1). 参数 ： input (Tensor) – 输入2维张量 out (Tensor, optional) – 输出张量 例子: &gt;&gt;&gt; x = torch.rand(10, 10) &gt;&gt;&gt; x 0.7800 0.2267 0.7855 0.9479 0.5914 0.7119 0.4437 0.9131 0.1289 0.1982 0.0045 0.0425 0.2229 0.4626 0.6210 0.0207 0.6338 0.7067 0.6381 0.8196 0.8350 0.7810 0.8526 0.9364 0.7504 0.2737 0.0694 0.5899 0.8516 0.3883 0.6280 0.6016 0.5357 0.2936 0.7827 0.2772 0.0744 0.2627 0.6326 0.9153 0.7897 0.0226 0.3102 0.0198 0.9415 0.9896 0.3528 0.9397 0.2074 0.6980 0.5235 0.6119 0.6522 0.3399 0.3205 0.5555 0.8454 0.3792 0.4927 0.6086 0.1048 0.0328 0.5734 0.6318 0.9802 0.4458 0.0979 0.3320 0.3701 0.0909 0.2616 0.3485 0.4370 0.5620 0.5291 0.8295 0.7693 0.1807 0.0650 0.8497 0.1655 0.2192 0.6913 0.0093 0.0178 0.3064 0.6715 0.5101 0.2561 0.3396 0.4370 0.4695 0.8333 0.1180 0.4266 0.4161 0.0699 0.4263 0.8865 0.2578 [torch.FloatTensor of size 10x10] &gt;&gt;&gt; x = torch.rand(10, 10) &gt;&gt;&gt; y = torch.inverse(x) &gt;&gt;&gt; z = torch.mm(x, y) &gt;&gt;&gt; z 1.0000 0.0000 0.0000 -0.0000 0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 1.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 -0.0000 -0.0000 0.0000 0.0000 1.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 1.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 1.0000 0.0000 0.0000 -0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 0.0000 1.0000 -0.0000 -0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 0.0000 0.0000 1.0000 0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 -0.0000 1.0000 -0.0000 0.0000 -0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 -0.0000 -0.0000 1.0000 -0.0000 -0.0000 0.0000 -0.0000 -0.0000 -0.0000 0.0000 -0.0000 -0.0000 0.0000 1.0000 [torch.FloatTensor of size 10x10] &gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(10))) # Max nonzero 5.096662789583206e-07 torch.mm torch.mm(mat1, mat2, out=None) → Tensor 对矩阵mat1和mat2进行相乘。 如果mat1 是一个n×m 张量，mat2 是一个 m×p 张量，将会输出一个 n×p 张量out。 参数 ： mat1 (Tensor) – 第一个相乘矩阵 mat2 (Tensor) – 第二个相乘矩阵 out (Tensor, optional) – 输出张量 例子: &gt;&gt;&gt; mat1 = torch.randn(2, 3) &gt;&gt;&gt; mat2 = torch.randn(3, 3) &gt;&gt;&gt; torch.mm(mat1, mat2) 0.0519 -0.3304 1.2232 4.3910 -5.1498 2.7571 [torch.FloatTensor of size 2x3] torch.mv torch.mv(mat, vec, out=None) → Tensor 对矩阵mat和向量vec进行相乘。 如果mat 是一个n×m张量，vec 是一个m元 1维张量，将会输出一个n 元 1维张量。 参数 ： mat (Tensor) – 相乘矩阵 vec (Tensor) – 相乘向量 out (Tensor, optional) – 输出张量 例子: &gt;&gt;&gt; mat = torch.randn(2, 3) &gt;&gt;&gt; vec = torch.randn(3) &gt;&gt;&gt; torch.mv(mat, vec) -2.0939 -2.2950 [torch.FloatTensor of size 2] torch.orgqr torch.orgqr() torch.ormqr torch.ormqr() torch.potrf torch.potrf() torch.potri torch.potri() torch.potrs torch.potrs() torch.pstrf torch.pstrf() torch.qr torch.qr(input, out=None) -&gt; (Tensor, Tensor) 计算输入矩阵的QR分解：返回两个矩阵q ,r， 使得 x=q∗r ，这里q 是一个半正交矩阵与 r 是一个上三角矩阵 本函数返回一个thin(reduced)QR分解。 注意 如果输入很大，可能可能会丢失精度。 注意 本函数依赖于你的LAPACK实现，虽然总能返回一个合法的分解，但不同平台可能得到不同的结果。 参数： input (Tensor) – 输入的2维张量 out (tuple, optional) – 输出元组tuple，包含Q和R 例子: &gt;&gt;&gt; a = torch.Tensor([[12, -51, 4], [6, 167, -68], [-4, 24, -41]]) &gt;&gt;&gt; q, r = torch.qr(a) &gt;&gt;&gt; q -0.8571 0.3943 0.3314 -0.4286 -0.9029 -0.0343 0.2857 -0.1714 0.9429 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; r -14.0000 -21.0000 14.0000 0.0000 -175.0000 70.0000 0.0000 0.0000 -35.0000 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.mm(q, r).round() 12 -51 4 6 167 -68 -4 24 -41 [torch.FloatTensor of size 3x3] &gt;&gt;&gt; torch.mm(q.t(), q).round() 1 -0 0 -0 1 0 0 0 1 [torch.FloatTensor of size 3x3] torch.svd torch.svd(input, some=True, out=None) -&gt; (Tensor, Tensor, Tensor) U,S,V=torch.svd(A)。 返回对形如 n×m的实矩阵 A 进行奇异值分解的结果，使得 A=USV’∗。 U 形状为 n×n，S 形状为 n×m ，V 形状为 m×m some 代表了需要计算的奇异值数目。如果 some=True, it computes some and some=False computes all. Irrespective of the original strides, the returned matrix U will be transposed, i.e. with strides (1, n) instead of (n, 1). 参数： input (Tensor) – 输入的2维张量 some (bool, optional) – 布尔值，控制需计算的奇异值数目 out (tuple, optional) – 结果tuple 例子： &gt;&gt;&gt; a = torch.Tensor([[8.79, 6.11, -9.15, 9.57, -3.49, 9.84], ... [9.93, 6.91, -7.93, 1.64, 4.02, 0.15], ... [9.83, 5.04, 4.86, 8.83, 9.80, -8.99], ... [5.45, -0.27, 4.85, 0.74, 10.00, -6.02], ... [3.16, 7.98, 3.01, 5.80, 4.27, -5.31]]).t() &gt;&gt;&gt; a 8.7900 9.9300 9.8300 5.4500 3.1600 6.1100 6.9100 5.0400 -0.2700 7.9800 -9.1500 -7.9300 4.8600 4.8500 3.0100 9.5700 1.6400 8.8300 0.7400 5.8000 -3.4900 4.0200 9.8000 10.0000 4.2700 9.8400 0.1500 -8.9900 -6.0200 -5.3100 [torch.FloatTensor of size 6x5] &gt;&gt;&gt; u, s, v = torch.svd(a) &gt;&gt;&gt; u -0.5911 0.2632 0.3554 0.3143 0.2299 -0.3976 0.2438 -0.2224 -0.7535 -0.3636 -0.0335 -0.6003 -0.4508 0.2334 -0.3055 -0.4297 0.2362 -0.6859 0.3319 0.1649 -0.4697 -0.3509 0.3874 0.1587 -0.5183 0.2934 0.5763 -0.0209 0.3791 -0.6526 [torch.FloatTensor of size 6x5] &gt;&gt;&gt; s 27.4687 22.6432 8.5584 5.9857 2.0149 [torch.FloatTensor of size 5] &gt;&gt;&gt; v -0.2514 0.8148 -0.2606 0.3967 -0.2180 -0.3968 0.3587 0.7008 -0.4507 0.1402 -0.6922 -0.2489 -0.2208 0.2513 0.5891 -0.3662 -0.3686 0.3859 0.4342 -0.6265 -0.4076 -0.0980 -0.4932 -0.6227 -0.4396 [torch.FloatTensor of size 5x5] &gt;&gt;&gt; torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t())) 8.934150226306685e-06 torch.symeig torch.symeig(input, eigenvectors=False, upper=True, out=None) -&gt; (Tensor, Tensor)e,V=torch.symeig(input) 返回实对称矩阵input的特征值和特征向量。 input 和 V 为 m×m 矩阵，e 是一个m 维向量。 此函数计算intput的所有特征值(和特征向量)，使得 input=Vdiag(e)V′布尔值参数eigenvectors 规定是否只计算特征向量。如果为False，则只计算特征值；若设为True，则两者都会计算。 因为输入矩阵 input 是对称的，所以默认只需要上三角矩阵。如果参数upper为 False，下三角矩阵部分也被利用。 注意： 返回矩阵总是被转置，无论输入矩阵的原始布局如何，总会被转置；即，总是有 stride (1, m) 而不是 (m, 1). 参数： input (Tensor) – 输入对称矩阵 eigenvectors (boolean, optional) – 布尔值（可选），控制是否计算特征向量 upper (boolean, optional) – 布尔值（可选），控制是否考虑上三角或下三角区域 out (tuple, optional) – 输出元组(Tensor, Tensor) 例子： &gt;&gt;&gt; a = torch.Tensor([[ 1.96, 0.00, 0.00, 0.00, 0.00], ... [-6.49, 3.80, 0.00, 0.00, 0.00], ... [-0.47, -6.39, 4.17, 0.00, 0.00], ... [-7.20, 1.50, -1.51, 5.70, 0.00], ... [-0.65, -6.34, 2.67, 1.80, -7.10]]).t() &gt;&gt;&gt; e, v = torch.symeig(a, eigenvectors=True) &gt;&gt;&gt; e -11.0656 -6.2287 0.8640 8.8655 16.0948 [torch.FloatTensor of size 5] &gt;&gt;&gt; v -0.2981 -0.6075 0.4026 -0.3745 0.4896 -0.5078 -0.2880 -0.4066 -0.3572 -0.6053 -0.0816 -0.3843 -0.6600 0.5008 0.3991 -0.0036 -0.4467 0.4553 0.6204 -0.4564 -0.8041 0.4480 0.1725 0.3108 0.1622 [torch.FloatTensor of size 5x5] torch.trtrs torch.trtrs()]]></content>
      <categories>
        <category>Pytorch框架</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于facenet的实时人脸检测]]></title>
    <url>%2F2018%2F07%2F27%2F%E5%9F%BA%E4%BA%8Efacenet%E7%9A%84%E5%AE%9E%E6%97%B6%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[参考自 https://github.com/shanren7/real_time_face_recognition 本人的项目代码 https://github.com/zouzhen/real_time_face_recognize 虽然名字相同，但里面的内容可是有很大的不同由于不能满足当前的tensorflow版本，以及未能满足设计要求，进行了优化与重新设计 基于facenet的实时人脸检测工作环境 python 3.6 tensorflow==1.9.0(可运行在无gpu版) 代码结构real_time_face_recognize |—— model_check_point（保存人脸识别模型） |—— models（储存了facenet采用的神经网络模型） |—— detect_face.py(主要实现人脸的检测，同时返回可能的人脸框) |—— facenet.py（这里存储了facenet的主要函数） |—— real_time_face_recognize.py(实现了实时人脸检测) 运行 从 https://github.com/davidsandberg/facenet 中下载预训练的分类模型，放在model_check_point下 使用pip install requirements.txt安装需要的包，建议在virtualenv环境安装 在目录下新建picture文件，将需要识别的人的图片放入其中，每人放入一张清晰的图片即可 执行python real_time_face_recognize.py 注意除可在facenet作者的github中下载模型外，我自己基于lfw训练集训练了一个模型，点击]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>人脸识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB学习记录]]></title>
    <url>%2F2018%2F07%2F16%2FMongoDB%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[MongoDB转载自https://github.com/zxhyJack/MyBlog/blob/master/mongodb/mongodb.md 基本概念 文档 是键值对的有序集合，这是MongDB的核心概念 集合 集合就是一组文档 动态模式 集合是动态模式的，这意味着集合里面的文档可以是是各式各样的 命名 集合使用名称进行命名 数据库 由多个集合构成数据库，一个MongDB实例可以承载多个数据库，每个数据库拥有0个或多个集合 基本操作 在终端运行mongod命令，启动时，shell将自动连接MongDB数据库，需确保数据库已启动，可充分利用Javascript的标准库，还可定义和调用Javascript函数。 创建数据库 use database_name 如果数据库存在，则进入指定数据库，否则，创建数据库此时需要写入数据，数据库才能真正创建成功 查看所有数据库 show databases | dbs 创建集合 db.createCollection(collection_name) 删除数据库先进入要删除的数据库，然后执行命令 db.dropDatabase() 删除集合 db.collection_name.drop() 增 db.collection_name.insert(document) exp: db.students.insert({ name:&apos;James&apos;, age: 32, gender:&apos;man&apos;, career:&apos;player&apos; }) 查 db.collection.find(&lt;query&gt;,&lt;projection&gt;) - query: 查询条件 - projection: 投影操作 exp: db.students.find() 改 db.collection.updateOne(&lt;query&gt;,&lt;update&gt;) // 更新第一个符合条件的集合 db.collection.updateMany(&lt;query&gt;,&lt;update&gt;) // 更新所有符合条件的集合 query: 查询条件 update： 更新的内容 exp: db.students.update({name:’James’},{$set:{gender:’woman’}}) 删 db.collection_name.deleteOne(&lt;query&gt;) // 删除第一个符合条件的集合 db.collection_name.deleteMany(&lt;query&gt;) // 删除所有符合条件的集合 exp: db.students.deleteOne({name:&apos;James&apos;}) 数据操作（重点）数据库的核心——CRUD，增加和删除较为简单，查询和修改较复杂 查询关系运算符 $gt 大于 $lt 小于 $gte 大于等于 $lte 小于等于 $eq | (key: value) 等于 $ne 不等于 先往数据库中添加一些数据 db.students.insert({&apos;name&apos;:&apos;张三&apos;,&apos;sex&apos;:&apos;男&apos;,&apos;age&apos;:19,&apos;score&apos;: 89,&apos;address&apos;: &apos;海淀区&apos;}) db.students.insert({&apos;name&apos;:&apos;李四&apos;,&apos;sex&apos;:&apos;女&apos;,&apos;age&apos;:20,&apos;score&apos;: 100,&apos;address&apos;: &apos;朝阳区&apos;}) db.students.insert({&apos;name&apos;:&apos;王五&apos;,&apos;sex&apos;:&apos;男&apos;,&apos;age&apos;:22,&apos;score&apos;: 50,&apos;address&apos;: &apos;西城区&apos;}) db.students.insert({&apos;name&apos;:&apos;赵六&apos;,&apos;sex&apos;:&apos;女&apos;,&apos;age&apos;:21,&apos;score&apos;: 60,&apos;address&apos;: &apos;东城区&apos;}) db.students.insert({&apos;name&apos;:&apos;孙七&apos;,&apos;sex&apos;:&apos;男&apos;,&apos;age&apos;:19,&apos;score&apos;: 70,&apos;address&apos;: &apos;海淀区&apos;}) db.students.insert({&apos;name&apos;:&apos;王八&apos;,&apos;sex&apos;:&apos;女&apos;,&apos;age&apos;:23,&apos;score&apos;: 90,&apos;address&apos;: &apos;海淀区&apos;}) db.students.insert({&apos;name&apos;:&apos;刘九&apos;,&apos;sex&apos;:&apos;女&apos;,&apos;age&apos;:35,&apos;score&apos;: 56,&apos;address&apos;: &apos;朝阳区&apos;}) db.students.insert({&apos;name&apos;:&apos;钱十&apos;,&apos;sex&apos;:&apos;男&apos;,&apos;age&apos;:27,&apos;score&apos;: 89,&apos;address&apos;: &apos;海淀区&apos;}) exp: 查询姓名是张三的学生信息 db.students.find({name:’张三’}).pretty() 查询性别是男的学生信息 db.students.find({sex:’男’}).pretty() 查询年龄大于19岁的学生 db.students.find({age:{$gt:19}}).pretty() 查询成绩大于等于60分的学生 db.students.find({score:{$gte:60}}).pretty() 查询姓名不是王五的信息 db.students.find({name:{$ne:’王五’}}).pretty() 逻辑运算符 $and 与 $or 或 $not | $nor 非 exp: 查询年龄在19 ~ 22岁的学生信息 db.students.find({age:{$gte:19,$lte:22}}).pretty() 逻辑运算中与连接是最容易的，只需要利用,分割多个条件即可 查询年龄小于20岁，或者成绩大于90分的学生信息 db.students.find( {$or: [ {age:{$lt:20}}, {score:{$gt:90}} ] }).pretty() 查询年龄大于等于20岁，且成绩小于等于90分的学生信息 db.students.find( {$and: [ {age:{$gte:20}}, {score:{$lte:90}} ] }).pretty() 查询年龄小于20岁的学生信息 db.students.find({age:{$lt:20}}).pretty() db.students.find({age:{$not:{$gte:20}}}).pretty() 取模$mod:[除数，余数] exp: 查询年龄除以20余1的学生信息 db.students.find({age:{$mod:[20,1]}}).pretty() 范围查询$in: 在范围之中$nin: 不在范围之中 exp: 查询姓名是”张三“、”李四、”王五“的学生 db.students.find({name: {$in:[&apos;张三&apos;,&apos;李四&apos;,&apos;王五&apos;]}}).pret ty() 查询姓名不是”张三“、”李四、”王五“的学生 db.students.find({name: {$nin:[&apos;张三&apos;,&apos;李四&apos;,&apos;王五&apos;]}}).pretty() 数组查询 $all $size $slice $elemMatch 首先在数据库中新增一些数据 db.students.insert({name:&apos;a&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,course:[&apos;语文&apos;,&apos;数学&apos;,&apos;英语&apos;,&apos;音乐&apos;,&apos;政治&apos;]}) db.students.insert({name:&apos;b&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,course:[&apos;语文&apos;,&apos;数学&apos;]}) db.students.insert({name:&apos;c&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,course:[&apos;语文&apos;,&apos;数学&apos;,&apos;英语&apos;]}) db.students.insert({name:&apos;d&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,course:[&apos;英语&apos;,&apos;音乐&apos;,&apos;政治&apos;]}) db.students.insert({name:&apos;e&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,course:[&apos;语文&apos;,&apos;政治&apos;]}) $all: 表示全都包括，用法： {$all:[内容1,内容2]} exp: 查询同时参加语文和数学的学生 db.students.find({course:{$all:[&apos;语文&apos;,&apos;数学&apos;]}}).pretty() 数组的操作，可以利用索引，使用key.index的方式来定义索引 查询数组中第二个内容是数学的学生(sh) db.students.find({&apos;course.1&apos;:&apos;数学&apos;}).pretty() $size: 控制数组元素数量 exp: 查询只有两门课程的学生 db.students.find({course:{$size: 2}}).pretty() $slice: 控制查询结果的返回数量 exp: 查询年龄是19岁的学生，要求之显示两门参加的课程 db.students.find({age:19},{course:{$slice:2}}).pretty() 此时查询返回的是前两门课程，可以设置参数来取出想要的内容 $slice:-2 //后两门 $slice: [1,2] // 第一个参数表示跳过的数据量，第二个参数表示返回的数据量 嵌套集合运算对象里面套对象 在数据库中新增数据 db.students.insert( { name:&apos;A&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;, course:[&apos;语文&apos;,&apos;数学&apos;,&apos;英语&apos;,&apos;音乐&apos;,&apos;政治&apos;], parents:[ {name:&apos;A(father)&apos;,age:50,job:&apos;工人&apos;}, {name:&apos;A(mother)&apos;,age:50,job:&apos;职员&apos;} ] }) db.students.insert( { name:&apos;B&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;, course:[&apos;语文&apos;,&apos;数学&apos;], parents:[ {name:&apos;B(father)&apos;,age:50,job:&apos;处长&apos;}, {name:&apos;B(mother)&apos;,age:50,job:&apos;局长&apos;} ] }) db.students.insert( { name:&apos;C&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;, course:[&apos;语文&apos;,&apos;数学&apos;,&apos;英语&apos;], parents:[ {name:&apos;C(father)&apos;,age:50,job:&apos;工人&apos;}, {name:&apos;C(mother)&apos;,age:50,job:&apos;局长&apos;} ] }) 对于嵌套的集合中数据的判断只能通过$elemMatch完成 语法：{ &lt;field&gt;: { $elemMatch: { &lt;query1&gt;, &lt;query2&gt;, ... } } } exp: 查询父母中有人是局长的信息 db.students.find({parents: {$elemMatch: {job: &apos;局长&apos;}}}).pretty() 判断某个字段是否存在{$exists:flag} flag为true表示存在，false表示不存在 exp: 查询具有parents成员的学生 db.students.find({parents:{$exists: true}}).pretty() 查询不具有course成员的学生 db.students.find({course: {$exists: false}}).pretty() 排序sort({ field: value }) value是1表示升序，-1表示降序 exp: 学生信息按照分数降序排列 db.students.find().sort({score:-1}).pretty() 分页显示skip(n): 跳过n条数据 limit(n): 返回n条数据 exp: 分页显示，第一页，每页显示5条数据 db.students.find({}).skip(0).limit(5).pretty() 分页显示，第二页，每页显示5条数据 db.students.find({}).skip(5).limit(5).pretty() 数据修改 | 更新updateOne() 修改匹配的第一条数据 updateMany() 修改所有匹配的数据 格式：updateOne(&lt;filter&gt;,&lt;update&gt;) 修改器$inc: 操作数字字段的数据内容 语法: {&quot;$inc&quot; : {成员 : 内容}} exp: 将所有年龄为19岁的学生成绩一律减少30分，年龄增加1 db.students.updateMany({age:19},{$inc:{score:-30,age:1}}) $set: 更新内容 语法：{$set: :{属性: 新内容}} exp: 将20岁学生的成绩修改为89 db.students.updateMany({age: 20},{$set: {score: 89}}) $unset: 删除某个属性及其内容 语法：{$unset: {属性: 1}} exp: 删除张三的年龄和成绩信息 db.students.updateOne({name:&apos;张三&apos;},{$unset: {age: 1,score: 1}}) $push: 向数组中添加数据 语法：{$push: {属性: value}} exp: 在李四的课程中添加语文 db.students.updateOne({name: &apos;李四&apos;},{$push: {course: &apos;语文&apos;}}) 如果需要向数组中添加多个数据，则需要用到$each exp: 在李四的课程中添加数学、英语 db.students.updateOne( {name:&apos;李四&apos;}, {$push: { course:{$each: [&apos;数学&apos;,&apos;英语&apos;]} } } ) $addToSet: 向数组里面添加一个新的数据 与$push的区别，$push添加的数据可能是重复的，$addToSet只有这个数据不存在时才会添加（去重） 语法：{$addToSet: {属性：value}} exp: 王五新增一门舞蹈课程 db.students.updateOne( {name:&apos;王五&apos;}, {$addToSet: {course:&apos;舞蹈&apos;}} ) $pop: 删除数组内的数据 语法：{$pop: {field: value}},value为-1表示删除第一个，value为1表示删除最后一个 exp: 删除王五的第一个课程 db.students.updateOne({name:&apos;王五&apos;},{$pop:{course:-1}}) 只是删除属性的内容，属性还在 $pull: 从数组中删除一个指定内容的数据 语法：{$pull: {field：value}} 进行数据比对，如果是该数据则删除 exp: 删除李四的语文课程 db.students.updateOne({name: &apos;李四&apos;},{$pull:{course:&apos;语文&apos;}}) $pullAll: 一次删除多个数据 语法：{$pullAll:{field:[value1,value2...]}} exp: 删除a的语文数学英语课程 db.students.updateOne({name:&apos;a&apos;},{$pullAll:{course:[&apos;语文&apos;,&apos;数学&apos;,&apos;英语&apos;]}}) $rename: 属性重命名 语法： {$rename: {旧属性名：新属性名}} exp: 把张三的name属性名改为姓名 db.students.updateOne({name:&apos;张三&apos;},{$rename:{name:&apos;姓名&apos;}})]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[facenet详解]]></title>
    <url>%2F2018%2F07%2F14%2Ffacenet%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[facenet算法初测 facenet代码地址 数据对齐详解 已训练模型下载(基于CASIA-WebFace) 主要参考博客 算法代码结构结构如图： |—— contribute (包含对人脸进行处理的函数) |—— data (原算法进行训练或测试时使用的图片数据) |—— lfw (储存的lfw数据集) |—— lfw_mtcnnpy_160 (储存的经过对齐后的图片数据) |—— models (存储训练模型) |—— src (核心功能相关的代码) |—— test (算法、模型测试相关的代码) |—— tmp (暂不清楚) |—— 其他 功能测试 预训练模型测试： python src/validate_on_lfw.py lfw_mtcnnpy_160 models\20180408-102900 然而测试结果并不是特别好，可能是仅用CPU的缘故 相似人脸对比结果： python src\compare.py models\20180408-102900 data\images\Anthony_Hopkins_0001.jpg data\images\Anthony_Hopkins_0002.jpg 不相似人脸对比结果： python src\compare.py models\20180408-102900 data\images\Anthony_Hopkins_0001.jpg lfw_mtcnnpy_160\Aaron_Eckhart\Aaron_Eckhart_0001.png]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>facenet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript使用记录]]></title>
    <url>%2F2018%2F07%2F12%2FJavaScript%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[Promise在JavaScript的世界中，所有代码都是单线程执行的。 由于这个“缺陷”，导致JavaScript的所有网络操作，浏览器事件，都必须是异步执行。异步执行可以用回调函数实现： function callback() { console.log(&apos;Done&apos;); } console.log(&apos;before setTimeout()&apos;); setTimeout(callback, 1000); // 1秒钟后调用callback函数 console.log(&apos;after setTimeout()&apos;); 观察上述代码执行，在Chrome的控制台输出可以看到： before setTimeout() after setTimeout() (等待1秒后) Done 可见，异步操作会在将来的某个时间点触发一个函数调用。 AJAX就是典型的异步操作。以之前的代码为例： request.onreadystatechange = function () { if (request.readyState === 4) { if (request.status === 200) { return success(request.responseText); } else { return fail(request.status); } } } 把回调函数success(request.responseText)和fail(request.status)写到一个AJAX操作里很正常，但是不好看，而且不利于代码复用。 有没有更好的写法？比如写成这样： var ajax = ajaxGet(&apos;http://...&apos;); ajax.ifSuccess(success) .ifFail(fail); 这种链式写法的好处在于，先统一执行AJAX逻辑，不关心如何处理结果，然后，根据结果是成功还是失败，在将来的某个时候调用success函数或fail函数。 古人云：“君子一诺千金”，这种“承诺将来会执行”的对象在JavaScript中称为Promise对象。Promise有各种开源实现，在ES6中被统一规范，由浏览器直接支持。 我们先看一个最简单的Promise例子：生成一个0-2之间的随机数，如果小于1，则等待一段时间后返回成功，否则返回失败： function test(resolve, reject) { var timeOut = Math.random() * 2; log(&apos;set timeout to: &apos; + timeOut + &apos; seconds.&apos;); setTimeout(function () { if (timeOut &lt; 1) { log(&apos;call resolve()...&apos;); resolve(&apos;200 OK&apos;); } else { log(&apos;call reject()...&apos;); reject(&apos;timeout in &apos; + timeOut + &apos; seconds.&apos;); } }, timeOut * 1000); } 这个test()函数有两个参数，这两个参数都是函数，如果执行成功，我们将调用resolve(‘200 OK’)，如果执行失败，我们将调用reject(‘timeout in ‘ + timeOut + ‘ seconds.’)。可以看出，test()函数只关心自身的逻辑，并不关心具体的resolve和reject将如何处理结果。 有了执行函数，我们就可以用一个Promise对象来执行它，并在将来某个时刻获得成功或失败的结果： var p1 = new Promise(test); var p2 = p1.then(function (result) { console.log(&apos;成功：&apos; + result); }); var p3 = p2.catch(function (reason) { console.log(&apos;失败：&apos; + reason); }); 变量p1是一个Promise对象，它负责执行test函数。由于test函数在内部是异步执行的，当test函数执行成功时，我们告诉Promise对象： // 如果成功，执行这个函数： p1.then(function (result) { console.log(&apos;成功：&apos; + result); }); 当test函数执行失败时，我们告诉Promise对象： p2.catch(function (reason) { console.log(&apos;失败：&apos; + reason); }); Promise对象可以串联起来，所以上述代码可以简化为： new Promise(test).then(function (result) { console.log(&apos;成功：&apos; + result); }).catch(function (reason) { console.log(&apos;失败：&apos; + reason); }); 可见Promise最大的好处是在异步执行的流程中，把执行代码和处理结果的代码清晰地分离了： Promise还可以做更多的事情，比如，有若干个异步任务，需要先做任务1，如果成功后再做任务2，任何任务失败则不再继续并执行错误处理函数。 要串行执行这样的异步任务，不用Promise需要写一层一层的嵌套代码。有了Promise，我们只需要简单地写： job1.then(job2).then(job3).catch(handleError); 其中，job1、job2和job3都是Promise对象。]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git使用]]></title>
    <url>%2F2018%2F07%2F12%2FGit%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[转载自https://blog.csdn.net/zwhfyy/article/details/8625228，如有侵权，请联系删除 出错信息Your local changes to the following files would be overwritten by mergeerror: Your local changes to the following files would be overwritten by merge: 123.txtPlease, commit your changes or stash them before you can merge. 如果希望保留生产服务器上所做的改动,仅仅并入新配置项, 处理方法如下: git stash git pull git stash pop 然后可以使用git diff -w +文件名 来确认代码自动合并的情况. 反过来,如果希望用代码库中的文件完全覆盖本地工作版本. 方法如下: git reset --hard git pull 其中git reset是针对版本,如果想针对文件回退本地修改,使用]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人脸识别算法发展情况]]></title>
    <url>%2F2018%2F07%2F10%2F%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%AE%97%E6%B3%95%E5%8F%91%E5%B1%95%E6%83%85%E5%86%B5%2F</url>
    <content type="text"><![CDATA[转载自https://zhuanlan.zhihu.com/p/36416906，如有侵权，请联系删除。 人脸识别概述人脸识别的目标是确定一张人脸图像的身份，即这个人是谁，这是机器学习和模式识别中的分类问题。它主要应用在身份识别和身份验证中。 人脸识别系统的组成人脸检测（Face Detection） 人脸对齐（Face Alignment） 人脸特征表征（Feature Representation） 人脸检测 人脸检测用于确定人脸在图像中的大小和位置，即解决“人脸在哪里”的问题，把真正的人脸区域从图像中裁剪出来，便于后续的人脸特征分析和识别。 人脸对齐 同一个人在不同的图像序列中可能呈现出不同的姿态和表情，这种情况是不利于人脸识别的。所以有必要将人脸图像都变换到一个统一的角度和姿态，这就是人脸对齐。它的原理是找到人脸的若干个关键点（基准点，如眼角，鼻尖，嘴角等），然后利用这些对应的关键点通过相似变换（Similarity Transform，旋转、缩放和平移）将人脸尽可能变换到标准人脸。 人脸特征表征 第三个模块是本文重点要讲的人脸识别算法，它接受的输入是标准化的人脸图像，通过特征建模得到向量化的人脸特征，最后通过分类器判别得到识别的结果。这里的关键是怎样得到对不同人脸有区分度的特征，通常我们在识别一个人时会看它的眉形、脸轮廓、鼻子形状、眼睛的类型等，人脸识别算法引擎要通过练习（训练）得到类似这样的有区分度的特征。本系列文章主要围绕人脸识别中的人脸特征表征进行展开，人脸检测和人脸对齐方法会在其它专题系列文章中进行介绍。 人脸识别算法的三个阶段人脸识别算法经历了早期算法，人工特征+分类器，深度学习3个阶段。目前深度学习算法是主流，极大的提高了人脸识别的精度。 早期算法 早期的算法有基于几何特征的算法，基于模板匹配的算法，子空间算法等多种类型。子空间算法将人脸图像当成一个高维的向量，将向量投影到低维空间中，投影之后得到的低维向量达到对不同的人具有良好的区分度。 子空间算法的典型代表是PCA（主成分分析，也称为特征脸EigenFace）[1]和LDA（线性判别分析，FisherFace）[2]。PCA的核心思想是在进行投影之后尽量多的保留原始数据的主要信息，降低数据的冗余信息，以利于后续的识别。LDA的核心思想是最大化类间差异，最小化类内差异，即保证同一个人的不同人脸图像在投影之后聚集在一起，不同人的人脸图像在投影之后被用一个大的间距分开。PCA和LDA最后都归结于求解矩阵的特征值和特征向量，这有成熟的数值算法可以实现。 PCA和LDA都是线性降维技术，但人脸在高维空间中的分布显然是非线性的，因此可以使用非线性降维算法，典型的代表是流形学习[3]和核（kernel）技术。流形学习假设向量点在高维空间中的分布具有某些几何形状，然后在保持这些几何形状约束的前提下将向量投影到低维空间中，这种投影是通过非线性变换完成的。 人工特征 + 分类器 第二阶段的人脸识别算法普遍采用了人工特征 + 分类器的思路。分类器有成熟的方案，如神经网络，支持向量机[7]，贝叶斯[8]等。这里的关键是人工特征的设计，它要能有效的区分不同的人。 描述图像的很多特征都先后被用于人脸识别问题，包括HOG、SIFT、Gabor、LBP等。它们中的典型代表是LBP（局部二值模式）特征[9]，这种特征简单却有效。LBP特征计算起来非常简单，部分解决了光照敏感问题，但还是存在姿态和表情的问题。 联合贝叶斯是对贝叶斯人脸的改进方法[8]，选用LBP和LE作为基础特征，将人脸图像的差异表示为相同人因姿态、表情等导致的差异以及不同人间的差异两个因素，用潜在变量组成的协方差，建立两张人脸的关联。文章的创新点在于将两个人脸表示进行联合建模，在人脸联合建模的时候，又使用了人脸的先验知识，将两张人脸的建模问题变为单张人脸图片的统计计算，更好的验证人脸的相关性，该方法在LFW上取得了92.4%的准确率。 人工特征的巅峰之作是出自CVPR 2013年MSRA的”Blessing of Dimisionality: High Dimensional Feature and Its Efficient Compression for Face Verification” [10]，一篇关于如何使用高维度特征在人脸验证中的文章，作者主要以LBP（Local Binary Pattern，局部二值特征）为例子，论述了高维特征和验证性能存在着正相关的关系，即人脸维度越高，验证的准确度就越高。 深度学习 第三个阶段是基于深度学习的方法，自2012年深度学习在ILSVRC-2012大放异彩后，很多研究者都在尝试将其应用在自己的方向，这极大的推动了深度学习的发展。卷积神经网络在图像分类中显示出了巨大的威力，通过学习得到的卷积核明显优于人工设计的特征+分类器的方案。在人脸识别的研究者利用卷积神经网络（CNN）对海量的人脸图片进行学习，然后对输入图像提取出对区分不同人的脸有用的特征向量，替代人工设计的特征。 在前期，研究人员在网络结构、输入数据的设计等方面尝试了各种方案，然后送入卷积神经网络进行经典的目标分类模型训练；在后期，主要的改进集中在损失函数上，即迫使卷积网络学习得到对分辨不同的人更有效的特征，这时候人脸识别领域彻底被深度学习改造了！ DeepFace[11]是CVPR2014上由Facebook提出的方法，是深度卷积神经网络在人脸识别领域的奠基之作，文中使用了3D模型来做人脸对齐任务，深度卷积神经网络针对对齐后的人脸Patch进行多类的分类学习，使用的是经典的交叉熵损失函数（Softmax）进行问题优化，最后通过特征嵌入（Feature Embedding）得到固定长度的人脸特征向量。Backbone网络使用了多层局部卷积结构（Local Convolution），原因是希望网络的不同卷积核能学习人脸不同区域的特征，但会导致参数量增大，要求数据量很大，回过头去看该策略并不是十分必要。 DeepFace在LFW上取得了97.35%的准确率，已经接近了人类的水平。之后Google推出FaceNet（Facenet论文地址），使用三元组损失函数(Triplet Loss)代替常用的Softmax交叉熵损失函数，在一个超球空间上进行优化使类内距离更紧凑，类间距离更远，最后得到了一个紧凑的128维人脸特征，其网络使用GoogLeNet的Inception模型，模型参数量较小，精度更高，在LFW上取得了99.63%的准确率，这种损失函数的思想也可以追溯到早期的LDA算法。 CVPR2014、CVPR2015香港中文大学汤晓鸥团队提出的DeepID系列是一组非常有代表性的工作，其中DeepID1[12]使用四层卷积，最后一层为Softmax，中间为Deep Hidden Identity Features，是学习到的人脸特征表示，并使用Multi-patch分别训练模型最后组合成高维特征，人脸验证阶段使用联合贝叶斯的方法；通过学习一个多类（10000类，每个类大约有20个实例）人脸识别任务来学习特征，文中指出，随着训练时要预测的人脸类越多，DeepID的泛化能力就越强。 人脸识别算法仓库 ageitgey/face_recognition: https://github.com/ageitgey/face_recognition davidsandberg/facenet: https://github.com/davidsandberg/facenet cmusatyalab/openface: https://github.com/cmusatyalab/openface kpzhang93/MTCNN_face_detection_alignment(人脸检测): https://github.com/kpzhang93/MTCNN_face_detection_alignment deepinsight/insightface: https://github.com/deepinsight/insightface nyoki-mtl/keras-facenet: https://github.com/nyoki-mtl/keras-facenet yuyang-huang/keras-inception-resnet-v2(网络结构): https://github.com/yuyang-huang/keras-inception-resnet-v2 yobibyte/yobiface: https://github.com/yobibyte/yobiface/tree/master/src 相关博客 应用一个基于Python的开源人脸识别库，face_recognition:https://blog.csdn.net/hongbin_xu/article/details/76284134 TensorFlow–实现人脸识别实验精讲 （Face Recognition using Tensorflow）:https://blog.csdn.net/niutianzhuang/article/details/79191167 基于卷积神经网络和tensorflow实现的人脸识别:https://blog.csdn.net/hy13684802853/article/details/79780805 keras/构建卷积神经网络人脸识别: https://blog.csdn.net/szj_huhu/article/details/75202254 人脸识别–(opencv、dlib、keras-TensorFlow）:https://blog.csdn.net/u014258362/article/details/80688224 TensorFlow实现人脸识别(5)——-利用训练好的模型实时进行人脸检测:https://blog.csdn.net/yunge812/article/details/79447584 基于keras的人脸识别:https://blog.csdn.net/Julymycin/article/details/79182222 史上最全的FaceNet源码使用方法和讲解（一）（附预训练模型下载）:https://blog.csdn.net/u013044310/article/details/79556099https://github.com/boyliwensheng/understand_facenet(作者整理代码) 基于 MTCNN/TensorFlow 实现人脸检测:https://blog.csdn.net/Mr_EvanChen/article/details/77650883 计算机视觉实时目标检测 TensorFlow Object Detection APIhttps://blog.csdn.net/chenhaifeng2016/article/details/74205717]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>人脸识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nodejs与Django的跨域问题]]></title>
    <url>%2F2018%2F07%2F08%2FNodejs%E4%B8%8EDjango%E7%9A%84%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Nodejs与Django的跨域问题由于采用前后端分离的编程方式，Django的csrf_token验证失效，出现跨域问题，在此记录一下解决方法。 1 安装django-cors-headers pip install django-cors-headers 2 配置settings.py文件 OK！问题解决！]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>Django</tag>
        <tag>Nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django使用MongoDB数据库]]></title>
    <url>%2F2018%2F07%2F08%2FDjango%E4%BD%BF%E7%94%A8MongoDB%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[之前学习使用Django搭建在线教育平台使用的是Mysql数据库，现在考虑到公司以后的发展及当前技术需求，更换为MongoDB数据库。在此记录一下更改操作： 1 首先安装mongoengine，并在setting中设置对应的位置 pip instal mongoengine 2 设置默认的数据库信息 3 设置Model 总结之前因为使用Django自带的admin后台管理系统，所以在像Mysql一样迁移数据库时出现错误。后来分析发现，目前使用Django所做的工作不需要用到后台管理系统，仅仅是作为一个后台服务，因此可直接运行。至此，设置完成。]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7为firewalld添加开放端口及相关操作]]></title>
    <url>%2F2018%2F07%2F05%2FCentOS7%E4%B8%BAfirewalld%E6%B7%BB%E5%8A%A0%E5%BC%80%E6%94%BE%E7%AB%AF%E5%8F%A3%E5%8F%8A%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[firewalld的基本使用启动： systemctl start firewalld 查看状态： systemctl status firewalld 停止： systemctl disable firewalld 禁用： systemctl stop firewalld systemctl是CentOS7的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。启动一个服务：systemctl start firewalld.service 关闭一个服务：systemctlstop firewalld.service 重启一个服务：systemctlrestart firewalld.service 显示一个服务的状态：systemctlstatus firewalld.service 在开机时启用一个服务：systemctlenable firewalld.service 在开机时禁用一个服务：systemctldisable firewalld.service 查看服务是否开机启动：systemctlis-enabled firewalld.service 查看已启动的服务列表：systemctllist-unit-files|grep enabled 查看启动失败的服务列表：systemctl--failed 配置firewalld-cmd查看版本： firewall-cmd --version 查看帮助： firewall-cmd --help 显示状态： firewall-cmd --state 查看所有打开的端口： firewall-cmd--zone=public --list-ports 更新防火墙规则： firewall-cmd --reload 查看区域信息: firewall-cmd--get-active-zones 查看指定接口所属区域： firewall-cmd--get-zone-of-interface=eth0 拒绝所有包：firewall-cmd --panic-on 取消拒绝状态： firewall-cmd --panic-off 查看是否拒绝： firewall-cmd --query-panic 添加firewall-cmd --zone=public --add-port=80/tcp --permanent （--permanent永久生效，没有此参数重启后失效） 重新载入firewall-cmd --reload 查看firewall-cmd --zone=public --query-port=80/tcp 删除firewall-cmd --zone=public --remove-port=80/tcp --permanent 查看firewall是否运行,下面两个命令都可以systemctl status firewalld.service firewall-cmd --state 查看当前开了哪些端口其实一个服务对应一个端口，每个服务对应/usr/lib/firewalld/services下面一个xml文件。 firewall-cmd --list-services 查看还有哪些服务可以打开firewall-cmd --get-services 查看所有打开的端口：firewall-cmd --zone=public --list-ports 更新防火墙规则：firewall-cmd --reload]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>firewalld</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nodejs与Django图片信息传输]]></title>
    <url>%2F2018%2F07%2F05%2FNodejs%E4%B8%8EDjango%E5%9B%BE%E7%89%87%E4%BF%A1%E6%81%AF%E4%BC%A0%E8%BE%93%2F</url>
    <content type="text"><![CDATA[Nodejs与Django图片信息传输由于公司需要Nodejs的前端与Django的后端进行交互，其中涉及到图片信息作为二进制流传输，在此记录前后端分离中二进制图片在Django中的保存与转换。 Nodejs中的数据传输Nodejs采用Input插件读取图片 其中涉及到公司大牛写的Webship框架，但传送数据的方式没有大的改变 Django保存传输的二进制图片首先，我们分析request请求中所包含的信息： 通过分析，发现request中FILES属性是前端发给我们的包含图片信息的内容，我们取出FILES属性中的内容赋值给files，再进行分析，看我们需要的图片究竟是什么样的内容和格式，内容如下： 可以发现，files变量中包含name属性，即我提交的图片名字，还有一个file属性，其是一个bytes格式的变量入口，这个可能就是我需要的二进制图片，经过测试，读取这个file属性得到的二进制流和我以‘rb’模式read()提交的同一个图片所得到的二进制流相等。至此，就找到了requst中所包含的图片信息，然后将其保存到指定路径中： 完成！]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>Django</tag>
        <tag>Nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django文档——Model字段类型]]></title>
    <url>%2F2018%2F06%2F14%2FDjango%E6%96%87%E6%A1%A3%E2%80%94%E2%80%94Model%E5%AD%97%E6%AE%B5%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[字段类型(Field types)AutoField它是一个根据 ID 自增长的 IntegerField 字段。通常，你不必直接使用该字段。如果你没在别的字段上指定主 键，Django 就会自动添加主键字段。 BigIntegerField64位整数，类似于IntegerField，范围从-9223372036854775808 到9223372036854775807。默认的form widget 是TextInput。 BooleanField一个布尔值(true/false)字段。 默认的form widget是CheckboxInput。 如果要使用null作为空值，可使用NullBooleanField。 CharFieldclass CharField(max_length=None[, **options]) 它是一个字符串字段，对小字符串和大字符串都适用。 对于更大的文本，应该使用TextField 。 默认的form widget是TextInput。 CharField 有一个必须传入的参数：max_length,字段的最大字符数。它作用于数据库层级和 Django 的数据验证层级。 CommaSeparatedInterFieldclass CommaSeparatedIntegerField(max_length=None[, **options]) 它用来存放以逗号间隔的整数序列。和 CharField 一样，必须为它提供 max_length 参数。而且要注意不同数据库对 max_length 的限制。 DateFieldclass DateField([auto_now=False, auto_now_add=False, **options]) 该字段利用 Python 的 datetime.date 实例来表示日期。下面是它额外的可选参数： DateField.auto_now：每一次保存对象时，Django 都会自动将该字段的值设置为当前时间。一般用来表示 “最后修改” 时间。要注意使用的是当前日期，而并非默认值，所以 不能通过重写默认值的办法来改变保存时间。 DateField.auto_now_add：在第一次创建对象时，Django 自动将该字段的值设置为当前时间，一般用来表示对象创建时间。它使用的同样是当前日期，而非默认值。 默认的form widget是TextInput。 Note:当auto_now或者auto_now_add设置为True时，字段会有editable=True和blank=True的设定。 DateTimeFieldclass DateTimeField([auto_now=False, auto_now_add=False, **options]) 该字段利用 datetime.datetime 实例表示日期和时间。该字段所按受的参数和 DateField 一样。 默认的form widget是TextInput。Django 的admin使用两个带有 JavaScript 快捷选项TextInput分别表示日期和时间。 DecimalFieldclass DecimalField(max_digits=None, decimal_places=None[, **options]) 它是使用 Decimal 实例表示固定精度的十进制数的字段。它有两个必须的参数： DecimalField.max_digits：数字允许的最大位数 DecimalField.decimal_places：小数的最大位数 例如，要存储的数字最大值是999，而带有两个小数位，你可以使用： 1models.DecimalField(…, max_digits=5, decimal_places=2)要存储大约是十亿级且带有10个小数位的数字，就这样写： 1models.DecimalField(…, max_digits=19, decimal_places=10)默认的form widget是TextInput。 EmailFieldclass EmailField([max_length=75, **options]) 它是带有 email 合法性检测的A CharField 。 Note：最大长度默认为75，并不能存储所有与RFC3696/5321兼容的email地址。如果要存储所有，请设置 max_length=254。设置为75是历史遗留问题。 FileFieldclass FileField(upload_to=None[, max_length=100, **options]) 文件上传字段 Note：该字段不支持 primary_key 和 unique 参数，否则会抛出 TypeError 异常。 它有一个必须的参数： FileField.upload_to 用于保存文件的本地文件系统。它根据 MEDIA_ROOT 设置确定该文件的 url 属性。 该路径可以包含 时间格式串strftime()，可以在上传文件的时候替换成当时日期／时间(这样，就不会出现在上传文件把某个目录塞满的情况了)。 该参数也可以是一个可调用项，比如是一个函数，可以调用函数获得包含文件名的上传路径。这个可调用项必须要接受两个参数， 并且返回一个保存文件用的 Unix-Style 的路径(用 / 斜杠)。两个参数分别是： instance ：定义了当前 FileField 的 model 实例。更准确地说，就是以该文件为附件的 model 实例。 大多数情况下，在保存该文件时， model 实例对象还并没有保存到数据库，这是因为它很有可能使用默认的 AutoField，而此时它还没有从数据库中获得主键值。 filename ：上传文件的原始名称。在生成最终路径的时候，有可能会用到它。 还有一个可选的参数： FileField.storage 负责保存和获取文件的对象。 默认的form widget是FileInput。 Note：在 model 中使用 FileField 或 ImageField 要按照以下的步骤： 1.在项目settings文件中，你要定义 MEDIA_ROOT ，将它的值设为用来存放上传文件的目录的完整路径。(基于性能的考虑，Django 没有将文件保存在数据库中). 然后定义 MEDIA_URL ，将它的值设为表示该目录的网址。 要确保 web 服务器所用的帐号拥有对该目录的写权限。 2.在 model 里面添加 FileField 或 ImageField ，并且确认已定义了 upload_to 项，让 Django 知道应该用 MEDIA_ROOT 的哪个子目录来保存文件。 3.存储在数据库当中的仅仅只是文件的路径(而且是相对于 MEDIA_ROOT 的相对路径)。你可能已经想到利用 Django 提供的 url 这个方便的属性。举个例子，如果你的 ImageField 名称是 mug_shot，那么你可以在模板 中使用 1就能得到图片的完整网址。 例如，假设你的 MEDIA_ROOT 被设为 ‘/home/media’，upload_to 被设为 ‘photos/%Y/%m/%d’。 upload_to 中 的 ‘%Y/%m/%d’ 是一个strftime()， ‘%Y’ 是四位的年份，’%m’ 是两位的月份， ‘%d’ 是两位的日子。如果你 在2007年01月15号上传了一个文件，那么这个文件就保存在 /home/media/photos/2007/01/15 目录下。 如果你想得到上传文件的本地文件名称，文件网址，或是文件的大小，你可以使用 name, url 和 size 属性。 Note：在上传文件时，要警惕保存文件的位置和文件的类型，这么做的原因是为了避免安全漏洞。对每一个上传 文件都要验证，这样你才能确保上传的文件是你想要的文件。举个例子，如果你盲目地让别人上传文件，而没有 对上传文件进行验证，如果保存文件的目录处于 web 服务器的根目录下，万一有人上传了一个 CGI 或是 PHP 脚本，然后通过访问脚本网址来运行上传的脚本，那可就太危险了。千万不要让这样的事情发生！ 默认情况下，FileField 实例在数据库中的对应列是 varchar(100) ，和其他字段一样，你可以利用max_length 参数改变字段的最大长度。 FileField and FieldFile class FieldFile 当你访问一个Model的FileField字段时，会得到一个FieldFile的实例作为代理去访问底层文件。实例有几种属性和方法可以用来和文件数据进行互动。 FieldFile.url 通过只读的方式调用底层存储(Storage)类的 url() 方法，来访问该文件的相对URL。 FieldFile.open(mode=’rb’) 类似于python的open()方法。 FieldFile.close() 类似于python的close()方法。 FieldFile.save(name,content,save=True) 这种方法将filename和文件内容传递到该字段然后存储到该模型。该方法需要两个必须的参数：name， 文件的名称， content， 包含文件内容的对象。save 参数是可选的，主 要是控制文件修改后实例是否保存。默认是 True 。需要注意的是，content 参数是 django.core.files.File 的一个实例，不是Python的内置File对象。你可以使 用他从现有的Python文件对象中构建一个文件，如下所示： 1234from django.core.files import File Open an existing file using Python’s built-in open()f = open(‘/tmp/hello.world’)myfile = File(f)或者从字符串中构造： 12from django.core.files.base import ContentFilemyfile = ContentFile(“hello world”)FieldFile.delete(save=True) 删除此实例相关的文件，并清除该字段的所有属性。 Note：当delete()被调用时，如果文件正好是打开的，该方法将关闭文件。 save 参数是可选的，控制文件删除后实例是否保存。默认是 True 。 需要注意的是，当一个模型被删除时，相关文件不被删除。如果想删除这些孤立的文件，需要自己去处理（比如，可以手动运行命令清理，也可以通过cron来定期执行清理命令） FilePathFieldclass FilePathField(path=None[, match=None, recursive=False, max_length=100, **options]) 它是一个 CharField ，它用来选择文件系统下某个目录里面的某些文件。它有三个专有的参数，只有第一个参 数是必须的： FilePathField.path 这个参数是必需的。它是一个目录的绝对路径，而这个目录就是 FilePathField 用来选择文件的那个目录。比 如： “/home/images”. FilePathField.match 可选参数。它是一个正则表达式字符串， FilePathField 用它来过滤文件名称，只有符合条件的文件才出现在 文件选择列表中。要注意正则表达式只匹配文件名，而不是匹配文件路径。例如：”foo.*.txt$” 只匹配名为 foo23.txt 而不匹配 bar.txt 和 foo23.gif。 FilePathField.recursive 可选参数。它的值是 True 或 False。默认值是 False。它指定是否包含 path 下的子目录。 FilePathField.allow_files 该项属于Django1.5新增内容。可选参数，它的值是 True 或 False。默认值是 True。它指定是否包含指定位置的文件。该项与allow_folders 必须有一个是 True。 FilePathField.allow_folders Django1.5新增内容。可选参数，它的值是True或False。默认是False。它指定是否包含指定位置的目录。该项与allow_files必须有一个是 True。 前面已经提到了 match 只匹配文件名称，而不是文件路径。所以下面这个例子： 1FilePathField(path=”/home/images”, match=”foo.*”, recursive=True)将匹配 /home/images/foo.gif ，而不匹配 /home/images/foo/bar.gif。这是因为 match 只匹配文件名(foo.gif 和 bar.gif). 默认情况下， FilePathField 实例在数据库中的对应列是varchar(100) 。和其他字段一样，你可以利用 max_length 参数改变字段的最大长度。 FloatFieldclass FloatField([**options]) 该字段在 Python 中使用float 实例来表示一个浮点数。 默认的form widget是TextInput。 请注意FloatField与DecimalField的区别。 ImageFieldclass ImageField(upload_to=None[, height_field=None, width_field=None, max_length=100,**options]) 和 FileField 一样，只是会验证上传对象是不是一个合法的图象文件。 除了那些在 FileField 中有效的参数之外， ImageField 还可以使用 File.height and File.width 两个属性 。 它有两个可选参数： ImageField.height_field 保存图片高度的字段名称。在保存对象时，会根据该字段设定的高度，对图片文件进行缩放转换。 ImageField.width_field 保存图片宽度的字段名称。在保存对象时，会根据该字段设定的宽度，对图片文件进行缩放转换。 默认情况下， ImageField 实例对应着数据库中的varchar(100) 列。和其他字段一样，你可以使 用 max_length 参数来改变字段的最大长度。 IntegerFieldclass IntegerField([**options]) 整数字段。默认的form widget是TextInput。 IPAddressFieldclass IPAddressField([**options]) 以字符串形式(比如 “192.0.2.30”)表示 IP 地址字段。默认的form widget是TextInput。 GenericIPAddressFieldclass GenericIPAddressField([**options]) Django1.4新增。 以字符串形式(比如 “192.0.2.30”或者”2a02:42fe::4”)表示 IP4或者IP6 地址字段。默认的form widget是TextInput。 IPv6的地址格式遵循RFC 4291 section 2.2。比如如果这个地址实际上是IPv4的地址，后32位可以用10进制数表示，例如 “::ffff:192.0.2.0”。 2001:0::0:01可以写成2001::1,而::ffff:0a0a:0a0a可以写成::ffff:10.10.10.10。字母都为小写。 GenericIPAddressField.protocol 验证输入协议的有效性。默认值是 ‘both’ 也就是IPv4或者IPv6。该项不区分大小写。 GenericIPAddressField.unpack_ipv4 解释IPv4映射的地址，像 ::ffff:192.0.2.1 。如果启用该选项，该地址将必解释为 192.0.2.1 。默认是禁止的。只有当 protocol 被设置为 ‘both’ 时才可以启用。 NullBooleanFieldclass NullBooleanField([**options]) 与 BooleanField 相似，但多了一个 NULL 选项。建议用该字段代替使用 null=True 选项的 BooleanField 。 默认的form widget是NullBooleanSelect。 PositiveIntegerFieldclass PositiveIntegerField([**options]) 和 IntegerField 相似，但字段值必须是非负数。 PositiveSmallIntegerFieldclass PositiveSmallIntegerField([**options]) 和 PositiveIntegerField 类似，但数值的取值范围较小，受限于数据库设置。 SlugFieldclass SlugField([max_length=50, **options]) Slug 是一个新闻术语，是指某个事件的短标签。它只能由字母，数字，下划线或连字符组成。通赏情况下，它被用做网址的一部分。 和 CharField 类似，你可以指定 max_length (要注意数据库兼容性和本节提到的 max_length )。如果没有指定 max_length ，Django 会默认字段长度为50。 该字段会自动设置 Field.db_index to True。 基于其他字段的值来自动填充 Slug 字段是很有用的。你可以在 Django 的管理后台中使用prepopulated_fields 来做到这一点。 SmallIntegerFieldclass SmallIntegerField([**options]) 和 IntegerField 类似，但数值的取值范围较小，受限于数据库的限制。 TextFieldclass TextField([**options]) 大文本字段。默认的form widget是Textarea。 TimeFieldclass TimeField([auto_now=False, auto_now_add=False, **options]) 该字段使用 Python 的 datetime.time 实例来表示时间。它和 DateField 接受同样的自动填充的参数。 默认的form widget是TextInput。 URLFieldclass URLField([max_length=200, **options]) 保存 URL 的 CharField 。 和所有 CharField 子类一样，URLField 接受可选的 max_length 参数，该参数默认值是200。]]></content>
  </entry>
  <entry>
    <title><![CDATA[点滴积累]]></title>
    <url>%2F2018%2F06%2F11%2F%E7%82%B9%E6%BB%B4%E7%A7%AF%E7%B4%AF%2F</url>
    <content type="text"><![CDATA[hexo生成博文插入图片 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true 在你的hexo目录下执行这样一句话npm install hexo-asset-image –save 等待一小段时间后，再运行hexo n “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹 最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片： ! [你想输入的替代文字](xxxx/图片名.jpg) 注意： xxxx是这个md文件的名字，也是同名文件夹的名字。只需要有文件夹名字即可，不需要有什么绝对路径。你想引入的图片就只需要放入xxxx这个文件夹内就好了，很像引用相对路径。 Error: That port is already in use.的错误。即端口号已经被占用,说明servr已经在运行了(也有可能在后台运行) 那么找到该进程,kill掉即可. 或者最简单的解决方法就是： 在终端输入 sudo fuser -k 8000/tcp 这样和端口8000相关的进程就都关了。 Centos下实现word转pdflibreoffice –headless –invisible –convert-to pdf 模版123.docx –outdir /filepath 爆破大数据平台Nodejs后端 1 创建数据库 1 使用redis、mongodb 2 使用Mysql 其中有DATATIME属性 2 使用Admzip以及正则表达式实现文档的替换 3 将生成的文档转换为PDF Django后端 1 设计Model 2 前后端分离传递数据 3 算法的嵌入 涉及算法 1 图像识别 2 自动布孔 Github学习资源 https://morvanzhou.github.io/（莫烦） https://cn.wordpress.org/（博客主题） https://blog.evjang.com http://bamos.github.io/ GAN生成对抗网络 GAN多种网络分析http://nooverfit.com/wp/%E7%8B%AC%E5%AE%B6%EF%BD%9Cgan%E5%A4%A7%E7%9B%98%E7%82%B9%EF%BC%8C%E8%81%8A%E8%81%8A%E8%BF%99%E4%BA%9B%E5%B9%B4%E7%9A%84%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-lsgan-wgan-cgan-info/ DCGAN代码：https://github.com/carpedm20/DCGAN-tensorflow tensorflowhttps://github.com/jacobgil/keras-dcgan keras 论文资料：https://github.com/zhangqianhui/AdversarialNetsPapers DCGAN、WGAN、WGAN-GP、LSGAN、BEGAN原理总结及对比：https://blog.csdn.net/qq_25737169/article/details/78857788 WGAN-GP：https://github.com/caogang/wgan-gphttps://github.com/tjwei/GANotebookshttps://github.com/jalola/improved-wgan-pytorchhttps://blog.csdn.net/omnispace/article/details/54942668(博客介绍) BEGAN全称是Boundary Equilibrium GANs：https://github.com/carpedm20/BEGAN-tensorflowhttps://github.com/Heumi/BEGAN-tensorflowhttps://github.com/carpedm20/BEGAN-pytorch Keras implementation of Image OutPainting：https://github.com/bendangnuksung/Image-OutPainting WGAN-GP与WGAN及GAN的比较：https://blog.csdn.net/qq_38826019/article/details/80786061 待查询问题 基于动量的优化算法（包括momentum和Adam） RMSProp的基本概念]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>日常记录</tag>
        <tag>小技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习——EM算法]]></title>
    <url>%2F2018%2F06%2F10%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94EM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[EM算法EM简介EM(Expectation Mmaximization) 是一种迭代算法， 用于含隐变量(Latent Variable) 的概率模型参数的极大似然估计， 或极大后验概率估计 EM算法由两步组成， 求期望的E步，和求极大的M步。EM算法可以看成是特殊情况下计算极大似然的一种算法。现实的数据经常有一些比较奇怪的问题，比如缺失数据、含有隐变量等问题。当这些问题出现的时候，计算极大似然函数通常是比较困难的，而EM算法可以解决这个问题。 EM算法已经有很多应用，比如最经典的Hidden Markov模型等。经济学中，除了逐渐开始受到重视的HMM模型（例如Yin and Zhao, 2015），其他领域也有可能涉及到EM算法，比如在Train的《Discrete Choice Methods with Simulation》就给出了一个 $mixed logit$ 模型的EM算法。 EM算法的预备知识 极大似然估计 1 举例说明：经典问题——学生身高问题 我们需要调查我们学校的男生和女生的身高分布。 假设你在校园里随便找了100个男生和100个女生。他们共200个人。将他们按照性别划分为两组，然后先统计抽样得到的100个男生的身高。假设他们的身高是服从高斯分布的。但是这个分布的均值u和方差∂2我们不知道，这两个参数就是我们要估计的。记作θ=[u, ∂]T。问题：我们知道样本所服从的概率分布的模型和一些样本，而不知道该模型中的参数。我们已知的有两个：（1）样本服从的分布模型（2）随机抽取的样本 需要通过极大似然估计求出的包括：模型的参数总的来说：极大似然估计就是用来估计模型参数的统计学方法。 2 如何估计 问题数学化： (1)样本集: x={$x_1,x_2,…,x_N$}, $N=100$。 (2)概率密度：$p(x_i|\theta)$ 抽到男生$i$（的身高）的概率 100个样本之间独立同分布，所以我同时抽到这100个男生的概率就是他们各自概率的乘积。就是从分布是$p(x|\theta)$ 的总体样本中抽取到这100个样本的概率，也就是样本集X中各个样本的联合概率，用下式表示：$$L(\theta)=L(x_1,…,x_n;\theta)=\prod_{i=1}^n{p(x_i|\theta)},\theta\in\phi$$这个概率反映了，在概率密度函数的参数是$\theta$时，得到X这组样本的概率。 需要找到一个参数θ，其对应的似然函数$L(\theta)$最大，也就是说抽到这100个男生（的身高）概率最大。这个叫做$\theta$的最大似然估计量，记为$$argmaxL(\theta)$$ 3 求最大似然函数估计值的一般步骤 首先，写出似然函数：$$L(\theta)=L(x_1,…,x_n;\theta)=\prod_{i=1}^n{p(x_i|\theta)},\theta\in\phi$$其次，对似然函数取对数，并整理：$$H(\theta)=lnL(\theta)=ln\prod_{i=1}^n{p(x_i|\theta)}=\sum_{i=1}^n{lnp(x_i|\theta)}$$然后，求导数，令导数为0，得到似然方程；最后，解似然方程，得到的参数即为所求。 4 总结 多数情况下我们是根据已知条件来推算结果，而极大似然估计是已经知道了结果，然后寻求使该结果出现的可能性最大的条件，以此作为估计值。]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>EM算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS环境搭建]]></title>
    <url>%2F2018%2F06%2F05%2FCentOS%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[系统安装电脑配置 微星1080Ti 至强E5 -2620v4 技嘉的主板 踩坑记录将CentOS 7.4镜像刻到U盘之后，向服务器安装时，使用U盘启动会出现两种启动选项，一种是UEFI启动选项，一种是默认的启动选项，如果不使用UEFI方式安装，那么一般是没有问题的，如果选择UEFI方式安装系统，那么引导系统时会出现如下的提示： [sdb] No Caching mode page found [sdb] Assuming drive cache:write through Could not boot /dev/root does not exist 然后命令行就卡在这了，现在只需要耐心等待，等一会之后会不断的滚动错误警告，这个时候继续等待，那么一会就会出来命令行输入界面，这个时候输入以下命令： ls /dev/sd* 输入命令之后会列出所有的存储设备，这个时候一般情况下第一块硬盘是sda，如果有多个分区，那么依次就是sda1、sda2等等，如果有两块硬盘那么就是sdb，U盘一般是排最后的号，如果有一块硬盘，那么U盘就是sdb，如果有两块硬盘，那么U盘就是sdc，U盘一般会有sdc和sdc4两个选项，sdc属于U盘存储，sdc4就是镜像所在分区了，这样一般是没有问题的，如果出现问题，那么接下来多配置几次就好了，接下来输入命令reboot重启计算机，在安装界面，先不要选择安装，这个时候按一下e键，会进入编辑界面，移动光标进行如下修改： 在第二行默认是：vmlinuz initrd=initrd.img inst.stage2=hd:LABEL=CentOS\x207\x20x86_64 rd.live.check quiet 把这行修改为：vmlinuz initrd=initrd.img inst.stage2=hd:/dev/sdc4:/ quiet 就是把hd:和quiet之间的内容修改为U盘镜像所在位置这样就可以了，注意要写成/dev/sdc4:/ 然后根据提示按Ctrl+X键就可以开始安装了，现在就正常进入安装界面了 NVIDIA驱动安装1、在官网上http://www.geforce.cn/drivers搜索到对应型号的显卡驱动并下载，下载到的驱动文件是一个后缀名为.run的文件（例如NVIDIA-Linux-x86_64-384.98.run）； 2、安装gcc编译环境以及内核相关的包： yum install kernel-devel kernel-doc kernel-headers gcc* glibc* glibc-*注意：安装内核包时需要先检查一下当前内核版本是否与所要安装的kernel-devel/kernel-doc/kernel-headers的版本一致，请务必保持两者版本一致，否则后续的编译过程会出问题。 3、禁用系统默认安装的 nouveau 驱动，修改/etc/modprobe.d/blacklist.conf 文件： 修改配置echo -e &quot;blacklist nouveau\noptions nouveau modeset=0&quot; &gt; /etc/modprobe.d/blacklist.conf 备份原来的镜像文件mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.bak 重建新镜像文件dracut /boot/initramfs-$(uname -r).img $(uname -r) 重启reboot 在命令行界面init 5 查看nouveau是否启动，如果结果为空即为禁用成功lsmod | grep nouveau 4、安装DKMS模块 DKMS全称是DynamicKernel ModuleSupport，它可以帮我们维护内核外的驱动程序，在内核版本变动之后可以自动重新生成新的模块。 sudo yum install DKMS 5、执行显卡驱动安装脚本（如果内核版本一致，就不需要指定–kernel-source-path和-k） ./NVIDIA-Linux-x86_64-384.98.run --kernel-source-path=/usr/src/kernels/3.10.0-693.11.1.el7.x86_64/ -k $(uname -r) --dkms -s 6、若步骤5执行过程中没报错，则安装成功。重启，执行nvidia-smi可查看相关信息。如若出现重启系统驱动找不到的情况，在装完驱动后，切记，先不要重启，使用 init5 和 init 3 交替切换，几次后，会进入图形界面（其中init 5为进入图形界面的命令），之后，在图形界面，重新编译一下启动项。 CUDA&amp;&amp;CUDNN 关于cuda和cudnn的安装，这一点尤其要注意。我在CentOS7.5(更新后的版本，初始装的时候为7.4)上安装CUDA9.2时，无法与NVIDIA的驱动匹配，因此退而求其次，选择了CUDA9.1，同时CUDNN选择7.0.5版本。注意 不要下载cudnn-9.1-linux-ppc64le-v7.1.tgz，因为ppc64le并不是针对X64的电脑系统CUDA的安装我选择了使用yum安装的方式，因为之前使用命令行，出现了未知的错误，导致系统重装。]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习的东西]]></title>
    <url>%2F2018%2F05%2F30%2F%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[function doDecrypt (pwd, onError) { console.log('in doDecrypt'); const txt = document.getElementById('enc_content').innerHTML; let plantext; try { const bytes = CryptoJS.AES.decrypt(txt, pwd); var plaintext = bytes.toString(CryptoJS.enc.Utf8); } catch(err) { if(onError) { onError(err); } return; } document.getElementById('enc_content').innerHTML = plaintext; document.getElementById('enc_content').style.display = 'block'; document.getElementById('enc_passwd').style.display = 'none'; if(typeof MathJax !== 'undefined') { MathJax.Hub.Queue( ['resetEquationNumbers', MathJax.InputJax.TeX], ['PreProcess', MathJax.Hub], ['Reprocess', MathJax.Hub] ); } } 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 var onError = function(error) { document.getElementById("enc_error").innerHTML = "password error!" }; function decrypt() { var passwd = document.getElementById("enc_pwd_input").value; console.log(passwd); doDecrypt(passwd, onError); }]]></content>
      <categories>
        <category>感悟</category>
      </categories>
      <tags>
        <tag>坚持</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django的网站逻辑]]></title>
    <url>%2F2018%2F05%2F29%2FDjango%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[Django实战——后台逻辑Django用户 （登录 注册 找回密码）登录 后台的操作放在对应app的views文件下，当我们在路由中添加一个url，django会自动为我们生成一个request，并添加到函数里面。首先判断请求方法，是POST还是GET。然后跳转到对应的页面进行操作。 对登录账户进行验证（采用类来做） 得到用户名和密码后，使用django.contrib.auth.authenticate进行验证，验证成功的话得到一个对象，然后进行对应后台逻辑的编写，即调用django.contrib.auth.login进行验证。 对登录成功后返回index.html文件的状态处理，需要在html文件中进行判断，用户是否登录，调用request.user.user.is_authenticated来进行判断。决定显示哪一行代码。 自定义认证方法，实现邮箱的登录方式（重定义方式） Seesion和Cookie机制 无状态请求 有状态请求 注册 准备工具 添加插件（captcha） 提交注册信息（包括注册码）发送邮件验证注册信息激活账户 找回密码 采用类似于激活账户的方式来实现]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Adm-zip工具]]></title>
    <url>%2F2018%2F05%2F29%2FAdm-zip%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[Adm-zip介绍 Adm-zip是JavaScript中的一款与压缩文件相关的插件，其功能相当的强大（我看来），我用其实现了对Word文档的内容替换。 Word 文档本质上是一个压缩 文件夹，其中的word文件夹下的document.xml文件是包含文档内容的文件，而我们需要操作的也正是这个文件。 Adm-zip这款插件则正好满足我们即对压缩文件内部条目文件的处理，同时又保证不影响压缩文件内部其余文件的要求。 我们需要的函数接口主要有四个，分别为： 读取压缩文件内指定目录里面的文件或者文件夹： Admzip.readAsText() 删除压缩文件内的指定文件或者文件夹： Admzip.deleteFile() 将指定文件写入到压缩文件夹中： Admzip.addFile() 将所做的更改重新写入文件（可以是当前文件，也可以重命名的word文档） Admzip.writeZip() 关于Adm-zip的使用方法，暂时只发现了这样一种，其还有别的Api接口，有兴趣的小伙伴可以自己再研究下^_^]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>Adm-zip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MARKDOWN语法熟悉]]></title>
    <url>%2F2018%2F05%2F28%2FMARKDOWN%E8%AF%AD%E6%B3%95%E7%86%9F%E6%82%89%2F</url>
    <content type="text"><![CDATA[基础标题测试一级标题二级标题三级标题四级标题五级标题六级标题换行和分段未换行未换行测试示例 换行后已换行测试（后有两个空格）示例 分段分段测试 分段 文本样式加粗(使用两个*号)斜体（使用一个号）*删除线(使用两个波浪线)‘底纹（单引号）’ 列表在Markdown 下，无序列表直接在文字前加 「 - 」 或者 「 * 」 即可，有序列表则直接在文字前加 「1.」「2.」「3.」 。符号要和文字之间加上一个字符的空格。 无序列表： 在文本前加 「 」 即可生成一个无序列表。快捷键：control + L （只能生成列表，不能生成子列表）在 「 」 前加两个空格键或者一个 tab 键就可以产生一个子列表。有序列表： 在文本前加 「字母.」 或 「数字.」 即可生成一个有序列表。注意，当你第一个序号使用什么作为标记的，那么同级别的列表就会自动使用其作为标记。 无序列表 1 2 2.1 2.1.1 2.1.1.1 3 有序列表 1 2 a. 2.1 b. 2.2 3 有序列表与无序列表混排 1 2 a. 2.1 b. 2.2* 2.2.1 引用只要在文本内容之前加 「 &gt; （大于号）」 即可将文本变成引用文本。 这是引用文本 图片与链接图片 链接Mou 水平线三个「 - 」或「 * 」都可以画出一条水平分割线 使用（—）的水平分割线 使用（***）的水平分割线 代码框两对「 123456代码前加四个空格键 代码前加一个 tab 键### 两对‘ ``` ’包裹```print(&apos;Hello Word!&apos;); 四个空格print(&apos;Hello Word!&apos;); 一个 tab 键print(&apos;Hello Word!&apos;); 脚注脚注总是成对出现的，「 [^1] 」作为标记，可以点击跳至末尾注解。「 [^1]: 」填写注解，不论写在什么位置，都会出现在文章的末尾。 点击右上方的小数字有注解$[^1]$ $[^1] :$这里是注解这是随机文本这是随机文本这是随机文本 注释注释是给自己看的，预览时也不会出现，当然发布出去别人也不会看见。 首行缩进关于首行缩进，网上争议很多，而技术本身并没有错，不是吗？在输入法的「全角」模式下，输入两个空格键即可。 引号在网页上写文章建议使用直角引号『「」』。 利用Markdown创建表格Markdown作为一种轻量级书写/写作语言，并没有提供很好的排版、编辑等功能。因此，如果想要利用Markdown创建表格（特别是复杂表格），其实是一项不太轻松的事情。经过笔者在简书平台上的测试与其他若干帖子的表述，Markdown应是只提供了最简单的创建表格与内容对齐方式的功能。总结而言，有如下两种最为直观的创建表格方式: 简单方式Name | Academy | score - | :-: | -: Harry Potter | Gryffindor| 90 Hermione Granger | Gryffindor | 100 Draco Malfoy | Slytherin | 90 Name Academy score Harry Potter Gryffindor 90 Hermione Granger Gryffindor 100 Draco Malfoy Slytherin 90 原生方式| Name | Academy | score | | - | :-: | -: | | Harry Potter | Gryffindor| 90 | | Hermione Granger | Gryffindor | 100 | | Draco Malfoy | Slytherin | 90 | Name Academy score Harry Potter Gryffindor 90 Hermione Granger Gryffindor 100 Draco Malfoy Slytherin 90 语法说明： 不管是哪种方式，第一行为表头，第二行分隔表头和主体部分，第三行开始每一行代表一个表格行； 列与列之间用管道符号 “|” 隔开，原生方式的表格每一行的两边也要有管道符。 可在第二行指定不同列单元格内容的对齐方式，默认为左对齐，在 “-” 右边加上 “:” 为右对齐，在 “-” 两侧同时加上 “:” 为居中对齐。 这样傻瓜的表格创建方式十分符合Markdown简小精悍的语言气质，具有上手快、即学即用的优势。但傻瓜的定义方式显然不能满足很多处女座的要求，比如文章——“Linux备忘录-Linux中文件/文件夹按照时间顺序升序/降序排列”的表格如下： | 参数 |详细解释|备注| | - | :-: | -: | | -l | use a long listing format |以长列表方式显示（显示出文件/文件夹详细信息） | | -t | sort by modification time |按照修改时间排序（默认最近被修改的文件/文件夹排在最前面） | |-r | reverse order while sorting |逆序排列| 参数 详细解释 备注 -l use a long listing format 以长列表方式显示（显示出文件/文件夹详细信息） -t sort by modification time 按照修改时间排序（默认最近被修改的文件/文件夹排在最前面） -r reverse order while sorting 逆序排列 单元格排列不齐整、第一列太窄而第三列略宽，如此不堪的视觉效果着实让强迫症患者们难以忍受。还好，利用HTML可以弥补Markdown这一缺陷，甚至可以在创建表格时其他诸多表现方面锦上添花。 Markdown 添加 MathJax 数学公式添加公式的方法行内公式 $行内公式$ 行间公式 $$行间公式$$ MathJax 数学公式语法呈现位置注意: 在公式的前一行和后一行，要注意空一行，否则公式会出错。 所有公式定义格式为 \$…$ 具体语句例如 \$\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$ 显示为： $\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$ 居中并放大显示 \$\$\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$$ 显示为： $$\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$$ 希腊字母 显示 命令 显示 命令 α \$\alpha\$ β \$\beta\$ γ \$\gamma\$ δ \$\delta\$ ϵ \$\epsilon\$ ζ \$\zeta\$ η \$\eta\$ θ \$\theta\$ ι \$\iota\$ κ \$\kappa\$ λ \$\lambda\$ μ \$\mu\$ ν \$\nu\$ ξ \$\xi\$ π \$\pi\$ ρ \$\rho\$ σ \$\sigma\$ τ \$\tau\$ υ \$\upsilon\$ ϕ \$\phi\$ χ \$\chi\$ ψ \$\psi\$ ω \$\omega\$ 如果需要大写的希腊字母，只需将命令的首字母大写即可(有的字母没有大写)，如 \$\gamma$ &amp; \$\Gamma$ $\gamma$ &amp; $\Gamma$ 若需要斜体希腊字母，在命令前加上var前缀即可(大写可斜)，如 \$\Gamma$ &amp; \$\varGamma$ $\Gamma$ &amp; $\varGamma$ 字母修饰上下标 上标：^ 下标：_ \$C_n^2$ $$C_n^2$$ 矢量 例1 \$\vec a$ $\vec a$ 例2 \$\overrightarrow a$ $\overrightarrow xy$ 字体 - Typewriter\$\mathtt {ABCDEFGHIJKLMNOPQRSTUVWXYZ}$ $\mathtt {ABCDEFGHIJKLMNOPQRSTUVWXYZ}$ \$\mathbb {ABCDEFGHIJKLMNOPQRSTUVWXYZ}$ $\mathbb {ABCDEFGHIJKLMNOPQRSTUVWXYZ}$ \$\mathsf {ABCDEFGHIJKLMNOPQRSTUVWXYZ}$ $\mathsf {ABCDEFGHIJKLMNOPQRSTUVWXYZ}$ 分组 - {}有分组功能，如\$10^{10}\$ \&amp; \$10^10\$$10^{10}$ &amp; $10^10$ 括号 小括号：\$()$呈现为 $()$ 中括号：\$[]$呈现为 $[]$ 尖括号：\$\langle\rangle$呈现为 $\langle\rangle$ - 此处为与分组符号{}相区别，使用转义字符\ 使用\left(或\right)使符号大小与邻近的公式相适应；该语句适用于所有括号类型 \$(\frac{x}{y})$呈现为 $(\frac{x}{y})$ 而\$\left(\frac{x}{y}\right)$呈现为 $\left(\frac{x}{y}\right)$ 注意: 在公式的前后，必须留有一个空格或者换行，否则无法识别。 求和、极限与积分求和：\sum 举例：\$\sum_{i=1}^n{a_i}$ $\sum_{i=1}^n{a_i}$ 极限：\$\lim_{x\to 0}$ $\lim_{x\to 0}$ 积分：\$\int$ $\int$ 举例：\$\int_0^\infty{fxdx}$ $\int_0^\infty{fxdx}$ \$\iint$ $\iint$ \$\iiint$ $\iiint$ 连乘：\$\prod$ $\prod$ 分式与根式分式(fractions)：\$\frac{公式1}{公式2}$ $\frac{公式1}{公式2}$ 根式：\$\sqrt[x]{y}$ $\sqrt[x]{y}$ 特殊符号 显示 命令 ∞ \$\infty$ ∪ \$\cup$ ∩ \$\cap$ ⊂ \$\subset$ ⊆ \$\subseteq$ ⊃ \$\supset$ ∈ \$\in$ ∉ \$\notin$ ∅ \$\varnothing$ ∀ \$\forall$ ∃ \$\exists$ ¬ \$\lnot$ ∇ \$\nabla$ ∂ \$\partial$ 空格 LaTeX语法本身会忽略空格的存在 小空格：\$a\ b$呈现为 $a\ b$ 4格空格：\$a\quad b$呈现为 $a\quad b$ 矩阵边框在起始、结束标记处用下列词替换matrix pmatrix：小括号边框 bmatrix：中括号边框 Bmatrix：大括号边框 vmatrix：单竖线边框 Vmatrix：双竖线边框 省略元素横省略号：\cdots竖省略号：\vdots斜省略号：\ddots举例 $$\begin{bmatrix}{a_{11}}&amp;{a_{12}}&amp;{\cdots}&amp;{a_{1n}}\\\ {a_{21}}&amp;{a_{22}}&amp;{\cdots}&amp;{a_{2n}}\\\ {\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\\ {a_{m1}}&amp;{a_{m2}}&amp;{\cdots}&amp;{a_{mn}}\\\ \end{bmatrix}$$ $$\begin{bmatrix}{a_{11}}&amp;{a_{12}}&amp;{\cdots}&amp;{a_{1n}}\\ {a_{21}}&amp;{a_{22}}&amp;{\cdots}&amp;{a_{2n}}\\ {\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\ {a_{m1}}&amp;{a_{m2}}&amp;{\cdots}&amp;{a_{mn}}\\ \end{bmatrix}$$ 阵列需要array环境：起始、结束处以{array}声明 对齐方式：在{array}后以{}逐行统一声明 左对齐：l；居中：c；右对齐：r 竖直线：在声明对齐方式时，插入|建立竖直线 插入水平线：\hline 方程组需要cases环境：起始、结束处以{cases}声明举例 $$\begin{cases} a_1x+b_1y+c_1z=d_1\\\ a_2x+b_2y+c_2z=d_2\\\ a_3x+b_3y+c_3z=d_3\\\ \end{cases}$$ $$\begin{cases} a_1x+b_1y+c_1z=d_1\\ a_2x+b_2y+c_2z=d_2\\ a_3x+b_3y+c_3z=d_3\\ \end{cases}$$]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具 Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一个博客！]]></title>
    <url>%2F2018%2F05%2F26%2F%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8D%9A%E5%AE%A2%EF%BC%81%2F</url>
    <content type="text"><![CDATA[第一个博客从这个博客开始，新的学习阶段开启了！]]></content>
      <categories>
        <category>记录</category>
      </categories>
      <tags>
        <tag>NoteBook</tag>
      </tags>
  </entry>
</search>
