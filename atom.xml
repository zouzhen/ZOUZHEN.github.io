<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZOUZHEN_BLOG</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-10-20T07:48:15.868Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ZOUZHEN</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>&#39;Pytorch--线性回归和逻辑回归&#39;</title>
    <link href="http://yoursite.com/2018/10/20/Pytorch-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2018/10/20/Pytorch-线性回归和逻辑回归/</id>
    <published>2018-10-20T05:53:49.000Z</published>
    <updated>2018-10-20T07:48:15.868Z</updated>
    
    <content type="html"><![CDATA[<h3 id="代码如下"><a href="#代码如下" class="headerlink" title="代码如下"></a><strong>代码如下</strong></h3><p>利用torch中的线性回归和逻辑回归模块实现</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">torch 一维线性回归算法</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成训练数据</span></span><br><span class="line">np.random.seed(<span class="number">10</span>)</span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">30</span>, <span class="number">20</span>)</span><br><span class="line">y = x * <span class="number">3</span> + np.random.normal(<span class="number">0</span>, <span class="number">5</span>, <span class="number">20</span>)</span><br><span class="line">x = np.array(x, dtype=np.float32).reshape([<span class="number">20</span>, <span class="number">1</span>])</span><br><span class="line">y = np.array(y, dtype=np.float32).reshape([<span class="number">20</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为torch中的张量形式</span></span><br><span class="line">x_train = torch.from_numpy(x)</span><br><span class="line">y_train = torch.from_numpy(y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    线性回归模型：一维线性回归</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LinearRegression, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model = LinearRegression().cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model = LinearRegression()</span><br><span class="line"></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        inputs = Variable(x_train).cuda()</span><br><span class="line">        target = Variable(y_train).cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        inputs = Variable(x_train)</span><br><span class="line">        target = Variable(y_train)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    out = model(inputs)</span><br><span class="line">    loss = criterion(out, target)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Epoch[&#123;&#125;/&#123;&#125;], loss:&#123;:.6f&#125;'</span>.format(epoch+<span class="number">1</span>, num_epochs, loss.data[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">model.eval()</span><br><span class="line">predict = model(Variable(x_train))</span><br><span class="line">predict = predict.data.numpy()</span><br><span class="line"></span><br><span class="line">plt.plot(x_train.numpy(), y_train.numpy(), <span class="string">'ro'</span>, label=<span class="string">'Original data'</span>)</span><br><span class="line">plt.plot(x_train.numpy(), predict, label=<span class="string">'predict data'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;代码如下&quot;&gt;&lt;a href=&quot;#代码如下&quot; class=&quot;headerlink&quot; title=&quot;代码如下&quot;&gt;&lt;/a&gt;&lt;strong&gt;代码如下&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;利用torch中的线性回归和逻辑回归模块实现&lt;/p&gt;
&lt;figure class=&quot;hig
      
    
    </summary>
    
      <category term="Pytorch框架" scheme="http://yoursite.com/categories/Pytorch%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="线性回归" scheme="http://yoursite.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="逻辑回归" scheme="http://yoursite.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>机器学习——线性回归和逻辑回归</title>
    <link href="http://yoursite.com/2018/10/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2018/10/17/机器学习——线性回归和逻辑回归/</id>
    <published>2018-10-17T10:01:24.000Z</published>
    <updated>2018-10-20T05:52:40.969Z</updated>
    
    <content type="html"><![CDATA[<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a><strong>线性回归</strong></h2><h3 id="简述"><a href="#简述" class="headerlink" title="简述"></a><strong>简述</strong></h3><p>在统计学中，线性回归（Linear Regression）是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合（自变量都是一次方）。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。简单来说，就是找到一条直线去拟合数据点。如下图：<br><img src="/2018/10/17/机器学习——线性回归和逻辑回归/Figure_1.png" alt="图片"></p><p>优点：结果易于理解，计算上不复杂。<br>缺点：对非线性数据拟合不好。<br>适用数据类型：数值型和标称型数据。<br>算法类型：回归算法</p><p>线性回归的模型函数如下：  </p><p>$$h_\theta = \theta^Tx$$  </p><p>它的损失函数如下：  </p><p>$$J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2$$  </p><p>通过训练数据集寻找参数的最优解，即求解可以得到$minJ(θ)$的参数向量$θ$,其中这里的参数向量也可以分为参数和$w$和$b$,分别表示权重和偏置值。<br>求解最优解的方法有最小二乘法和梯度下降法。</p><ul><li><p><strong>梯度下降法</strong><br>  梯度下降算法的思想如下(这里以一元线性回归为例)：</p><p>  首先，我们有一个代价函数，假设是$J(θ_0,θ_1)$，我们的目标是$minθ_0,θ_1 J(θ_0,θ_1)$。<br>  接下来的做法是：</p><ul><li>首先是随机选择一个参数的组合$(θ_0,θ_1)$,一般是设$θ_0=0,θ_1=0$;</li><li><p>然后是不断改变$(θ_0,θ_1)$，并计算代价函数，直到一个局部最小值。之所以是局部最小值，是因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值，选择不同的初始参数组合，可能会找到不同的局部最小值。<br>下面给出梯度下降算法的公式：<br>repeat until convergence{</p><p>  $$\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(θ_0,θ_1)(for\ j =0\ and\ j=1)$$<br>}<br>也就是在梯度下降中，不断重复上述公式直到收敛，也就是找到局部最小值局部最小值。其中符号$:=$是赋值符号的意思。</p></li><li><p>而应用梯度下降法到线性回归，则公式如下：  </p><p>$$\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{i})-y^i)$$<br>$$\theta_1 := \theta_1 - \alpha\frac{1}{m}\sum_{i=1}^m((h_\theta(x^i)-y^i)\cdot x^i)$$  </p><p>公式中的$\alpha$称为学习率(learning rate)，它决定了我们沿着能让代价函数下降程度最大的<br>方向向下迈进的步子有多大。<br>在梯度下降中，还涉及都一个参数更新的问题，即更新$(\theta_0,\theta_1)$,一般我们的做法是同步更新.<br>最后，上述梯度下降算法公式实际上是一个叫<strong>批量梯度下降(batch gradient descent)</strong>，即它在每次梯度下降中都是使用整个训练集的数据，<br>所以公式中是带有$ \sum_{i=1}^m $.</p></li></ul></li><li><p><strong>岭回归（ridge regression）</strong><br>  岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价，获得回归系数更为符合实际、更可靠的回归方法，对病态数据的耐受性远远强于最小二乘法。</p><p>  岭回归分析法是从根本上消除复共线性影响的统计方法。岭回归模型通过在相关矩阵中引入一个很小的岭参数$K（1&gt;K&gt;0）$，并将它加到主对角线元素上，从而降低参数的最小二乘估计中复共线特征向量的影响，减小复共线变量系数最小二乘估计的方法，以保证参数估计更接近真实情况。岭回归分析将所有的变量引入模型中，比逐步回归分析提供更多的信息。</p></li></ul><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a><strong>代码实现</strong></h3><p>线性回归的相关数据及代码<a href="https://github.com/zouzhen/machine-learning-algorithms-in-python" target="_blank" rel="noopener">点此</a></p><ul><li><p>使用sklearn包中的线性回归算法</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets, linear_model</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error, r2_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the diabetes dataset</span></span><br><span class="line">diabetes = datasets.load_diabetes()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use only one feature</span></span><br><span class="line">diabetes_X = diabetes.data[:, np.newaxis, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the data into training/testing sets</span></span><br><span class="line">diabetes_X_train = diabetes_X[:<span class="number">-20</span>]</span><br><span class="line">diabetes_X_test = diabetes_X[<span class="number">-20</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the targets into training/testing sets</span></span><br><span class="line">diabetes_y_train = diabetes.target[:<span class="number">-20</span>]</span><br><span class="line">diabetes_y_test = diabetes.target[<span class="number">-20</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create linear regression object</span></span><br><span class="line">regr = linear_model.LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model using the training sets</span></span><br><span class="line">regr.fit(diabetes_X_train, diabetes_y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions using the testing set</span></span><br><span class="line">diabetes_y_pred = regr.predict(diabetes_X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The coefficients</span></span><br><span class="line">print(<span class="string">'Coefficients: \n'</span>, regr.coef_)</span><br><span class="line"><span class="comment"># The mean squared error</span></span><br><span class="line">print(<span class="string">"Mean squared error: %.2f"</span></span><br><span class="line">      % mean_squared_error(diabetes_y_test, diabetes_y_pred))</span><br><span class="line"><span class="comment"># Explained variance score: 1 is perfect prediction</span></span><br><span class="line">print(<span class="string">'Variance score: %.2f'</span> % r2_score(diabetes_y_test, diabetes_y_pred))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot outputs</span></span><br><span class="line">plt.scatter(diabetes_X_test, diabetes_y_test,  color=<span class="string">'black'</span>)</span><br><span class="line">plt.plot(diabetes_X_test, diabetes_y_pred, color=<span class="string">'blue'</span>, linewidth=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">plt.xticks(())</span><br><span class="line">plt.yticks(())</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li><li><p>使用代码实现算法</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">path = os.path.dirname(os.getcwd()) + <span class="string">'\data\ex1data1.txt'</span></span><br><span class="line">data = pd.read_csv(path, header=<span class="keyword">None</span>, names=[<span class="string">'Population'</span>, <span class="string">'Profit'</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(X, y, theta)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    损失函数</span></span><br><span class="line"><span class="string">    X: 自变量</span></span><br><span class="line"><span class="string">    y: 因变量</span></span><br><span class="line"><span class="string">    theta: 参数向量</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    inner = np.power(((X * theta.T) - y), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.sum(inner) / (<span class="number">2</span> * len(X))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X, y, theta, alpha, iters)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    梯度下降算法</span></span><br><span class="line"><span class="string">    X: 自变量</span></span><br><span class="line"><span class="string">    y: 因变量</span></span><br><span class="line"><span class="string">    theta: 参数向量</span></span><br><span class="line"><span class="string">    alpha: 学习率</span></span><br><span class="line"><span class="string">    iters: 计算次数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 暂存参数向量</span></span><br><span class="line">    temp = np.matrix(np.zeros(theta.shape))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将参数向量降为一维，返回视图，可以修改原始的参数向量</span></span><br><span class="line">    parameters = int(theta.ravel().shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 损失值消耗记录</span></span><br><span class="line">    cost = np.zeros(iters)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度下降的计算</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">        error = (X * theta.T) - y</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(parameters):</span><br><span class="line">            term = np.multiply(error, X[:, j])</span><br><span class="line">            temp[<span class="number">0</span>, j] = theta[<span class="number">0</span>, j] - ((alpha / len(X)) * np.sum(term))</span><br><span class="line"></span><br><span class="line">        theta = temp</span><br><span class="line">        cost[i] = computeCost(X, y, theta)</span><br><span class="line">    <span class="keyword">return</span> theta, cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># append a ones column to the front of the data set</span></span><br><span class="line">data.insert(<span class="number">0</span>, <span class="string">'Ones'</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set X (training data) and y (target variable)</span></span><br><span class="line">cols = data.shape[<span class="number">1</span>]</span><br><span class="line">X = data.iloc[:, <span class="number">0</span>:cols - <span class="number">1</span>]</span><br><span class="line">y = data.iloc[:, cols - <span class="number">1</span>:cols]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># convert from data frames to numpy matrices</span></span><br><span class="line">X = np.matrix(X.values)</span><br><span class="line">y = np.matrix(y.values)</span><br><span class="line">theta = np.matrix(np.array([<span class="number">0</span>, <span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize variables for learning rate and iterations</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line">iters = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># perform gradient descent to "fit" the model parameters</span></span><br><span class="line">g, cost = gradientDescent(X, y, theta, alpha, iters)</span><br><span class="line"></span><br><span class="line">x = np.linspace(data.Population.min(), data.Population.max(), <span class="number">100</span>)</span><br><span class="line">f = g[<span class="number">0</span>, <span class="number">0</span>] + (g[<span class="number">0</span>, <span class="number">1</span>] * x)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">ax.plot(x, f, <span class="string">'r'</span>, label=<span class="string">'Prediction'</span>)</span><br><span class="line">ax.scatter(data.Population, data.Profit, label=<span class="string">'Traning Data'</span>)</span><br><span class="line">ax.legend(loc=<span class="number">2</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Population'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Profit'</span>)</span><br><span class="line">ax.set_title(<span class="string">'Predicted Profit vs. Population Size'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看损失值的变化</span></span><br><span class="line"><span class="comment"># fig, ax = plt.subplots(figsize=(12,8))</span></span><br><span class="line"><span class="comment"># ax.plot(np.arange(iters), cost, 'r')</span></span><br><span class="line"><span class="comment"># ax.set_xlabel('Iterations')</span></span><br><span class="line"><span class="comment"># ax.set_ylabel('Cost')</span></span><br><span class="line"><span class="comment"># ax.set_title('Error vs. Training Epoch')</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a><strong>逻辑回归</strong></h2><h3 id="简述-1"><a href="#简述-1" class="headerlink" title="简述"></a><strong>简述</strong></h3><p>Logistic回归算法基于$Sigmoid$函数，或者说$Sigmoid$就是逻辑回归函数。$Sigmoid$函数定义如下： $\frac{1}{1+e^{-z}}$。函数值域范围$(0,1)$。<br>因此逻辑回归函数的表达式如下：  </p><p>$$h_\theta = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}}$$<br>$$其中，g(z) = \frac{1}{1+e^{-z}}$$  </p><p>其导数形式为：  </p><p>$$g\prime(z) = \frac{d}{dz}\frac{1}{1+e^{-z}}$$<br>$$=\frac{1}{(1+e^{-z})^2}(e^{-z})$$<br>$$=\frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}})$$<br>$$ = g(z)(1-g(z))$$</p><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a><strong>代价函数</strong></h3><p>逻辑回归方法主要是用最大似然估计来学习的，所以单个样本的后验概率为：  </p><p>$$p(y|x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}$$</p><p>到整个样本的后验概率就是:  </p><p>$$L(\theta) = p(y|X;\theta)$$<br>$$ = \prod_{i=1}^{m}p(y^i|x^i;\theta)$$<br>$$ = \prod_{i=1}^{m}(h_\theta(x^i))^{y^i}(1-h_\theta(x^i))^{1-y^i}$$<br>$$其中，P(y=1|x;\theta)=h_\theta(x),P(y=0|x;\theta)=1-h_\theta(x)$$<br>$$通过对数进一步简化有：l(\theta) = \log L(\theta) = \sum_{i=1}^m(y^i\log h(x^i)+(1-y^i)\log(1-h(x^i)))$$</p><p>而逻辑回归的代价函数就是$−l(\theta)$。也就是如下所示：  </p><p>$$J(\theta) = \frac{1}{m}\left[\sum_{i=1}^{m}y^i\log h_\theta(x^i)+(1-y^i)\log(1-h_\theta(x^i))\right]$$</p><p>同样可以使用梯度下降算法来求解使得代价函数最小的参数。其梯度下降法公式为：  </p><p>$$\frac{\partial}{\partial\theta_j}l(\theta) = \left(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)}\right)\frac{\partial}{\partial\theta_j}g(\theta^Tx)$$<br>$$= \left(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)}\right)\left(1-g(\theta^Tx)\frac{\partial}{\partial\theta_j}(\theta^Tx)\right)g(\theta^Tx)$$<br>$$= (y(1-g(\theta^Tx))-(1-y)g(\theta^Tx))x_j$$<br>$$= (y-h_\theta(x))x_j$$</p><p>$$\theta_j := \theta_j + \alpha(y^i-h_\theta(x^i)x_j^i$$</p><ul><li><p>总结<br>  优点：</p><p>  　　1、实现简单；</p><p>  　　2、分类时计算量非常小，速度很快，存储资源低；</p><p>  缺点：</p><p>  　　1、容易欠拟合，一般准确度不太高</p><p>  　　2、只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；</p><p>  适用数据类型：数值型和标称型数据。<br>  类别：分类算法。<br>  试用场景：解决二分类问题。<br>  如下图：<br>  <img src="/2018/10/17/机器学习——线性回归和逻辑回归/Figure_3.png" alt="图片3"><br>  <img src="/2018/10/17/机器学习——线性回归和逻辑回归/Figure_2.png" alt="图片2"></p></li></ul><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a><strong>代码实现</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">Plot multinomial and One-vs-Rest Logistic Regression</span></span><br><span class="line"><span class="string">'</span><span class="string">''</span></span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># make 3-class dataset for classification</span></span><br><span class="line">centers = [[-5, 0], [0, 1.5], [5, -1]]</span><br><span class="line">X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)</span><br><span class="line">transformation = [[0.4, 0.2], [-0.4, 1.2]]</span><br><span class="line">X = np.dot(X, transformation)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> multi_class <span class="keyword">in</span> (<span class="string">'multinomial'</span>, <span class="string">'ovr'</span>):</span><br><span class="line">    clf = LogisticRegression(solver=<span class="string">'sag'</span>, max_iter=100, random_state=42,</span><br><span class="line">                             multi_class=multi_class).fit(X, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print the training scores</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"training score : %.3f (%s)"</span> % (clf.score(X, y), multi_class))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create a mesh to plot in</span></span><br><span class="line">    h = .02  <span class="comment"># step size in the mesh</span></span><br><span class="line">    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                         np.arange(y_min, y_max, h))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot the decision boundary. For that, we will assign a color to each</span></span><br><span class="line">    <span class="comment"># point in the mesh [x_min, x_max]x[y_min, y_max].</span></span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    <span class="comment"># Put the result into a color plot</span></span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)</span><br><span class="line">    plt.title(<span class="string">"Decision surface of LogisticRegression (%s)"</span> % multi_class)</span><br><span class="line">    plt.axis(<span class="string">'tight'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot also the training points</span></span><br><span class="line">    colors = <span class="string">"bry"</span></span><br><span class="line">    <span class="keyword">for</span> i, color <span class="keyword">in</span> zip(clf.classes_, colors):</span><br><span class="line">        idx = np.where(y == i)</span><br><span class="line">        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired,</span><br><span class="line">                    edgecolor=<span class="string">'black'</span>, s=20)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot the three one-against-all classifiers</span></span><br><span class="line">    xmin, xmax = plt.xlim()</span><br><span class="line">    ymin, ymax = plt.ylim()</span><br><span class="line">    coef = clf.coef_</span><br><span class="line">    intercept = clf.intercept_</span><br><span class="line"></span><br><span class="line">    def plot_hyperplane(c, color):</span><br><span class="line">        def line(x0):</span><br><span class="line">            <span class="built_in">return</span> (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]</span><br><span class="line">        plt.plot([xmin, xmax], [line(xmin), line(xmax)],</span><br><span class="line">                 ls=<span class="string">"--"</span>, color=color)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, color <span class="keyword">in</span> zip(clf.classes_, colors):</span><br><span class="line">        plot_hyperplane(i, color)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">代码实现(加入正则化)</span></span><br><span class="line"><span class="string">'</span><span class="string">''</span></span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import scipy.optimize as opt</span><br><span class="line">import os</span><br><span class="line">path = os.path.dirname(os.getcwd()) + <span class="string">'\data\ex2data1.txt'</span></span><br><span class="line">data2 = pd.read_csv(path, header=None, names=[<span class="string">'Test 1'</span>, <span class="string">'Test 2'</span>, <span class="string">'Accepted'</span>])</span><br><span class="line"></span><br><span class="line">positive = data2[data2[<span class="string">'Accepted'</span>].isin([1])]</span><br><span class="line">negative = data2[data2[<span class="string">'Accepted'</span>].isin([0])]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sigmoid(z):</span><br><span class="line">    <span class="built_in">return</span> 1 / (1 + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def costReg(theta, X, y, learningRate):</span><br><span class="line">    theta = np.matrix(theta)</span><br><span class="line">    X = np.matrix(X)</span><br><span class="line">    y = np.matrix(y)</span><br><span class="line">    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))</span><br><span class="line">    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))</span><br><span class="line">    reg = (learningRate / 2 * len(X)) * np.sum(np.power(theta[:, 1:theta.shape[1]], 2))</span><br><span class="line">    <span class="built_in">return</span> np.sum(first - second) / (len(X)) + reg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def gradientReg(theta, X, y, learningRate):</span><br><span class="line">    theta = np.matrix(theta)</span><br><span class="line">    X = np.matrix(X)</span><br><span class="line">    y = np.matrix(y)</span><br><span class="line"></span><br><span class="line">    parameters = int(theta.ravel().shape[1])</span><br><span class="line">    grad = np.zeros(parameters)</span><br><span class="line"></span><br><span class="line">    error = sigmoid(X * theta.T) - y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(parameters):</span><br><span class="line">        term = np.multiply(error, X[:,i])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i == 0):</span><br><span class="line">            grad[i] = np.sum(term) / len(X)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            grad[i] = (np.sum(term) / len(X)) + ((learningRate / len(X)) * theta[:,i])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> grad</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def predict(theta, X):</span><br><span class="line">    probability = sigmoid(X * theta.T)</span><br><span class="line">    <span class="built_in">return</span> [1 <span class="keyword">if</span> x &gt;= 0.5 <span class="keyword">else</span> 0 <span class="keyword">for</span> x <span class="keyword">in</span> probability]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">degree = 5</span><br><span class="line">x1 = data2[<span class="string">'Test 1'</span>]</span><br><span class="line">x2 = data2[<span class="string">'Test 2'</span>]</span><br><span class="line"></span><br><span class="line">data2.insert(3, <span class="string">'Ones'</span>, 1)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(1, degree):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(0, i):</span><br><span class="line">        data2[<span class="string">'F'</span> + str(i) + str(j)] = np.power(x1, i-j) * np.power(x2, j)</span><br><span class="line"></span><br><span class="line">data2.drop(<span class="string">'Test 1'</span>, axis=1, inplace=True)</span><br><span class="line">data2.drop(<span class="string">'Test 2'</span>, axis=1, inplace=True)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set X and y (remember from above that we moved the label to column 0)</span></span><br><span class="line">cols = data2.shape[1]</span><br><span class="line">X2 = data2.iloc[:,1:cols]</span><br><span class="line">y2 = data2.iloc[:,0:1]</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert to numpy arrays and initalize the parameter array theta</span></span><br><span class="line">X2 = np.array(X2.values)</span><br><span class="line">y2 = np.array(y2.values)</span><br><span class="line">theta2 = np.zeros(11)</span><br><span class="line"></span><br><span class="line">learningRate = 0.1</span><br><span class="line">result2 = opt.fmin_tnc(func=costReg, x0=theta2, fprime=gradientReg, args=(X2, y2, learningRate))</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(costReg(theta2, X2, y2, learningRate))</span></span><br><span class="line">theta_min = np.matrix(result2[0])</span><br><span class="line">predictions = predict(theta_min, X2)</span><br><span class="line">correct = [1 <span class="keyword">if</span> ((a == 1 and b == 1) or (a == 0 and b == 0)) <span class="keyword">else</span> 0 <span class="keyword">for</span> (a, b) <span class="keyword">in</span> zip(predictions, y2)]</span><br><span class="line">accuracy = (sum(map(int, correct)) % len(correct))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'accuracy = &#123;0&#125;%'</span>.format(accuracy))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;线性回归&quot;&gt;&lt;a href=&quot;#线性回归&quot; class=&quot;headerlink&quot; title=&quot;线性回归&quot;&gt;&lt;/a&gt;&lt;strong&gt;线性回归&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;简述&quot;&gt;&lt;a href=&quot;#简述&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
      <category term="机器学习算法" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="线性回归" scheme="http://yoursite.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="逻辑回归" scheme="http://yoursite.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>scikit-learn整理</title>
    <link href="http://yoursite.com/2018/10/17/scikit-learn%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/10/17/scikit-learn整理/</id>
    <published>2018-10-17T07:54:09.000Z</published>
    <updated>2018-10-17T09:51:48.931Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Flask整理</title>
    <link href="http://yoursite.com/2018/10/17/Flask%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/10/17/Flask整理/</id>
    <published>2018-10-17T07:50:11.000Z</published>
    <updated>2018-10-17T09:51:48.921Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用工具"><a href="#使用工具" class="headerlink" title="使用工具"></a><strong>使用工具</strong></h2><h4 id="Flask后端-Postgresql数据库-JS前端（我未使用）"><a href="#Flask后端-Postgresql数据库-JS前端（我未使用）" class="headerlink" title="Flask后端 + Postgresql数据库 + JS前端（我未使用）"></a>Flask后端 + Postgresql数据库 + JS前端（我未使用）</h4><h2 id="Flask搭建"><a href="#Flask搭建" class="headerlink" title="Flask搭建"></a>Flask搭建</h2><ul><li><p><strong>确定确定目录结构</strong></p><ol><li>app/algorithms: 用来存放相关的算法文件</li><li>app/models: 用来存放数据库的操作</li><li>app/web: 用来存放路由和视图函数</li><li>manage: flask的启动文件</li></ol></li><li><p><strong>确定路由注册方式</strong></p><ol><li>使用蓝图形式来注册路由</li></ol></li><li><p><strong>确定数据库操作方式</strong></p><ol><li>使用sqlalchemy及psycopg2来控制Postgresql数据库</li><li>由于主要是用来进行数据读取的，所以采用非ORM方式构建的表结构，这种方式方便进行查询过滤操作</li></ol></li></ul><h2 id="基于sqlalchemy的Postgresql数据库访问操作"><a href="#基于sqlalchemy的Postgresql数据库访问操作" class="headerlink" title="基于sqlalchemy的Postgresql数据库访问操作"></a>基于sqlalchemy的Postgresql数据库访问操作</h2><ul><li><strong>创建表结构</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sqlalchemy.engine <span class="keyword">import</span> create_engine</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.schema <span class="keyword">import</span> MetaData, Table, Column, ForeignKey, Sequence</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.types <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.sql.expression <span class="keyword">import</span> select,and_</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line">engine = create_engine(<span class="string">'postgres://user:password@hosts/builder'</span>, echo=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">metadata = MetaData()</span><br><span class="line">metadata.bind = engine</span><br><span class="line">    <span class="comment"># 创建桥梁索引表</span></span><br><span class="line">bridges_table = Table(<span class="string">'bridges'</span>, metadata,</span><br><span class="line">                      Column(<span class="string">'id'</span>, Integer, primary_key=<span class="keyword">True</span>),</span><br><span class="line">                      Column(<span class="string">'org_id'</span>, Integer, nullable=<span class="keyword">False</span>),</span><br><span class="line">                      Column(<span class="string">'user_id'</span>, Integer, nullable=<span class="keyword">False</span>),</span><br><span class="line">                      Column(<span class="string">'name'</span>, VARCHAR(length=<span class="number">255</span>), nullable=<span class="keyword">False</span>),</span><br><span class="line">                      Column(<span class="string">'created_date'</span>, TIMESTAMP, nullable=<span class="keyword">False</span>),</span><br><span class="line">                      Column(<span class="string">'finished_date'</span>, TIMESTAMP, nullable=<span class="keyword">True</span>),</span><br><span class="line">                    <span class="comment">#   autoload=True,</span></span><br><span class="line">                      )</span><br></pre></td></tr></table></figure><p>这种方式，有助于进行表查询，具体的相关API介绍及使用放那格式可<a href="https://docs.sqlalchemy.org/en/latest/genindex.html" target="_blank" rel="noopener">点此</a>查看。</p><ul><li><strong>相关操作</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">()</span>:</span></span><br><span class="line">    s = book_table.insert().values(title=<span class="string">'测试写入2'</span>,time=datetime.now())</span><br><span class="line">    c = engine.execute(s)</span><br><span class="line">    c.close()</span><br><span class="line">    <span class="keyword">return</span> c.inserted_primary_key</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">query_code</span><span class="params">(id)</span>:</span></span><br><span class="line">    info = &#123;<span class="string">'id'</span>: <span class="string">''</span>, <span class="string">'title'</span>: <span class="string">''</span>&#125;</span><br><span class="line">    s = select([bridge_jobs_table.c.id.label(<span class="string">'name'</span>)]).where(and_(bridge_jobs_table.c.kind==<span class="string">'桩基'</span>,bridge_jobs_table.c.name==<span class="string">'起钻'</span>))</span><br><span class="line">    codename_query = engine.execute(s)</span><br><span class="line">    print(codename_query.keys())</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> codename_query:</span><br><span class="line">        print(row[<span class="number">0</span>])</span><br><span class="line">    codename_query.close()</span><br><span class="line">    <span class="keyword">return</span> info</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updata</span><span class="params">(id, title)</span>:</span></span><br><span class="line">    s = book_table.update().where(book_table.c.id == id).values(title=title, id=id)</span><br><span class="line">    c = engine.execute(s)</span><br><span class="line">    c.close()</span><br></pre></td></tr></table></figure><h2 id="Flask相关知识"><a href="#Flask相关知识" class="headerlink" title="Flask相关知识"></a>Flask相关知识</h2><ul><li><p><strong>路由操作</strong></p><ol><li><p>静态路由</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@web.route('/hello', methods=['POST', 'GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'hello world!'</span></span><br></pre></td></tr></table></figure></li><li><p>参数路由</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@web.route('/hello/&lt;string:name&gt;', methods=['POST', 'GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'hello %d '</span>% name</span><br></pre></td></tr></table></figure></li><li><p>JSON返回</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@web.route('/hello/&lt;string:name&gt;', methods=['POST', 'GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> jsonify(<span class="string">'hello %d '</span>% name)</span><br></pre></td></tr></table></figure></li><li><p>使用蓝图方式注册路由</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_app</span><span class="params">()</span>:</span></span><br><span class="line">    app = Flask(__name__)</span><br><span class="line">    app.config.from_object(<span class="string">'app.setting'</span>)</span><br><span class="line"></span><br><span class="line">    register_blueprint(app)</span><br><span class="line">    <span class="comment"># db.init_app(app)</span></span><br><span class="line">    <span class="comment"># db.create_app(app=app)</span></span><br><span class="line">    <span class="keyword">return</span> app</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">register_blueprint</span><span class="params">(app)</span>:</span></span><br><span class="line">    <span class="keyword">from</span> app.web.view <span class="keyword">import</span> web</span><br><span class="line">    app.register_blueprint(web)</span><br></pre></td></tr></table></figure></li></ol></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;使用工具&quot;&gt;&lt;a href=&quot;#使用工具&quot; class=&quot;headerlink&quot; title=&quot;使用工具&quot;&gt;&lt;/a&gt;&lt;strong&gt;使用工具&lt;/strong&gt;&lt;/h2&gt;&lt;h4 id=&quot;Flask后端-Postgresql数据库-JS前端（我未使用）&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="Python后端" scheme="http://yoursite.com/categories/Python%E5%90%8E%E7%AB%AF/"/>
    
    
      <category term="flask框架" scheme="http://yoursite.com/tags/flask%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>自省</title>
    <link href="http://yoursite.com/2018/10/06/%E8%87%AA%E7%9C%81/"/>
    <id>http://yoursite.com/2018/10/06/自省/</id>
    <published>2018-10-06T03:06:43.000Z</published>
    <updated>2018-10-06T03:12:28.241Z</updated>
    
    <content type="html"><![CDATA[<script src="/js/crypto-js.js"></script><script>function doDecrypt (pwd, onError) {console.log('in doDecrypt');const txt = document.getElementById('enc_content').innerHTML;let plantext;try {const bytes = CryptoJS.AES.decrypt(txt, pwd);var plaintext = bytes.toString(CryptoJS.enc.Utf8);} catch(err) {if(onError) {onError(err);}return;}document.getElementById('enc_content').innerHTML = plaintext;document.getElementById('enc_content').style.display = 'block';document.getElementById('enc_passwd').style.display = 'none';if(typeof MathJax !== 'undefined') {MathJax.Hub.Queue(['resetEquationNumbers', MathJax.InputJax.TeX],['PreProcess', MathJax.Hub],['Reprocess', MathJax.Hub]);}}</script><div id="enc_content" style="display:none">U2FsdGVkX1+nxC/HRHG7UFqCqGEmfkcjhP2Atjk/Gq7tMJtBJOnQAQui0DORcklx1LI3uBOIrOIflLG/R4cU0f01EK2hrqt1RSufZkc8HcNhaPeSJWllh7khM9h5tVQJAgWeJrADhimW2iSy0DiHZwmBV6/ui9IrS80R8N8qsr0hPP6Ivh0/2s9fa1/l9wZIPCcDF0EmP3LruDUN0u6ITcpuDaYJ3PajNTzrmPSQfIoUzB4lJtBLZRTCeGXx7tPe4BEMVj39gbbs0L0ebPvtJ1gnzXL8eHYhH6XJGqZejRVOIhyaMkwPeCypY5XW7aFEZSkh3+w7wrSGK1bF2RwRF87ndswhGxiiu9kPB/kprQk=</div><div id="enc_passwd"> <input id="enc_pwd_input" type="password" style="border-radius: 5px;border-style: groove;height: 30px;width: 50%;cursor: auto;font-size: 102%;color: currentColor;outline: none;text-overflow: initial;padding-left: 5px;" onkeydown="if (event.keyCode == 13) { decrypt(); return false;}"> <input type="submit" value="解&nbsp;密" onclick="decrypt()" style="width: 58px;height: 34px;border-radius: 5px;background-color: white;border-style: solid;color: currentColor;"><div id="enc_error" style="display: inline-block;color: #d84527;margin-left: 10px"></div><script>var onError = function(error) {document.getElementById("enc_error").innerHTML = "password error!"};function decrypt() {var passwd = document.getElementById("enc_pwd_input").value;console.log(passwd);doDecrypt(passwd, onError);}</script></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;/js/crypto-js.js&quot;&gt;&lt;/script&gt;
&lt;script&gt;
function doDecrypt (pwd, onError) {
	console.log(&#39;in doDecrypt&#39;);
	const txt = document.ge
      
    
    </summary>
    
      <category term="感悟" scheme="http://yoursite.com/categories/%E6%84%9F%E6%82%9F/"/>
    
    
      <category term="自我反省" scheme="http://yoursite.com/tags/%E8%87%AA%E6%88%91%E5%8F%8D%E7%9C%81/"/>
    
  </entry>
  
  <entry>
    <title>PostgreSQL学习记录</title>
    <link href="http://yoursite.com/2018/09/26/PostgreSQL%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2018/09/26/PostgreSQL学习记录/</id>
    <published>2018-09-26T06:26:56.000Z</published>
    <updated>2018-10-17T09:54:05.116Z</updated>
    
    <content type="html"><![CDATA[<p>select<br>fetchall()<br>fetchmany()<br>fetchone()</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;select&lt;br&gt;fetchall()&lt;br&gt;fetchmany()&lt;br&gt;fetchone()&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>docker基础</title>
    <link href="http://yoursite.com/2018/09/03/docker%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/09/03/docker基础/</id>
    <published>2018-09-03T02:44:54.000Z</published>
    <updated>2018-10-17T09:51:48.925Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Python入门整理</title>
    <link href="http://yoursite.com/2018/08/21/Python%E5%85%A5%E9%97%A8%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/08/21/Python入门整理/</id>
    <published>2018-08-21T11:23:32.000Z</published>
    <updated>2018-08-30T07:21:18.750Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Python基础"><a href="#Python基础" class="headerlink" title="Python基础"></a>Python基础</h3><ol><li>python的执行问题<ol><li>1 python文件的后缀名可以是任意的</li><li>2 考虑到文件之间的导入问题，统一以.py文件结尾</li></ol></li><li>基本运算符<ol start="2"><li>1 算数运算符（结果为值）</li><li>2 赋值运算符（结果为值）</li><li>3 比较运算符（结果为布尔值）</li><li>4 逻辑运算符（结果为布尔值）</li><li>5 成员运算符（结果为布尔值）</li></ol></li><li>基本数据类型<ol start="3"><li>1 数字</li><li>2 字符串</li><li>3 列表</li><li>4 元组</li><li>5 字典</li><li>6 布尔值</li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Python基础&quot;&gt;&lt;a href=&quot;#Python基础&quot; class=&quot;headerlink&quot; title=&quot;Python基础&quot;&gt;&lt;/a&gt;Python基础&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;python的执行问题&lt;ol&gt;
&lt;li&gt;1 python文件的后缀名可以是任意的
      
    
    </summary>
    
      <category term="编程语言" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow入门整理</title>
    <link href="http://yoursite.com/2018/08/05/tensorflow%E5%85%A5%E9%97%A8%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/08/05/tensorflow入门整理/</id>
    <published>2018-08-05T10:26:06.000Z</published>
    <updated>2018-08-07T10:08:38.972Z</updated>
    
    <content type="html"><![CDATA[<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><blockquote><p>tf.placeholder()</p></blockquote><p>是tensorflow的一种特殊变量，这种变量并非在初始化时定义好内容，而是在训练的时候才将数据填入其中。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;tf.placeholder()&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;是tensorflow
      
    
    </summary>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>vim基本使用方法</title>
    <link href="http://yoursite.com/2018/08/01/vim%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2018/08/01/vim基本使用方法/</id>
    <published>2018-08-01T07:39:35.000Z</published>
    <updated>2018-08-05T09:10:09.401Z</updated>
    
    <content type="html"><![CDATA[<h2 id="vi的基本概念"><a href="#vi的基本概念" class="headerlink" title="vi的基本概念"></a>vi的基本概念</h2><p>基本上vi可以分为三种状态，分别是命令模式（command mode）、插入模式（Insert mode）和底行模式（last line mode），各模式的功能区分如下：</p><ol><li>命令行模式command mode）<br>控制屏幕光标的移动，字符、字或行的删除，移动复制某区段及进入Insert mode下，或者到 last line mode。</li><li>插入模式（Insert mode）<br>只有在Insert mode下，才可以做文字输入，按「ESC」键可回到命令行模式。</li><li>底行模式（last line mode）<br>将文件保存或退出vi，也可以设置编辑环境，如寻找字符串、列出行号……等。</li></ol><p>不过一般我们在使用时把vi简化成两个模式，就是将底行模式（last line mode）也算入命令行模式command mode）。</p><h2 id="vi的基本操作"><a href="#vi的基本操作" class="headerlink" title="vi的基本操作"></a>vi的基本操作</h2><ol><li><p>进入vi<br>在系统提示符号输入vi及文件名称后，就进入vi全屏幕编辑画面：$ vi myfile。不过有一点要特别注意，就是您进入vi之后，是处于「命令行模式（command mode）」，您要切换到「插入模式（Insert mode）」才能够输入文字。初次使用vi的人都会想先用上下左右键移动光标，结果电脑一直哔哔叫，把自己气个半死，所以进入vi后，先不要乱动，转换到「插入模式（Insert mode）」再说吧！</p></li><li><p>切换至插入模式（Insert mode）编辑文件<br>在「命令行模式（command mode）」下按一下字母「i」就可以进入「插入模式（Insert mode）」，这时候你就可以开始输入文字了。</p></li><li><p>Insert 的切换<br>您目前处于「插入模式（Insert mode）」，您就只能一直输入文字，如果您发现输错了字！想用光标键往回移动，将该字删除，就要先按一下「ESC」键转到「命令行模式（command mode）」再删除文字。</p></li><li><p>退出vi及保存文件<br>在「命令行模式（command mode）」下，按一下「：」冒号键进入「Last line mode」，例如：<br>: w filename （输入 「w filename」将文章以指定的文件名filename保存）<br>: wq (输入「wq」，存盘并退出vi)<br>: q! (输入q!， 不存盘强制退出vi)</p></li></ol><h2 id="命令行模式（command-mode）功能键"><a href="#命令行模式（command-mode）功能键" class="headerlink" title="命令行模式（command mode）功能键"></a>命令行模式（command mode）功能键</h2><ol><li><p>插入模式<br>按「i」切换进入插入模式「insert mode」，按“i”进入插入模式后是从光标当前位置开始输入文件；<br>按「a」进入插入模式后，是从目前光标所在位置的下一个位置开始输入文字；<br>按「o」进入插入模式后，是插入新的一行，从行首开始输入文字。</p></li><li><p>从插入模式切换为命令行模式<br>按「ESC」键。</p></li><li><p>移动光标<br>vi可以直接用键盘上的光标来上下左右移动，但正规的vi是用小写英文字母「h」、「j」、「k」、「l」，分别控制光标左、下、上、右移一格。<br>按「ctrl」+「b」：屏幕往“后”移动一页。<br>按「ctrl」+「f」：屏幕往“前”移动一页。<br>按「ctrl」+「u」：屏幕往“后”移动半页。<br>按「ctrl」+「d」：屏幕往“前”移动半页。<br>按数字「0」：移到文章的开头。<br>按「G」：移动到文章的最后。<br>按「$」：移动到光标所在行的“行尾”。<br>按「^」：移动到光标所在行的“行首”<br>按「w」：光标跳到下个字的开头<br>按「e」：光标跳到下个字的字尾<br>按「b」：光标回到上个字的开头<br>按「#l」：光标移到该行的第#个位置，如：5l,56l。</p></li><li><p>删除文字<br>「x」：每按一次，删除光标所在位置的“后面”一个字符。<br>「#x」：例如，「6x」表示删除光标所在位置的“后面”6个字符。<br>「X」：大写的X，每按一次，删除光标所在位置的“前面”一个字符。<br>「#X」：例如，「20X」表示删除光标所在位置的“前面”20个字符。<br>「dd」：删除光标所在行。<br>「#dd」：从光标所在行开始删除#行</p></li><li><p>复制<br>「yw」：将光标所在之处到字尾的字符复制到缓冲区中。<br>「#yw」：复制#个字到缓冲区<br>「yy」：复制光标所在行到缓冲区。<br>「#yy」：例如，「6yy」表示拷贝从光标所在的该行“往下数”6行文字。<br>「p」：将缓冲区内的字符贴到光标所在位置。注意：所有与“y”有关的复制命令都必须与“p”配合才能完成复制与粘贴功能。</p></li><li><p>替换<br>「r」：替换光标所在处的字符。<br>「R」：替换光标所到之处的字符，直到按下「ESC」键为止。</p></li><li><p>回复上一次操作<br>「u」：如果您误执行一个命令，可以马上按下「u」，回到上一个操作。按多次“u”可以执行多次回复。</p></li><li><p>更改<br>「cw」：更改光标所在处的字到字尾处<br>「c#w」：例如，「c3w」表示更改3个字</p></li><li><p>跳至指定的行<br>「ctrl」+「g」列出光标所在行的行号。<br>「#G」：例如，「15G」，表示移动光标至文章的第15行行首。</p></li></ol><h2 id="Last-line-mode下命令简介"><a href="#Last-line-mode下命令简介" class="headerlink" title="Last line mode下命令简介"></a>Last line mode下命令简介</h2><p>　　在使用「last line mode」之前，请记住先按「ESC」键确定您已经处于「command mode」下后，再按「：」冒号即可进入「last line mode」。</p><ol><li><p>列出行号<br>「set nu」：输入「set nu」后，会在文件中的每一行前面列出行号。</p></li><li><p>跳到文件中的某一行<br>「#」：「#」号表示一个数字，在冒号后输入一个数字，再按回车键就会跳到该行了，如输入数字15，再回车，就会跳到文章的第15行。</p></li><li><p>查找字符<br>「/关键字」：先按「/」键，再输入您想寻找的字符，如果第一次找的关键字不是您想要的，可以一直按「n」会往后寻找到您要的关键字为止。<br>「?关键字」：先按「?」键，再输入您想寻找的字符，如果第一次找的关键字不是您想要的，可以一直按「n」会往前寻找到您要的关键字为止。</p></li><li><p>保存文件<br>「w」：在冒号输入字母「w」就可以将文件保存起来。</p></li><li><p>离开vi<br>「q」：按「q」就是退出，如果无法离开vi，可以在「q」后跟一个「!」强制离开vi。<br>「qw」：一般建议离开时，搭配「w」一起使用，这样在退出的时候还可以保存文件。</p></li></ol><h2 id="vi命令列表"><a href="#vi命令列表" class="headerlink" title="vi命令列表"></a>vi命令列表</h2><ol><li><p>下表列出命令模式下的一些键的功能：</p><p> h左移光标一个字符<br> l右移光标一个字符<br> k光标上移一行<br> j光标下移一行<br> ^光标移动至行首<br> 0数字“0”，光标移至文章的开头<br> G光标移至文章的最后<br> $光标移动至行尾<br> Ctrl+f向前翻屏<br> Ctrl+b向后翻屏<br> Ctrl+d向前翻半屏<br> Ctrl+u向后翻半屏<br> i在光标位置前插入字符<br> a在光标所在位置的后一个字符开始增加<br> o插入新的一行，从行首开始输入<br> ESC从输入状态退至命令状态<br> x删除光标后面的字符<br> #x删除光标后的＃个字符<br> X(大写X)，删除光标前面的字符<br> #X删除光标前面的#个字符<br> dd删除光标所在的行<br> #dd删除从光标所在行数的#行<br> yw复制光标所在位置的一个字<br> #yw复制光标所在位置的#个字<br> yy复制光标所在位置的一行<br> #yy复制从光标所在行数的#行<br> p粘贴<br> u取消操作<br> cw更改光标所在位置的一个字<br> #cw更改光标所在位置的#个字</p></li></ol><ol start="2"><li><p>下表列出行命令模式下的一些指令  </p><p> w filename储存正在编辑的文件为filename<br> wq filename储存正在编辑的文件为filename，并退出vi<br> q!放弃所有修改，退出vi<br> set nu显示行号<br> /或?查找，在/后输入要查找的内容<br> n与/或?一起使用，如果查找的内容不是想要找的关键字，按n或向后（与/联用）或向前（与?联用）继续查找，直到找到为止。</p></li></ol><p>高手总结的图：<img src="/2018/08/01/vim基本使用方法/vim.jpg" alt="vim"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;vi的基本概念&quot;&gt;&lt;a href=&quot;#vi的基本概念&quot; class=&quot;headerlink&quot; title=&quot;vi的基本概念&quot;&gt;&lt;/a&gt;vi的基本概念&lt;/h2&gt;&lt;p&gt;基本上vi可以分为三种状态，分别是命令模式（command mode）、插入模式（Insert mo
      
    
    </summary>
    
      <category term="工具" scheme="http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="vim" scheme="http://yoursite.com/tags/vim/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch入门</title>
    <link href="http://yoursite.com/2018/08/01/PyTorch%E5%85%A5%E9%97%A8/"/>
    <id>http://yoursite.com/2018/08/01/PyTorch入门/</id>
    <published>2018-08-01T06:59:07.000Z</published>
    <updated>2018-10-20T05:56:23.094Z</updated>
    
    <content type="html"><![CDATA[<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><hr><ul><li><font color="#00dddd" size="4">张量</font><br>  </li></ul><hr><p>张量类似于NumPy的ndarray，另外还有Tensors也可用于GPU以加速计算。  </p><pre><code>from __future__ import print_functionimport torch  </code></pre><p>构造一个未初始化的5x3矩阵：</p><pre><code>x = torch.empty(5, 3)print(x)    tensor([[ 3.2401e+18,  0.0000e+00,  1.3474e-08],    [ 4.5586e-41,  1.3476e-08,  4.5586e-41],    [ 1.3476e-08,  4.5586e-41,  1.3474e-08],    [ 4.5586e-41,  1.3475e-08,  4.5586e-41],    [ 1.3476e-08,  4.5586e-41,  1.3476e-08]])  </code></pre><p>构造一个矩阵填充的零和dtype long：</p><pre><code>x = torch.zeros(5, 3, dtype=torch.long)print(x)  tensor([[ 0,  0,  0],        [ 0,  0,  0],        [ 0,  0,  0],        [ 0,  0,  0],        [ 0,  0,  0]])  </code></pre><p>直接从数据构造张量：</p><pre><code>x = torch.tensor([5.5, 3])print(x)tensor([ 5.5000,  3.0000])</code></pre><p>或者根据现有的张量创建张量。除非用户提供新值，否则这些方法将重用输入张量的属性，例如dtype</p><pre><code>x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizesprint(x)x = torch.randn_like(x, dtype=torch.float)    # override dtype!print(x)                                      # result has the same sizetensor([[ 1.,  1.,  1.],        [ 1.,  1.,  1.],        [ 1.,  1.,  1.],        [ 1.,  1.,  1.],        [ 1.,  1.,  1.]], dtype=torch.float64)tensor([[ 0.2641,  0.0149,  0.7355],        [ 0.6106, -1.2480,  1.0592],        [ 2.6305,  0.5582,  0.3042],        [-1.4410,  2.4951, -0.0818],        [ 0.8605,  0.0001, -0.7220]])</code></pre><p>得到它的大小：  </p><pre><code>print(x.size())torch.Size([5, 3])  </code></pre><p><strong>注意</strong>  </p><p><strong>torch.Size</strong> 实际上是一个元组，因此它支持所有元组操作。  </p><hr><ul><li><font color="#00dddd" size="4">操作</font><br>   </li></ul><hr><p>操作有多种语法。在下面的示例中，我们将查看添加操作。</p><p>增加：语法1  </p><pre><code>y = torch.rand(5, 3)print(x + y)tensor([[ 0.7355,  0.2798,  0.9392],        [ 1.0300, -0.6085,  1.7991],        [ 2.8120,  1.2438,  1.2999],        [-1.0534,  2.8053,  0.0163],        [ 1.4088,  0.9000, -0.1172]])</code></pre><p>增加：语法2</p><pre><code>print(torch.add(x, y))tensor([[ 0.7355,  0.2798,  0.9392],        [ 1.0300, -0.6085,  1.7991],        [ 2.8120,  1.2438,  1.2999],        [-1.0534,  2.8053,  0.0163],        [ 1.4088,  0.9000, -0.1172]])</code></pre><p>增加：提供输出张量作为参数</p><pre><code>result = torch.empty(5, 3)torch.add(x, y, out=result)print(result)tensor([[ 0.7355,  0.2798,  0.9392],        [ 1.0300, -0.6085,  1.7991],        [ 2.8120,  1.2438,  1.2999],        [-1.0534,  2.8053,  0.0163],        [ 1.4088,  0.9000, -0.1172]])</code></pre><p>增加：就地</p><pre><code># adds x to yy.add_(x)print(y)tensor([[ 0.7355,  0.2798,  0.9392],        [ 1.0300, -0.6085,  1.7991],        [ 2.8120,  1.2438,  1.2999],        [-1.0534,  2.8053,  0.0163],        [ 1.4088,  0.9000, -0.1172]])</code></pre><p><strong>注意</strong></p><p>任何使原位张量变形的操作都是用<em>。后固定的。例如：x.copy</em>(y)，x.t_()，将改变x。</p><p>你可以使用标准的NumPy索引与所有的铃声和​​口哨！</p><pre><code>print(x[:, 1])tensor([ 0.0149, -1.2480,  0.5582,  2.4951,  0.0001])</code></pre><p>调整大小：如果要调整大小/重塑张量，可以使用torch.view：</p><pre><code>x = torch.randn(4, 4)y = x.view(16)z = x.view(-1, 8)  # the size -1 is inferred from other dimensionsprint(x.size(), y.size(), z.size())torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</code></pre><p>如果你有一个元素张量，用于.item()获取值作为Python数字</p><pre><code>x = torch.randn(1)print(x)print(x.item())tensor([ 1.3159])1.3159412145614624</code></pre><h3 id="NumPy-Bridge"><a href="#NumPy-Bridge" class="headerlink" title="NumPy Bridge"></a>NumPy Bridge</h3><p>将Torch Tensor转换为NumPy阵列（反之亦然）是一件轻而易举的事。<br>Torch Tensor和NumPy阵列将共享其底层内存位置，更改一个将改变另一个。</p><hr><ul><li><font color="#00dddd" size="4">将Torch Tensor转换为NumPy数组</font><br>    </li></ul><hr><pre><code>a = torch.ones(5)print(a)tensor([ 1.,  1.,  1.,  1.,  1.])b = a.numpy()print(b)[1. 1. 1. 1. 1.]</code></pre><p>了解numpy数组的值如何变化。</p><pre><code>a.add_(1)print(a)print(b)tensor([ 2.,  2.,  2.,  2.,  2.])[2. 2. 2. 2. 2.]</code></pre><hr><ul><li><font color="#00dddd" size="4">将NumPy数组转换为Torch Tensor</font><br></li></ul><hr><p>了解更改np阵列如何自动更改Torch Tensor</p><pre><code>import numpy as npa = np.ones(5)b = torch.from_numpy(a)np.add(a, 1, out=a)print(a)print(b)[2. 2. 2. 2. 2.]tensor([ 2.,  2.,  2.,  2.,  2.], dtype=torch.float64)</code></pre><p>除了CharTensor之外，CPU上的所有Tensors都支持转换为NumPy并返回。</p><hr><ul><li><font color="#00dddd" size="4">CUDA Tensors</font><br></li></ul><hr><p>可以使用该.to方法将张量移动到任何设备上。</p><pre><code># let us run this cell only if CUDA is available# We will use ``torch.device`` objects to move tensors in and out of GPUif torch.cuda.is_available():    device = torch.device(&quot;cuda&quot;)          # a CUDA device object    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU    x = x.to(device)                       # or just use strings ``.to(&quot;cuda&quot;)``    z = x + y    print(z)    print(z.to(&quot;cpu&quot;, torch.double))       # ``.to`` can also change dtype together!tensor([ 2.3159], device=&apos;cuda:0&apos;)tensor([ 2.3159], dtype=torch.float64)</code></pre><hr><ul><li><font color="#00dddd" size="6">torch</font><br></li></ul><hr><blockquote><p>torch.eye</p></blockquote><p>torch.eye(n, m=None, out=None)<br>返回一个2维张量，对角线位置全1，其它位置全0</p><p>参数:</p><ul><li>n (int ) – 行数</li><li>m (int, optional) – 列数.如果为None,则默认为n</li><li>out (Tensor, optinal) - Output tensor</li></ul><p>返回值: 对角线位置全1，其它位置全0的2维张量</p><p>返回值类型: <font color="#00099ff" size="4">Tensor</font><br></p><p>例子:</p><pre><code>&gt;&gt;&gt; torch.eye(3)1  0  00  1  00  0  1[torch.FloatTensor of size 3x3]</code></pre><blockquote><p>from_numpy</p></blockquote><p>torch.from_numpy(ndarray) → Tensor<br>Numpy桥，将numpy.ndarray 转换为pytorch的 Tensor。 返回的张量tensor和numpy的ndarray共享同一内存空间。修改一个会导致另外一个也被修改。返回的张量不能改变大小。</p><p>例子:</p><pre><code>&gt;&gt;&gt; a = numpy.array([1, 2, 3])&gt;&gt;&gt; t = torch.from_numpy(a)&gt;&gt;&gt; ttorch.LongTensor([1, 2, 3])&gt;&gt;&gt; t[0] = -1&gt;&gt;&gt; aarray([-1,  2,  3])</code></pre><blockquote><p>torch.linspace</p></blockquote><p>torch.linspace(start, end, steps=100, out=None) → Tensor<br>返回一个1维张量，包含在区间start 和 end 上均匀间隔的steps个点。 输出1维张量的长度为steps。</p><p>参数:</p><ul><li>start (float) – 序列的起始点</li><li>end (float) – 序列的最终值</li><li>steps (int) – 在start 和 end间生成的样本数  </li><li>out (Tensor, optional) – 结果张量  </li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; torch.linspace(3, 10, steps=5)3.00004.75006.50008.250010.0000[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.linspace(-10, 10, steps=5)-10-50510[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5)-10-50510[torch.FloatTensor of size 5]</code></pre><blockquote><p>torch.logspace</p></blockquote><p>torch.logspace(start, end, steps=100, out=None) → Tensor<br>返回一个1维张量，包含在区间 10start 和 10end上以对数刻度均匀间隔的steps个点。 输出1维张量的长度为steps。</p><p>参数:</p><ul><li>start (float) – 序列的起始点</li><li>end (float) – 序列的最终值</li><li>steps (int) – 在start 和 end间生成的样本数  </li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5)1.0000e-101.0000e-051.0000e+001.0000e+051.0000e+10[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5)1.25892.11353.54815.956610.0000[torch.FloatTensor of size 5]</code></pre><blockquote><p>torch.ones</p></blockquote><p>torch.ones(*sizes, out=None) → Tensor<br>返回一个全为1 的张量，形状由可变参数sizes定义。</p><p>参数:</p><ul><li>sizes (int…) – 整数序列，定义了输出形状  </li><li>out (Tensor, optional) – 结果张量  </li></ul><p>例子:  </p><pre><code>&gt;&gt;&gt; torch.ones(2, 3)1  1  11  1  1[torch.FloatTensor of size 2x3]&gt;&gt;&gt; torch.ones(5)11111[torch.FloatTensor of size 5]</code></pre><blockquote><p>torch.rand</p></blockquote><p>torch.rand(*sizes, out=None) → Tensor<br>返回一个张量，包含了从区间[0,1)的均匀分布中抽取的一组随机数，形状由可变参数sizes 定义。</p><p>参数:</p><ul><li>sizes (int…) – 整数序列，定义了输出形状  </li><li>out (Tensor, optinal) - 结果张量  </li></ul><p>例子：  </p><pre><code>&gt;&gt;&gt; torch.rand(4)0.91930.33470.32320.7715[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.rand(2, 3)0.5010  0.5140  0.07190.1435  0.5636  0.0538[torch.FloatTensor of size 2x3]</code></pre><blockquote><p>torch.randn</p></blockquote><p>torch.randn(*sizes, out=None) → Tensor<br>返回一个张量，包含了从标准正态分布(均值为0，方差为 1，即高斯白噪声)中抽取一组随机数，形状由可变参数sizes定义。  </p><p>参数:</p><ul><li>sizes (int…) – 整数序列，定义了输出形状</li><li>out (Tensor, optinal) - 结果张量  </li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; torch.randn(4)-0.11450.0094-1.17170.9846[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.randn(2, 3)1.4339  0.3351 -1.09991.5458 -0.9643 -0.3558[torch.FloatTensor of size 2x3]</code></pre><blockquote><p>torch.randperm</p></blockquote><p>torch.randperm(n, out=None) → LongTensor<br>给定参数n，返回一个从0 到n -1 的随机整数排列。</p><p>参数:</p><ul><li>n (int) – 上边界(不包含)  </li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; torch.randperm(4)2130[torch.LongTensor of size 4]</code></pre><blockquote><p>torch.arange</p></blockquote><p>torch.arange(start, end, step=1, out=None) → Tensor<br>返回一个1维张量，长度为 floor((end−start)/step)。包含从start到end，以step为步长的一组序列值(默认步长为1)。</p><p>参数:</p><ul><li>start (float) – 序列的起始点</li><li>end (float) – 序列的终止点</li><li>step (float) – 相邻点的间隔大小</li></ul><p>out (Tensor, optional) – 结果张量<br>例子：</p><pre><code>&gt;&gt;&gt; torch.arange(1, 4)123[torch.FloatTensor of size 3]&gt;&gt;&gt; torch.arange(1, 2.5, 0.5)1.00001.50002.0000[torch.FloatTensor of size 3]</code></pre><blockquote><p>torch.range</p></blockquote><p>torch.range(start, end, step=1, out=None) → Tensor<br>返回一个1维张量，有 floor((end−start)/step)+1 个元素。包含在半开区间[start, end）从start开始，以step为步长的一组值。 step 是两个值之间的间隔，即 xi+1=xi+step</p><font color="#ff0000" face="黑体">警告：建议使用函数 torch.arange()<br></font><br>参数:<br><br><em> start (float) – 序列的起始点</em> end (float) – 序列的最终值<br><em> step (int) – 相邻点的间隔大小<br><br>out (Tensor, optional) – 结果张量<br>例子：<br><br>    &gt;&gt;&gt; torch.range(1, 4)<br><br>    1<br>    2<br>    3<br>    4<br>    [torch.FloatTensor of size 4]<br><br>    &gt;&gt;&gt; torch.range(1, 4, 0.5)<br><br>    1.0000<br>    1.5000<br>    2.0000<br>    2.5000<br>    3.0000<br>    3.5000<br>    4.0000<br>    [torch.FloatTensor of size 7]<br><br>&gt; torch.zeros<br><br>torch.zeros(</em>sizes, out=None) → Tensor<br>返回一个全为标量 0 的张量，形状由可变参数sizes 定义。<br><br>参数:<br><br><em> sizes (int…) – 整数序列，定义了输出形状<br><br>( out (Tensor, optional) – 结果张量<br>例子：<br><br>    &gt;&gt;&gt; torch.zeros(2, 3)<br><br>    0  0  0<br>    0  0  0<br>    [torch.FloatTensor of size 2x3]<br><br>    &gt;&gt;&gt; torch.zeros(5)<br><br>    0<br>    0<br>    0<br>    0<br>    0<br>    [torch.FloatTensor of size 5]<br><br>#### 索引,切片,连接,换位Indexing, Slicing, Joining, Mutating Ops<br>—<br>&gt; torch.cat<br><br>torch.cat(inputs, dimension=0) → Tensor<br>在给定维度上对输入的张量序列seq 进行连接操作。<br><br>torch.cat()可以看做 torch.split() 和 torch.chunk()的反操作。 cat() 函数可以通过下面例子更好的理解。<br><br>参数:</em> inputs (sequence of Tensors) – 可以是任意相同Tensor 类型的python 序列<br><em> dimension (int, optional) – 沿着此维连接张量序列。<br><br>例子：<br><br>    &gt;&gt;&gt; x = torch.randn(2, 3)<br>    &gt;&gt;&gt; x<br><br>    0.5983 -0.0341  2.4918<br>    1.5981 -0.5265 -0.8735<br>    [torch.FloatTensor of size 2x3]<br><br>    &gt;&gt;&gt; torch.cat((x, x, x), 0)<br><br>    0.5983 -0.0341  2.4918<br>    1.5981 -0.5265 -0.8735<br>    0.5983 -0.0341  2.4918<br>    1.5981 -0.5265 -0.8735<br>    0.5983 -0.0341  2.4918<br>    1.5981 -0.5265 -0.8735<br>    [torch.FloatTensor of size 6x3]<br><br>    &gt;&gt;&gt; torch.cat((x, x, x), 1)<br><br>    0.5983 -0.0341  2.4918  0.5983 -0.0341  2.4918  0.5983 -0.0341  2.4918<br>    1.5981 -0.5265 -0.8735  1.5981 -0.5265 -0.8735  1.5981 -0.5265 -0.8735<br>    [torch.FloatTensor of size 2x9]<br><br>&gt;torch.chunk<br><br>torch.chunk(tensor, chunks, dim=0)<br>在给定维度(轴)上将输入张量进行分块儿。<br><br>参数:</em> tensor (Tensor) – 待分块的输入张量<br><em> chunks (int) – 分块的个数</em> dim (int) – 沿着此维度进行分块<br><br>&gt; torch.gather<br><br>torch.gather(input, dim, index, out=None) → Tensor<br>沿给定轴dim，将输入索引张量index指定位置的值进行聚合。<br><br>对一个3维张量，输出可以定义为：<br><br>    out[i][j][k] = tensor[index[i][j][k]][j][k]  # dim=0<br>    out[i][j][k] = tensor[i][index[i][j][k]][k]  # dim=1<br>    out[i][j][k] = tensor[i][j][index[i][j][k]]  # dim=3<br>例子：<br><br>    &gt;&gt;&gt; t = torch.Tensor([[1,2],[3,4]])<br>    &gt;&gt;&gt; torch.gather(t, 1, torch.LongTensor([[0,0],[1,0]]))<br>    1  1<br>    4  3<br>    [torch.FloatTensor of size 2x2]<br>参数:<br><br><em> input (Tensor) – 源张量</em> dim (int) – 索引的轴<br><em> index (LongTensor) – 聚合元素的下标</em> out (Tensor, optional) – 目标张量<br><br>&gt; torch.index_select<br><br>torch.index_select(input, dim, index, out=None) → Tensor<br>沿着指定维度对输入进行切片，取index中指定的相应项(index为一个LongTensor)，然后返回到一个新的张量， 返回的张量与原始张量<em>Tensor</em>有相同的维度(在指定轴上)。<br><br><font color="#ff0000" face="黑体">注意： 返回的张量不与原始张量共享内存空间。<br></font>  <p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 索引的轴</li><li>index (LongTensor) – 包含索引下标的一维张量</li><li>out (Tensor, optional) – 目标张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)&gt;&gt;&gt; x1.2045  2.4084  0.4001  1.13720.5596  1.5677  0.6219 -0.79541.3635 -1.2313 -0.5414 -1.8478[torch.FloatTensor of size 3x4]&gt;&gt;&gt; indices = torch.LongTensor([0, 2])&gt;&gt;&gt; torch.index_select(x, 0, indices)1.2045  2.4084  0.4001  1.13721.3635 -1.2313 -0.5414 -1.8478[torch.FloatTensor of size 2x4]&gt;&gt;&gt; torch.index_select(x, 1, indices)1.2045  0.40010.5596  0.62191.3635 -0.5414[torch.FloatTensor of size 3x2]</code></pre><blockquote><p>torch.masked_select</p></blockquote><p>torch.masked_select(input, mask, out=None) → Tensor<br>根据掩码张量mask中的二元值，取输入张量中的指定项( mask为一个 ByteTensor)，将取值返回到一个新的1D张量，</p><p>张量 mask须跟input张量有相同数量的元素数目，但形状或维度不需要相同。 注意： 返回的张量不与原始张量共享内存空间。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>mask (ByteTensor) – 掩码张量，包含了二元索引值</li><li>out (Tensor, optional) – 目标张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)&gt;&gt;&gt; x1.2045  2.4084  0.4001  1.13720.5596  1.5677  0.6219 -0.79541.3635 -1.2313 -0.5414 -1.8478[torch.FloatTensor of size 3x4]&gt;&gt;&gt; indices = torch.LongTensor([0, 2])&gt;&gt;&gt; torch.index_select(x, 0, indices)1.2045  2.4084  0.4001  1.13721.3635 -1.2313 -0.5414 -1.8478[torch.FloatTensor of size 2x4]&gt;&gt;&gt; torch.index_select(x, 1, indices)1.2045  0.40010.5596  0.62191.3635 -0.5414[torch.FloatTensor of size 3x2]</code></pre><blockquote><p>torch.nonzero</p></blockquote><p>torch.nonzero(input, out=None) → LongTensor<br>返回一个包含输入input中非零元素索引的张量。输出张量中的每行包含输入中非零元素的索引。</p><p>如果输入input有n维，则输出的索引张量output的形状为 z x n, 这里 z 是输入张量input中所有非零元素的个数。</p><p>参数:</p><ul><li>input (Tensor) – 源张量</li><li>out (LongTensor, optional) – 包含索引值的结果张量  </li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; torch.nonzero(torch.Tensor([1, 1, 1, 0, 1]))0124[torch.LongTensor of size 4x1]&gt;&gt;&gt; torch.nonzero(torch.Tensor([[0.6, 0.0, 0.0, 0.0],...                             [0.0, 0.4, 0.0, 0.0],...                             [0.0, 0.0, 1.2, 0.0],...                             [0.0, 0.0, 0.0,-0.4]]))0  01  12  23  3[torch.LongTensor of size 4x2]</code></pre><blockquote><p>torch.split</p></blockquote><p>torch.split(tensor, split_size, dim=0)<br>将输入张量分割成相等形状的chunks（如果可分）。 如果沿指定维的张量形状大小不能被split_size 整分， 则最后一个分块会小于其它分块。</p><p>参数:</p><ul><li>tensor (Tensor) – 待分割张量</li><li>split_size (int) – 单个分块的形状大小</li><li>dim (int) – 沿着此维进行分割</li></ul><blockquote><p>torch.squeeze</p></blockquote><p>torch.squeeze(input, dim=None, out=None)<br>将输入张量形状中的1 去除并返回。 如果输入是形如(A×1×B×1×C×1×D)，那么输出形状就为： (A×B×C×D)<br>当给定dim时，那么挤压操作只在给定维度上。例如，输入形状为: (A×1×B), squeeze(input, 0) 将会保持张量不变，只有用 squeeze(input, 1)，形状会变成 (A×B)。</p><font color="#ff0000" face="黑体">注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。<br></font>  <p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int, optional) – 如果给定，则input只会在给定维度挤压</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.zeros(2,1,2,1,2)&gt;&gt;&gt; x.size()(2L, 1L, 2L, 1L, 2L)&gt;&gt;&gt; y = torch.squeeze(x)&gt;&gt;&gt; y.size()(2L, 2L, 2L)&gt;&gt;&gt; y = torch.squeeze(x, 0)&gt;&gt;&gt; y.size()(2L, 1L, 2L, 1L, 2L)&gt;&gt;&gt; y = torch.squeeze(x, 1)&gt;&gt;&gt; y.size()(2L, 2L, 1L, 2L)</code></pre><blockquote><p>torch.stack[source]</p></blockquote><p>torch.stack(sequence, dim=0)<br>沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。</p><p>参数:</p><ul><li>sqequence (Sequence) – 待连接的张量序列</li><li>dim (int) – 插入的维度。必须介于 0 与 待连接的张量序列数之间。</li></ul><blockquote><p>torch.t</p></blockquote><p>torch.t(input, out=None) → Tensor<br>输入一个矩阵（2维张量），并转置0, 1维。 可以被视为函数transpose(input, 0, 1)的简写函数。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)&gt;&gt;&gt; x0.4834  0.6907  1.3417-0.1300  0.5295  0.2321[torch.FloatTensor of size 2x3]&gt;&gt;&gt; torch.t(x)0.4834 -0.13000.6907  0.52951.3417  0.2321[torch.FloatTensor of size 3x2]</code></pre><blockquote><p>torch.transpose</p></blockquote><p>torch.transpose(input, dim0, dim1, out=None) → Tensor<br>返回输入矩阵input的转置。交换维度dim0和dim1。 输出张量与输入张量共享内存，所以改变其中一个会导致另外一个也被修改。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>dim0 (int) – 转置的第一维</li><li><p>dim1 (int) – 转置的第二维</p><pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)&gt;&gt;&gt; x0.5983 -0.0341  2.49181.5981 -0.5265 -0.8735[torch.FloatTensor of size 2x3]&gt;&gt;&gt; torch.transpose(x, 0, 1)0.5983  1.5981-0.0341 -0.52652.4918 -0.8735[torch.FloatTensor of size 3x2]</code></pre></li></ul><blockquote><p>torch.unbind</p></blockquote><p>torch.unbind(tensor, dim=0)[source]<br>移除指定维后，返回一个元组，包含了沿着指定维切片后的各个切片</p><p>参数:</p><ul><li>tensor (Tensor) – 输入张量</li><li>dim (int) – 删除的维度</li></ul><blockquote><p>torch.unsqueeze</p></blockquote><p>torch.unsqueeze(input, dim, out=None)<br>返回一个新的张量，对输入的制定位置插入维度 1</p><font color="#ff0000" face="黑体">注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。<br></font>  <p>如果dim为负，则将会被转化dim+input.dim()+1</p><p>参数:  </p><ul><li>tensor (Tensor) – 输入张量</li><li>dim (int) – 插入维度的索引</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.Tensor([1, 2, 3, 4])&gt;&gt;&gt; torch.unsqueeze(x, 0)1  2  3  4[torch.FloatTensor of size 1x4]&gt;&gt;&gt; torch.unsqueeze(x, 1)1234[torch.FloatTensor of size 4x1]</code></pre><h4 id="随机抽样-Random-sampling"><a href="#随机抽样-Random-sampling" class="headerlink" title="随机抽样 Random sampling"></a>随机抽样 Random sampling</h4><hr><blockquote><p>torch.manual_seed</p></blockquote><p>torch.manual_seed(seed)<br>设定生成随机数的种子，并返回一个 torch._C.Generator 对象.</p><p>参数: </p><ul><li>seed (int or long) – 种子.</li></ul><blockquote><p>torch.initial_seed</p></blockquote><p>torch.initial_seed()<br>返回生成随机数的原始种子值（python long）。</p><blockquote><p>torch.get_rng_state</p></blockquote><p>torch.get_rng_state()[source]<br>返回随机生成器状态(ByteTensor)</p><blockquote><p>torch.set_rng_state</p></blockquote><p>torch.set_rng_state(new_state)[source]<br>设定随机生成器状态 参数: new_state (torch.ByteTensor) – 期望的状态</p><blockquote><p>torch.default_generator</p></blockquote><p>torch.default_generator = &lt;torch._C.Generator object&gt;</p><blockquote><p>torch.bernoulli</p></blockquote><p>torch.bernoulli(input, out=None) → Tensor<br>从伯努利分布中抽取二元随机数(0 或者 1)。</p><p>输入张量须包含用于抽取上述二元随机值的概率。 因此，输入中的所有值都必须在［0,1］区间，即 0&lt;=inputi&lt;=1<br>输出张量的第i个元素值， 将会以输入张量的第i个概率值等于1。</p><p>返回值将会是与输入相同大小的张量，每个值为0或者1 </p><p>参数:</p><ul><li>input (Tensor) – 输入为伯努利分布的概率值</li><li>out (Tensor, optional) – 输出张量(可选)  </li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.Tensor(3, 3).uniform_(0, 1) # generate a uniform random matrix with range [0, 1]&gt;&gt;&gt; a0.7544  0.8140  0.98420.5282  0.0595  0.64450.1925  0.9553  0.9732[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.bernoulli(a)1  1  10  0  10  1  1[torch.FloatTensor of size 3x3]&gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing &quot;1&quot; is 1&gt;&gt;&gt; torch.bernoulli(a)1  1  11  1  11  1  1[torch.FloatTensor of size 3x3]&gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing &quot;1&quot; is 0&gt;&gt;&gt; torch.bernoulli(a)0  0  00  0  00  0  0[torch.FloatTensor of size 3x3]</code></pre><blockquote><p>torch.multinomial</p></blockquote><p>torch.multinomial(input, num_samples,replacement=False, out=None) → LongTensor<br>返回一个张量，每行包含从input相应行中定义的多项分布中抽取的num_samples个样本。</p><font color="#ff0000" face="黑体"> [注意]:输入input每行的值不需要总和为1 (这里我们用来做权重)，但是必须非负且总和不能为0。<br></font><p>当抽取样本时，依次从左到右排列(第一个样本对应第一列)。</p><p>如果输入input是一个向量，输出out也是一个相同长度num_samples的向量。如果输入input是有 m行的矩阵，输出out是形如m×n的矩阵。</p><p>如果参数replacement 为 True, 则样本抽取可以重复。否则，一个样本在每行不能被重复抽取。</p><p>参数num_samples必须小于input长度(即，input的列数，如果是input是一个矩阵)。</p><p>参数:</p><ul><li>input (Tensor) – 包含概率值的张量</li><li>num_samples (int) – 抽取的样本数</li><li>replacement (bool, optional) – 布尔值，决定是否能重复抽取</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; weights = torch.Tensor([0, 10, 3, 0]) # create a Tensor of weights&gt;&gt;&gt; torch.multinomial(weights, 4)1200[torch.LongTensor of size 4]&gt;&gt;&gt; torch.multinomial(weights, 4, replacement=True)1212[torch.LongTensor of size 4]</code></pre><blockquote><p>torch.normal()</p></blockquote><p>torch.normal(means, std, out=None)<br>返回一个张量，包含从给定参数means,std的离散正态分布中抽取随机数。 均值means是一个张量，包含每个输出元素相关的正态分布的均值。 std是一个张量，包含每个输出元素相关的正态分布的标准差。 均值和标准差的形状不须匹配，但每个张量的元素个数须相同。</p><p>参数:</p><ul><li>means (Tensor) – 均值</li><li>std (Tensor) – 标准差</li><li>out (Tensor) – 可选的输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; torch.normal(means=torch.arange(1, 11), std=torch.arange(1, 0, -0.1))1.51041.69552.48954.91854.98956.91557.36838.18368.71649.8916[torch.FloatTensor of size 10]torch.normal(mean=0.0, std, out=None)</code></pre><p>与上面函数类似，所有抽取的样本共享均值。</p><p>参数:</p><ul><li>means (Tensor,optional) – 所有分布均值</li><li>std (Tensor) – 每个元素的标准差</li><li>out (Tensor) – 可选的输出张量</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; torch.normal(mean=0.5, std=torch.arange(1, 6))0.57230.0871-0.3783-2.568910.7893[torch.FloatTensor of size 5]</code></pre><p>torch.normal(means, std=1.0, out=None)<br>与上面函数类似，所有抽取的样本共享标准差。</p><p>参数:</p><ul><li>means (Tensor) – 每个元素的均值</li><li>std (float, optional) – 所有分布的标准差</li><li>out (Tensor) – 可选的输出张量</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; torch.normal(means=torch.arange(1, 6))1.16812.88843.77182.56164.2500[torch.FloatTensor of size 5]</code></pre><h4 id="序列化-Serialization"><a href="#序列化-Serialization" class="headerlink" title="序列化 Serialization"></a>序列化 Serialization</h4><hr><blockquote><p>torch.saves[source]</p></blockquote><p>torch.save(obj, f, pickle_module=<module 'pickle'="" from="" '="" home="" jenkins="" miniconda="" lib="" python3.5="" pickle.py'="">, pickle_protocol=2)<br>保存一个对象到一个硬盘文件上 参考: Recommended approach for saving a model </module></p><p>参数：</p><ul><li>obj – 保存对象</li><li>f － 类文件对象 (返回文件描述符)或一个保存文件名的字符串</li><li>pickle_module – 用于pickling元数据和对象的模块</li><li>pickle_protocol – 指定pickle protocal 可以覆盖默认参数</li></ul><blockquote><p>torch.load[source]</p></blockquote><p>torch.load(f, map_location=None, pickle_module=<module 'pickle'="" from="" '="" home="" jenkins="" miniconda="" lib="" python3.5="" pickle.py'="">)<br>从磁盘文件中读取一个通过torch.save()保存的对象。<br>torch.load() 可通过参数map_location 动态地进行内存重映射，使其能从不动设备中读取文件。一般调用时，需两个参数: storage 和 location tag. 返回不同地址中的storage，或着返回None (此时地址可以通过默认方法进行解析). 如果这个参数是字典的话，意味着其是从文件的地址标记到当前系统的地址标记的映射。 默认情况下， location tags中 “cpu”对应host tensors，‘cuda:device_id’ (e.g. ‘cuda:2’) 对应cuda tensors。 用户可以通过register_package进行扩展，使用自己定义的标记和反序列化方法。</module></p><p>参数:</p><ul><li>f – 类文件对象 (返回文件描述符)或一个保存文件名的字符串</li><li>map_location – 一个函数或字典规定如何remap存储位置</li><li>pickle_module – 用于unpickling元数据和对象的模块 (必须匹配序列化文件时的pickle_module )</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;)# Load all tensors onto the CPU&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location=lambda storage, loc: storage)# Map tensors from GPU 1 to GPU 0&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location={&apos;cuda:1&apos;:&apos;cuda:0&apos;})</code></pre><h4 id="并行化-Parallelism"><a href="#并行化-Parallelism" class="headerlink" title="并行化 Parallelism"></a>并行化 Parallelism</h4><hr><blockquote><p>torch.get_num_threads</p></blockquote><p>torch.get_num_threads() → int<br>获得用于并行化CPU操作的OpenMP线程数</p><blockquote><p>torch.set_num_threads</p></blockquote><p>torch.set_num_threads(int)<br>设定用于并行化CPU操作的OpenMP线程数</p><h4 id="数学操作Math-operations"><a href="#数学操作Math-operations" class="headerlink" title="数学操作Math operations"></a>数学操作Math operations</h4><hr><h5 id="Pointwise-Ops"><a href="#Pointwise-Ops" class="headerlink" title="Pointwise Ops"></a>Pointwise Ops</h5><blockquote><p>torch.abs</p></blockquote><p>torch.abs(input, out=None) → Tensor<br>计算输入张量的每个元素绝对值</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.abs(torch.FloatTensor([-1, -2, 3]))FloatTensor([1, 2, 3])</code></pre><p>torch.acos(input, out=None) → Tensor<br>torch.acos(input, out=None) → Tensor</p><p>返回一个新张量，包含输入张量每个元素的反余弦。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.acos(a)2.26081.29561.1075    nan[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.add()</p></blockquote><p>torch.add(input, value, out=None)<br>对输入张量input逐元素加上标量值value，并返回结果到一个新的张量out，即 out=tensor+value。</p><p>如果输入input是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】</p><ul><li>input (Tensor) – 输入张量</li><li>value (Number) – 添加到输入每个元素的数</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a0.4050-1.22271.8688-0.4185[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.add(a, 20)20.405018.777321.868819.5815[torch.FloatTensor of size 4]</code></pre><blockquote><blockquote><p>torch.add(input, value=1, other, out=None)</p></blockquote></blockquote><p>other 张量的每个元素乘以一个标量值value，并加到iput 张量上。返回结果到输出张量out。即，out=input+(other∗value)</p><p>两个张量 input and other的尺寸不需要匹配，但元素总数必须一样。</p><p>注意 :当两个张量形状不匹配时，输入张量的形状会作为输出张量的尺寸。</p><p>如果other是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】</p><p>参数:</p><ul><li>input (Tensor) – 第一个输入张量</li><li>value (Number) – 用于第二个张量的尺寸因子</li><li>other (Tensor) – 第二个输入张量</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; import torch&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.93102.03300.0852-0.2941[torch.FloatTensor of size 4]&gt;&gt;&gt; b = torch.randn(2, 2)&gt;&gt;&gt; b1.0663  0.2544-0.1513  0.0749[torch.FloatTensor of size 2x2]&gt;&gt;&gt; torch.add(a, 10, b)9.73224.5770-1.42790.4552[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.addcdiv</p></blockquote><p>torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None) → Tensor<br>用tensor2对tensor1逐元素相除，然后乘以标量值value 并加到tensor。</p><p>张量的形状不需要匹配，但元素数量必须一致。</p><p>如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。</p><p>参数：</p><ul><li>tensor (Tensor) – 张量，对 tensor1 ./ tensor 进行相加</li><li>value (Number, optional) – 标量，对 tensor1 ./ tensor2 进行相乘</li><li>tensor1 (Tensor) – 张量，作为被除数(分子)</li><li>tensor2 (Tensor) –张量，作为除数(分母)</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; t = torch.randn(2, 3)&gt;&gt;&gt; t1 = torch.randn(1, 6)&gt;&gt;&gt; t2 = torch.randn(6, 1)&gt;&gt;&gt; torch.addcdiv(t, 0.1, t1, t2)0.0122 -0.0188 -0.23540.7396 -1.5721  1.2878[torch.FloatTensor of size 2x3]</code></pre><blockquote><p>torch.addcmul</p></blockquote><p>torch.addcmul(tensor, value=1, tensor1, tensor2, out=None) → Tensor<br>用tensor2对tensor1逐元素相乘，并对结果乘以标量值value然后加到tensor。 张量的形状不需要匹配，但元素数量必须一致。 如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。</p><p>参数：</p><ul><li>tensor (Tensor) – 张量，对tensor1 ./ tensor 进行相加</li><li>value (Number, optional) – 标量，对 tensor1 . tensor2 进行相乘</li><li>tensor1 (Tensor) – 张量，作为乘子1</li><li>tensor2 (Tensor) –张量，作为乘子2</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; t = torch.randn(2, 3)&gt;&gt;&gt; t1 = torch.randn(1, 6)&gt;&gt;&gt; t2 = torch.randn(6, 1)&gt;&gt;&gt; torch.addcmul(t, 0.1, t1, t2)0.0122 -0.0188 -0.23540.7396 -1.5721  1.2878[torch.FloatTensor of size 2x3]</code></pre><blockquote><p>torch.asin</p></blockquote><p>torch.asin(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的反正弦函数</p><p>参数：</p><ul><li>tensor (Tensor) – 输入张量</li><li>nout (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.asin(a)-0.69000.27520.4633    nan[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.atan</p></blockquote><p>torch.atan(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的反正切函数</p><p>参数：</p><ul><li>tensor (Tensor) – 输入张量</li></ul><p>out (Tensor, optional) – 输出张量</p><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.atan(a)-0.56690.26530.42030.9196[torch.FloatTensor of size 4]</code></pre><ul><li>torch.atan2</li></ul><p>torch.atan2(input1, input2, out=None) → Tensor<br>返回一个新张量，包含两个输入张量input1和input2的反正切函数</p><p>参数：</p><ul><li>input1 (Tensor) – 第一个输入张量</li><li>input2 (Tensor) – 第二个输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.atan2(a, torch.randn(4))-2.41672.97550.93631.6613[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.ceil</p></blockquote><p>torch.ceil(input, out=None) → Tensor<br>天井函数，对输入input张量每个元素向上取整, 即取不小于每个元素的最小整数，并返回结果到输出。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.38690.3912-0.8634-0.5468[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.ceil(a)21-0-0[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.clamp</p></blockquote><p>torch.clamp(input, min, max, out=None) → Tensor<br>将输入input张量每个元素的夹紧到区间 $[min,max]$，并返回结果到一个新张量。</p><p>操作定义如下：</p><pre><code>      | min, if x_i &lt; miny_i = | x_i, if min &lt;= x_i &lt;= max      | max, if x_i &gt; max</code></pre><p>如果输入是FloatTensor or DoubleTensor类型，则参数min max 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，min， max取整数、实数皆可。】</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>min (Number) – 限制范围下限</li><li>max (Number) – 限制范围上限</li><li>Nout (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.38690.3912-0.8634-0.5468[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.clamp(a, min=-0.5, max=0.5)0.50000.3912-0.5000-0.5000[torch.FloatTensor of size 4]</code></pre><blockquote><blockquote><p>torch.clamp(input, *, min, out=None) → Tensor</p></blockquote></blockquote><p>将输入input张量每个元素的限制到不小于min ，并返回结果到一个新张量。</p><p>如果输入是FloatTensor or DoubleTensor类型，则参数 min 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，min取整数、实数皆可。】</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>value (Number) – 限制范围下限</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.38690.3912-0.8634-0.5468[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.clamp(a, min=0.5)1.38690.50000.50000.5000[torch.FloatTensor of size 4]</code></pre><blockquote><blockquote><p>torch.clamp(input, *, max, out=None) → Tensor</p></blockquote></blockquote><p>将输入input张量每个元素的限制到不大于max ，并返回结果到一个新张量。</p><p>如果输入是FloatTensor or DoubleTensor类型，则参数 max 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，max取整数、实数皆可。】</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>value (Number) – 限制范围上限</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.38690.3912-0.8634-0.5468[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.clamp(a, max=0.5)0.50000.3912-0.8634-0.5468[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.cos</p></blockquote><p>torch.cos(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的余弦。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.cos(a)0.80410.96330.90180.2557[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.cosh</p></blockquote><p>torch.cosh(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的双曲余弦。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.cosh(a)1.20951.03721.10151.9917[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.div()</p></blockquote><p>torch.div(input, value, out=None)<br>将input逐元素除以标量值value，并返回结果到输出张量out。 即 out=tensor/value<br>如果输入是FloatTensor or DoubleTensor类型，则参数 value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>value (Number) – 除数</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(5)&gt;&gt;&gt; a-0.6147-1.1237-0.1604-0.68530.1063[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.div(a, 0.5)-1.2294-2.2474-0.3208-1.37060.2126[torch.FloatTensor of size 5]</code></pre><blockquote><blockquote><p>torch.div(input, other, out=None)</p></blockquote></blockquote><p>两张量input和other逐元素相除，并将结果返回到输出。即， outi=inputi/otheri<br>两张量形状不须匹配，但元素数须一致。</p><font color="#ff0000" face="黑体">注意：当形状不匹配时，input的形状作为输出张量的形状。<br></font><p>参数：</p><ul><li>input (Tensor) – 张量(分子)</li><li>other (Tensor) – 张量(分母)</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4,4)&gt;&gt;&gt; a-0.1810  0.4017  0.2863 -0.10130.6183  2.0696  0.9012 -1.59330.5679  0.4743 -0.0117 -0.1266-0.1213  0.9629  0.2682  1.5968[torch.FloatTensor of size 4x4]&gt;&gt;&gt; b = torch.randn(8, 2)&gt;&gt;&gt; b0.8774  0.76500.8866  1.4805-0.6490  1.11721.4259 -0.81461.4633 -0.12280.4643 -0.60290.3492  1.52701.6103 -0.6291[torch.FloatTensor of size 8x2]&gt;&gt;&gt; torch.div(a, b)-0.2062  0.5251  0.3229 -0.0684-0.9528  1.8525  0.6320  1.95590.3881 -3.8625 -0.0253  0.2099-0.3473  0.6306  0.1666 -2.5381[torch.FloatTensor of size 4x4]</code></pre><blockquote><p>torch.exp</p></blockquote><p>torch.exp(tensor, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的指数。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li></ul><p>out (Tensor, optional) – 输出张量</p><pre><code>&gt;&gt;&gt; torch.exp(torch.Tensor([0, math.log(2)]))torch.FloatTensor([1, 2])</code></pre><blockquote><p>torch.floor</p></blockquote><p>torch.floor(input, out=None) → Tensor<br>床函数: 返回一个新张量，包含输入input张量每个元素的floor，即不小于元素的最大整数。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.38690.3912-0.8634-0.5468[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.floor(a)10-1-1[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.fmod</p></blockquote><p>torch.fmod(input, divisor, out=None) → Tensor<br>计算除法余数。 除数与被除数可能同时含有整数和浮点数。此时，余数的正负与被除数相同。</p><p>参数：</p><ul><li>input (Tensor) – 被除数</li><li>divisor (Tensor or float) – 除数，一个数或与被除数相同类型的张量  </li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; torch.fmod(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2)torch.FloatTensor([-1, -0, -1, 1, 0, 1])&gt;&gt;&gt; torch.fmod(torch.Tensor([1, 2, 3, 4, 5]), 1.5)torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5])</code></pre><p>参考: torch.remainder(), 计算逐元素余数， 相当于python 中的 % 操作符。</p><blockquote><p>torch.frac</p></blockquote><p>torch.frac(tensor, out=None) → Tensor<br>返回每个元素的分数部分。</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.frac(torch.Tensor([1, 2.5, -3.2])torch.FloatTensor([0, 0.5, -0.2])</code></pre><blockquote><p>torch.lerp</p></blockquote><p>torch.lerp(start, end, weight, out=None)<br>对两个张量以start，end做线性插值， 将结果返回到输出张量。<br>即，outi=starti+weight∗(endi−starti)</p><p>参数：</p><ul><li>start (Tensor) – 起始点张量</li><li>end (Tensor) – 终止点张量</li><li>weight (float) – 插值公式的weight</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; start = torch.arange(1, 5)&gt;&gt;&gt; end = torch.Tensor(4).fill_(10)&gt;&gt;&gt; start1234[torch.FloatTensor of size 4]&gt;&gt;&gt; end10101010[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.lerp(start, end, 0.5)5.50006.00006.50007.0000[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.log</p></blockquote><p>torch.log(input, out=None) → Tensor<br>计算input 的自然对数</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(5)&gt;&gt;&gt; a-0.41830.3722-0.30910.41490.5857[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.log(a)    nan-0.9883    nan-0.8797-0.5349[torch.FloatTensor of size 5]</code></pre><blockquote><p>torch.log1p</p></blockquote><p>torch.log1p(input, out=None) → Tensor<br>计算 input+1的自然对数 yi=log(xi+1)  </p><font color="#ff0000" face="黑体">注意：对值比较小的输入，此函数比torch.log()更准确。<br></font><p>如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(5)&gt;&gt;&gt; a-0.41830.3722-0.30910.41490.5857[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.log1p(a)-0.54180.3164-0.36970.34710.4611[torch.FloatTensor of size 5]</code></pre><blockquote><p>torch.mul</p></blockquote><p>torch.mul(input, value, out=None)<br>用标量值value乘以输入input的每个元素，并返回一个新的结果张量。 out=tensor∗value<br>如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>value (Number) – 乘到每个元素的数</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(3)&gt;&gt;&gt; a-0.9374-0.5254-0.6069[torch.FloatTensor of size 3]&gt;&gt;&gt; torch.mul(a, 100)-93.7411-52.5374-60.6908[torch.FloatTensor of size 3]</code></pre><blockquote><blockquote><p>torch.mul(input, other, out=None)</p></blockquote></blockquote><p>两个张量input,other按元素进行相乘，并返回到输出张量。即计算outi=inputi∗otheri<br>两计算张量形状不须匹配，但总元素数须一致。 注意：当形状不匹配时，input的形状作为输入张量的形状。</p><p>参数：</p><ul><li>input (Tensor) – 第一个相乘张量</li><li>other (Tensor) – 第二个相乘张量</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4,4)&gt;&gt;&gt; a-0.7280  0.0598 -1.4327 -0.5825-0.1427 -0.0690  0.0821 -0.3270-0.9241  0.5110  0.4070 -1.1188-0.8308  0.7426 -0.6240 -1.1582[torch.FloatTensor of size 4x4]&gt;&gt;&gt; b = torch.randn(2, 8)&gt;&gt;&gt; b0.0430 -1.0775  0.6015  1.1647 -0.6549  0.0308 -0.1670  1.0742-1.2593  0.0292 -0.0849  0.4530  1.2404 -0.4659 -0.1840  0.5974[torch.FloatTensor of size 2x8]&gt;&gt;&gt; torch.mul(a, b)-0.0313 -0.0645 -0.8618 -0.67840.0934 -0.0021 -0.0137 -0.35131.1638  0.0149 -0.0346 -0.5068-1.0304 -0.3460  0.1148 -0.6919[torch.FloatTensor of size 4x4]</code></pre><blockquote><p>torch.neg</p></blockquote><p>torch.neg(input, out=None) → Tensor<br>返回一个新张量，包含输入input 张量按元素取负。 即， out=−1∗input</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(5)&gt;&gt;&gt; a-0.44301.1690-0.8836-0.45650.2968[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.neg(a)0.4430-1.16900.88360.4565-0.2968[torch.FloatTensor of size 5]</code></pre><blockquote><p>torch.pow</p></blockquote><p>torch.pow(input, exponent, out=None)<br>对输入input的按元素求exponent次幂值，并返回结果张量。 幂值exponent 可以为单一 float 数或者与input相同元素数的张量。</p><p>当幂值为标量时，执行操作：<br>outi=xexponent  </p><p>当幂值为张量时，执行操作：<br>outi=xexponenti</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>exponent (float or Tensor) – 幂值</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.5274-0.8232-2.11281.7558[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.pow(a, 2)0.27810.67764.46403.0829[torch.FloatTensor of size 4]&gt;&gt;&gt; exp = torch.arange(1, 5)&gt;&gt;&gt; a = torch.arange(1, 5)&gt;&gt;&gt; a1234[torch.FloatTensor of size 4]&gt;&gt;&gt; exp1234[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.pow(a, exp)1427256[torch.FloatTensor of size 4]</code></pre><blockquote><blockquote><p>torch.pow(base, input, out=None)</p></blockquote></blockquote><p>base 为标量浮点值,input为张量， 返回的输出张量 out 与输入张量相同形状。</p><p>执行操作为:<br>outi=baseinputi</p><p>参数：</p><ul><li>base (float) – 标量值，指数的底</li><li>input ( Tensor) – 幂值</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; exp = torch.arange(1, 5)&gt;&gt;&gt; base = 2&gt;&gt;&gt; torch.pow(base, exp)24816[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.reciprocal</p></blockquote><p>torch.reciprocal(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的倒数，即 1.0/x。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.38690.3912-0.8634-0.5468[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.reciprocal(a)0.72102.5565-1.1583-1.8289[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.remainder</p></blockquote><p>torch.remainder(input, divisor, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的除法余数。 除数与被除数可能同时包含整数或浮点数。余数与除数有相同的符号。</p><p>参数：</p><ul><li>input (Tensor) – 被除数</li><li>divisor (Tensor or float) – 除数，一个数或者与除数相同大小的张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; torch.remainder(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2)torch.FloatTensor([1, 0, 1, 1, 0, 1])&gt;&gt;&gt; torch.remainder(torch.Tensor([1, 2, 3, 4, 5]), 1.5)torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5])</code></pre><p>参考: 函数torch.fmod() 同样可以计算除法余数，相当于 C 的 库函数fmod()</p><ul><li>torch.round</li></ul><p>torch.round(input, out=None) → Tensor<br>返回一个新张量，将输入input张量每个元素舍入到最近的整数。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.22901.3409-0.5662-0.0899[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.round(a)11-1-0[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.rsqrt</p></blockquote><p>torch.rsqrt(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的平方根倒数。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.22901.3409-0.5662-0.0899[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.rsqrt(a)0.90200.8636    nan    nan[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.sigmoid</p></blockquote><p>torch.sigmoid(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的sigmoid值。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.49721.35120.1056-0.2650[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.sigmoid(a)0.37820.79430.52640.4341[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.sign</p></blockquote><p>torch.sign(input, out=None) → Tensor<br>符号函数：返回一个新张量，包含输入input张量每个元素的正负。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.sign(a)-1111[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.sin</p></blockquote><p>torch.sin(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的正弦。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.sin(a)-0.59440.26840.43220.9667[torch.FloatTensor of size 4]</code></pre><ul><li>torch.sinh</li></ul><p>torch.sinh(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的双曲正弦。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.sinh(a)-0.68040.27510.46191.7225[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.sqrt</p></blockquote><p>torch.sqrt(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的平方根。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.22901.3409-0.5662-0.0899[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.sqrt(a)1.10861.1580    nan    nan[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.tan</p></blockquote><p>torch.tan(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的正切。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.tan(a)-0.73920.27860.47923.7801[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.tanh</p></blockquote><p>torch.tanh(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的双曲正切。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.tanh(a)-0.56250.26530.41930.8648[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.trunc</p></blockquote><p>torch.trunc(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的截断值(标量x的截断值是最接近其的整数，其比x更接近零。简而言之，有符号数的小数部分被舍弃)。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.49721.35120.1056-0.2650[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.trunc(a)-010-0[torch.FloatTensor of size 4]</code></pre><h5 id="Reduction-Ops"><a href="#Reduction-Ops" class="headerlink" title="Reduction Ops"></a>Reduction Ops</h5><blockquote><p>torch.cumprod</p></blockquote><p>torch.cumprod(input, dim, out=None) → Tensor<br>返回输入沿指定维度的累积积。例如，如果输入是一个N 元向量，则结果也是一个N 元向量，第i 个输出元素值为$ yi=x1∗x2∗x3∗…∗xi $</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 累积积操作的维度</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(10)&gt;&gt;&gt; a1.11481.84231.4143-0.44031.2859-1.2514-0.47481.1735-1.6332-0.4272[torch.FloatTensor of size 10]&gt;&gt;&gt; torch.cumprod(a, dim=0)1.11482.05372.9045-1.2788-1.64442.0578-0.9770-1.14661.8726-0.8000[torch.FloatTensor of size 10]&gt;&gt;&gt; a[5] = 0.0&gt;&gt;&gt; torch.cumprod(a, dim=0)1.11482.05372.9045-1.2788-1.6444-0.00000.00000.0000-0.00000.0000[torch.FloatTensor of size 10]</code></pre><blockquote><p>torch.cumsum</p></blockquote><p>torch.cumsum(input, dim, out=None) → Tensor<br>返回输入沿指定维度的累积和。例如，如果输入是一个N元向量，则结果也是一个N元向量，第i 个输出元素值为 yi=x1+x2+x3+…+xi</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 累积和操作的维度</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(10)&gt;&gt;&gt; a-0.6039-0.2214-0.3705-0.01691.3415-0.12300.97190.6081-0.12861.0947[torch.FloatTensor of size 10]&gt;&gt;&gt; torch.cumsum(a, dim=0)-0.6039-0.8253-1.1958-1.21270.12880.00580.97771.58581.45722.5519[torch.FloatTensor of size 10]</code></pre><blockquote><p>torch.dist</p></blockquote><p>torch.dist(input, other, p=2, out=None) → Tensor<br>返回 (input - other) 的 p范数 。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>other (Tensor) – 右侧输入张量</li><li>p (float, optional) – 所计算的范数</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.randn(4)&gt;&gt;&gt; x0.2505-0.4571-0.37330.7807[torch.FloatTensor of size 4]&gt;&gt;&gt; y = torch.randn(4)&gt;&gt;&gt; y0.7782-0.51851.4106-2.4063[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.dist(x, y, 3.5)3.302832063224223&gt;&gt;&gt; torch.dist(x, y, 3)3.3677282206393286&gt;&gt;&gt; torch.dist(x, y, 0)inf&gt;&gt;&gt; torch.dist(x, y, 1)5.560028076171875</code></pre><blockquote><p>torch.mean</p></blockquote><p>torch.mean(input) → float<br>返回输入张量所有元素的均值。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; a-0.2946 -0.9143  2.1809[torch.FloatTensor of size 1x3]&gt;&gt;&gt; torch.mean(a)0.32398951053619385</code></pre><blockquote><blockquote><p>torch.mean(input, dim, out=None) → Tensor</p></blockquote></blockquote><p>返回输入张量给定维度dim上每行的均值。</p><p>输出形状与输入相同，除了给定维度上为1.</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – the dimension to reduce</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; a-1.2738 -0.3058  0.1230 -1.96150.8771 -0.5430 -0.9233  0.98791.4107  0.0317 -0.6823  0.2255-1.3854  0.4953 -0.2160  0.2435[torch.FloatTensor of size 4x4]&gt;&gt;&gt; torch.mean(a, 1)-0.85450.09970.2464-0.2157[torch.FloatTensor of size 4x1]</code></pre><blockquote><p>torch.median</p></blockquote><p>torch.median(input, dim=-1, values=None, indices=None) -&gt; (Tensor, LongTensor)<br>返回输入张量给定维度每行的中位数，同时返回一个包含中位数的索引的LongTensor。</p><p>dim值默认为输入张量的最后一维。 输出形状与输入相同，除了给定维度上为1.</p><font color="#ff0000" face="黑体">注意: 这个函数还没有在torch.cuda.Tensor中定义<br></font><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 缩减的维度</li><li>values (Tensor, optional) – 结果张量</li><li>indices (Tensor, optional) – 返回的索引结果张量  </li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a-0.6891 -0.66620.2697  0.74120.5254 -0.74020.5528 -0.2399[torch.FloatTensor of size 4x2]&gt;&gt;&gt; a = torch.randn(4, 5)&gt;&gt;&gt; a0.4056 -0.3372  1.0973 -2.4884  0.43342.1336  0.3841  0.1404 -0.1821 -0.7646-0.2403  1.3975 -2.0068  0.1298  0.0212-1.5371 -0.7257 -0.4871 -0.2359 -1.1724[torch.FloatTensor of size 4x5]&gt;&gt;&gt; torch.median(a, 1)(0.40560.14040.0212-0.7257[torch.FloatTensor of size 4x1],0241[torch.LongTensor of size 4x1])</code></pre><blockquote><p>torch.mode</p></blockquote><p>torch.mode(input, dim=-1, values=None, indices=None) -&gt; (Tensor, LongTensor)<br>返回给定维dim上，每行的众数值。 同时返回一个LongTensor，包含众数职的索引。dim值默认为输入张量的最后一维。</p><p>输出形状与输入相同，除了给定维度上为1.</p><font color="#ff0000" face="黑体">注意: 这个函数还没有在torch.cuda.Tensor中定义<br></font><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 缩减的维度</li><li>values (Tensor, optional) – 结果张量</li><li>indices (Tensor, optional) – 返回的索引张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a-0.6891 -0.66620.2697  0.74120.5254 -0.74020.5528 -0.2399[torch.FloatTensor of size 4x2]&gt;&gt;&gt; a = torch.randn(4, 5)&gt;&gt;&gt; a0.4056 -0.3372  1.0973 -2.4884  0.43342.1336  0.3841  0.1404 -0.1821 -0.7646-0.2403  1.3975 -2.0068  0.1298  0.0212-1.5371 -0.7257 -0.4871 -0.2359 -1.1724[torch.FloatTensor of size 4x5]&gt;&gt;&gt; torch.mode(a, 1)(-2.4884-0.7646-2.0068-1.5371[torch.FloatTensor of size 4x1],3420[torch.LongTensor of size 4x1])</code></pre><blockquote><p>torch.norm</p></blockquote><p>torch.norm(input, p=2) → float<br>返回输入张量input 的p 范数。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>p (float,optional) – 范数计算中的幂指数值</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; a-0.4376 -0.5328  0.9547[torch.FloatTensor of size 1x3]&gt;&gt;&gt; torch.norm(a, 3)1.0338925067372466</code></pre><blockquote><blockquote><p>torch.norm(input, p, dim, out=None) → Tensor</p></blockquote></blockquote><p>返回输入张量给定维dim 上每行的p 范数。 输出形状与输入相同，除了给定维度上为1.</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>p (float) – 范数计算中的幂指数值</li><li>dim (int) – 缩减的维度</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4, 2)&gt;&gt;&gt; a-0.6891 -0.66620.2697  0.74120.5254 -0.74020.5528 -0.2399[torch.FloatTensor of size 4x2]&gt;&gt;&gt; torch.norm(a, 2, 1)0.95850.78880.90770.6026[torch.FloatTensor of size 4x1]&gt;&gt;&gt; torch.norm(a, 0, 1)2222[torch.FloatTensor of size 4x1]</code></pre><blockquote><p>torch.prod</p></blockquote><p>torch.prod(input) → float<br>返回输入张量input 所有元素的积。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; a0.6170  0.3546  0.0253[torch.FloatTensor of size 1x3]&gt;&gt;&gt; torch.prod(a)0.005537458061418483</code></pre><blockquote><blockquote><p>orch.prod(input, dim, out=None) → Tensor</p></blockquote></blockquote><p>返回输入张量给定维度上每行的积。 输出形状与输入相同，除了给定维度上为1.</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 缩减的维度</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4, 2)&gt;&gt;&gt; a0.1598 -0.6884-0.1831 -0.4412-0.9925 -0.6244-0.2416 -0.8080[torch.FloatTensor of size 4x2]&gt;&gt;&gt; torch.prod(a, 1)-0.11000.08080.61970.1952[torch.FloatTensor of size 4x1]</code></pre><blockquote><p>torch.std</p></blockquote><p>torch.std(input) → float<br>返回输入张量input 所有元素的标准差。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; a-1.3063  1.4182 -0.3061[torch.FloatTensor of size 1x3]&gt;&gt;&gt; torch.std(a)1.3782334731508061</code></pre><blockquote><blockquote><p>torch.std(input, dim, out=None) → Tensor</p></blockquote></blockquote><p>返回输入张量给定维度上每行的标准差。 输出形状与输入相同，除了给定维度上为1.</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 缩减的维度</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; a0.1889 -2.4856  0.0043  1.8169-0.7701 -0.4682 -2.2410  0.40980.1919 -1.1856 -1.0361  0.90850.0173  1.0662  0.2143 -0.5576[torch.FloatTensor of size 4x4]&gt;&gt;&gt; torch.std(a, dim=1)1.77561.10251.00450.6725[torch.FloatTensor of size 4x1]</code></pre><blockquote><p>torch.sum</p></blockquote><p>torch.sum(input) → float<br>返回输入张量input 所有元素的和。</p><p>输出形状与输入相同，除了给定维度上为1.</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; a0.6170  0.3546  0.0253[torch.FloatTensor of size 1x3]&gt;&gt;&gt; torch.sum(a)0.9969287421554327</code></pre><blockquote><blockquote><p>torch.sum(input, dim, out=None) → Tensor</p></blockquote></blockquote><p>返回输入张量给定维度上每行的和。 输出形状与输入相同，除了给定维度上为1.</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 缩减的维度</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; a-0.4640  0.0609  0.1122  0.4784-1.3063  1.6443  0.4714 -0.7396-1.3561 -0.1959  1.0609 -1.98552.6833  0.5746 -0.5709 -0.4430[torch.FloatTensor of size 4x4]&gt;&gt;&gt; torch.sum(a, 1)0.18740.0698-2.47672.2440[torch.FloatTensor of size 4x1]</code></pre><blockquote><p>torch.var</p></blockquote><p>torch.var(input) → float<br>返回输入张量所有元素的方差</p><p>输出形状与输入相同，除了给定维度上为1.</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; a-1.3063  1.4182 -0.3061[torch.FloatTensor of size 1x3]&gt;&gt;&gt; torch.var(a)1.899527506513334</code></pre><blockquote><blockquote><p>torch.var(input, dim, out=None) → Tensor</p></blockquote></blockquote><p>返回输入张量给定维度上每行的方差。 输出形状与输入相同，除了给定维度上为1.</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – the dimension to reduce</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; a-1.2738 -0.3058  0.1230 -1.96150.8771 -0.5430 -0.9233  0.98791.4107  0.0317 -0.6823  0.2255-1.3854  0.4953 -0.2160  0.2435[torch.FloatTensor of size 4x4]&gt;&gt;&gt; torch.var(a, 1)0.88590.95090.75480.6949[torch.FloatTensor of size 4x1]</code></pre><h4 id="比较操作-Comparison-Ops"><a href="#比较操作-Comparison-Ops" class="headerlink" title="比较操作 Comparison Ops"></a>比较操作 Comparison Ops</h4><hr><blockquote><p>torch.eq</p></blockquote><p>torch.eq(input, other, out=None) → Tensor<br>比较元素相等性。第二个参数可为一个数或与第一个参数同类型形状的张量。</p><p>参数：</p><ul><li>input (Tensor) – 待比较张量</li><li>other (Tensor or float) – 比较张量或数</li><li>out (Tensor, optional) – 输出张量，须为 ByteTensor类型 or 与input同类型</li></ul><p>返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(相等为1，不等为0 )</p><p>返回类型： Tensor</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.eq(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))1  00  1[torch.ByteTensor of size 2x2]</code></pre><blockquote><p>torch.equal</p></blockquote><p>torch.equal(tensor1, tensor2) → bool<br>如果两个张量有相同的形状和元素值，则返回True ，否则 False。</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.equal(torch.Tensor([1, 2]), torch.Tensor([1, 2]))True</code></pre><blockquote><p>torch.ge</p></blockquote><p>torch.ge(input, other, out=None) → Tensor<br>逐元素比较input和other，即是否 input&gt;=other。</p><p>如果两个张量有相同的形状和元素值，则返回True ，否则 False。 第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p><p>参数:</p><ul><li>input (Tensor) – 待对比的张量</li><li>other (Tensor or float) – 对比的张量或float值</li><li>out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。<br>返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;= other )。  </li></ul><p>返回类型： Tensor</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.ge(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))1  10  1[torch.ByteTensor of size 2x2]</code></pre><blockquote><p>torch.gt</p></blockquote><p>torch.gt(input, other, out=None) → Tensor<br>逐元素比较input和other ， 即是否input&gt;other 如果两个张量有相同的形状和元素值，则返回True ，否则 False。 第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p><p>参数:</p><ul><li>input (Tensor) – 要对比的张量</li><li>other (Tensor or float) – 要对比的张量或float值</li><li>out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。<br>返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;= other )。 返回类型： Tensor</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; torch.gt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))0  10  0[torch.ByteTensor of size 2x2]</code></pre><blockquote><p>torch.kthvalue</p></blockquote><p>torch.kthvalue(input, k, dim=None, out=None) -&gt; (Tensor, LongTensor)<br>取输入张量input指定维上第k 个最小值。如果不指定dim，则默认为input的最后一维。</p><p>返回一个元组 (values,indices)，其中indices是原始输入张量input中沿dim维的第 k 个最小值下标。</p><p>参数:</p><ul><li>input (Tensor) – 要对比的张量</li><li>k (int) – 第 k 个最小值</li><li>dim (int, optional) – 沿着此维进行排序</li><li>out (tuple, optional) – 输出元组 (Tensor, LongTensor) 可选地给定作为 输出 buffers</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.arange(1, 6)&gt;&gt;&gt; x12345[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.kthvalue(x, 4)(4[torch.FloatTensor of size 1],3[torch.LongTensor of size 1])</code></pre><blockquote><p>torch.le</p></blockquote><p>torch.le(input, other, out=None) → Tensor<br>逐元素比较input和other ， 即是否input&lt;=other 第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p><p>参数:</p><ul><li>input (Tensor) – 要对比的张量</li><li>other (Tensor or float ) – 对比的张量或float值</li><li>out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。<br>返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;= other )。</li></ul><p>返回类型： Tensor</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.le(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))1  01  1[torch.ByteTensor of size 2x2]</code></pre><blockquote><p>torch.lt</p></blockquote><p>torch.lt(input, other, out=None) → Tensor<br>逐元素比较input和other ， 即是否 input&lt;other</p><p>第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p><p>参数:</p><ul><li>input (Tensor) – 要对比的张量</li><li>other (Tensor or float ) – 对比的张量或float值</li><li>out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。</li></ul><p>input： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 tensor &gt;= other )。 </p><p>返回类型： Tensor</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.lt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))0  01  0[torch.ByteTensor of size 2x2]</code></pre><blockquote><p>torch.max</p></blockquote><p>torch.max()<br>返回输入张量所有元素的最大值。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; a0.4729 -0.2266 -0.2085[torch.FloatTensor of size 1x3]&gt;&gt;&gt; torch.max(a)0.4729</code></pre><blockquote><blockquote><p>torch.max(input, dim, max=None, max_indices=None) -&gt; (Tensor, LongTensor)</p></blockquote></blockquote><p>返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引。</p><p>输出形状中，将dim维设定为1，其它与输入形状保持一致。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 指定的维度</li><li>max (Tensor, optional) – 结果张量，包含给定维度上的最大值</li><li>max_indices (LongTensor, optional) – 结果张量，包含给定维度上每个最大值的位置索引</li></ul><p>例子：</p><pre><code>&gt;&gt; a = torch.randn(4, 4)&gt;&gt; a0.0692  0.3142  1.2513 -0.54280.9288  0.8552 -0.2073  0.64091.0695 -0.0101 -2.4507 -1.22300.7426 -0.7666  0.4862 -0.6628torch.FloatTensor of size 4x4]&gt;&gt;&gt; torch.max(a, 1)(1.25130.92881.06950.7426[torch.FloatTensor of size 4x1],2000[torch.LongTensor of size 4x1])</code></pre><blockquote><p>torch.max(input, other, out=None) → Tensor</p></blockquote><p>返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引。 即，outi=max(inputi,otheri)<br>输出形状中，将dim维设定为1，其它与输入形状保持一致。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>other (Tensor) – 输出张量</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.38690.3912-0.8634-0.5468[torch.FloatTensor of size 4]&gt;&gt;&gt; b = torch.randn(4)&gt;&gt;&gt; b1.0067-0.80100.62580.3627[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.max(a, b)1.38690.39120.62580.3627[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.min</p></blockquote><p>torch.min(input) → float<br>返回输入张量所有元素的最小值。</p><p>参数: </p><ul><li>input (Tensor) – 输入张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; a0.4729 -0.2266 -0.2085[torch.FloatTensor of size 1x3]&gt;&gt;&gt; torch.min(a)-0.22663167119026184</code></pre><blockquote><blockquote><p>torch.min(input, dim, min=None, min_indices=None) -&gt; (Tensor, LongTensor)</p></blockquote></blockquote><p>返回输入张量给定维度上每行的最小值，并同时返回每个最小值的位置索引。</p><p>输出形状中，将dim维设定为1，其它与输入形状保持一致。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 指定的维度</li><li>min (Tensor, optional) – 结果张量，包含给定维度上的最小值</li><li>min_indices (LongTensor, optional) – 结果张量，包含给定维度上每个最小值的位置索引</li></ul><p>例子：</p><pre><code>&gt;&gt; a = torch.randn(4, 4)&gt;&gt; a0.0692  0.3142  1.2513 -0.54280.9288  0.8552 -0.2073  0.64091.0695 -0.0101 -2.4507 -1.22300.7426 -0.7666  0.4862 -0.6628torch.FloatTensor of size 4x4]&gt;&gt; torch.min(a, 1)0.54280.20732.45070.7666torch.FloatTensor of size 4x1]3221torch.LongTensor of size 4x1]</code></pre><blockquote><blockquote><p>torch.min(input, other, out=None) → Tensor</p></blockquote></blockquote><p>input中逐元素与other相应位置的元素对比，返回最小值到输出张量。即，outi=min(tensori,otheri)<br>两张量形状不需匹配，但元素数须相同。</p><font color="#ff0000" face="黑体">注意：当形状不匹配时，input的形状作为返回张量的形状。<br></font><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>other (Tensor) – 第二个输入张量</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.38690.3912-0.8634-0.5468[torch.FloatTensor of size 4]&gt;&gt;&gt; b = torch.randn(4)&gt;&gt;&gt; b1.0067-0.80100.62580.3627[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.min(a, b)1.0067-0.8010-0.8634-0.5468[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.ne</p></blockquote><p>torch.ne(input, other, out=None) → Tensor<br>逐元素比较input和other ， 即是否 input!=other。 第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p><p>参数:</p><ul><li>input (Tensor) – 待对比的张量</li><li>other (Tensor or float) – 对比的张量或float值</li><li>out (Tensor, optional) – 输出张量。必须为ByteTensor或者与input相同类型。</li></ul><p>返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果 (如果 tensor != other 为True ，返回1)。</p><p>返回类型： Tensor</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.ne(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))0  11  0[torch.ByteTensor of size 2x2]</code></pre><blockquote><p>torch.sort</p></blockquote><p>torch.sort(input, dim=None, descending=False, out=None) -&gt; (Tensor, LongTensor)<br>对输入张量input沿着指定维按升序排序。如果不给定dim，则默认为输入的最后一维。如果指定参数descending为True，则按降序排序</p><p>返回元组 (sorted_tensor, sorted_indices) ， sorted_indices 为原始输入中的下标。</p><p>参数:</p><ul><li>input (Tensor) – 要对比的张量</li><li>dim (int, optional) – 沿着此维排序</li><li>descending (bool, optional) – 布尔值，控制升降排序</li><li>out (tuple, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)&gt;&gt;&gt; sorted, indices = torch.sort(x)&gt;&gt;&gt; sorted-1.6747  0.0610  0.1190  1.4137-1.4782  0.7159  1.0341  1.3678-0.3324 -0.0782  0.3518  0.4763[torch.FloatTensor of size 3x4]&gt;&gt;&gt; indices0  1  3  22  1  0  33  1  0  2[torch.LongTensor of size 3x4]&gt;&gt;&gt; sorted, indices = torch.sort(x, 0)&gt;&gt;&gt; sorted-1.6747 -0.0782 -1.4782 -0.33240.3518  0.0610  0.4763  0.11901.0341  0.7159  1.4137  1.3678[torch.FloatTensor of size 3x4]&gt;&gt;&gt; indices0  2  1  22  0  2  01  1  0  1[torch.LongTensor of size 3x4]</code></pre><blockquote><p>torch.topk</p></blockquote><p>torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor)<br>沿给定dim维度返回输入张量input中 k 个最大值。 如果不指定dim，则默认为input的最后一维。 如果为largest为 False ，则返回最小的 k 个值。</p><p>返回一个元组 (values,indices)，其中indices是原始输入张量input中测元素下标。 如果设定布尔值sorted 为<em>True</em>，将会确保返回的 k 个值被排序。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>k (int) – “top-k”中的k</li><li>dim (int, optional) – 排序的维</li><li>largest (bool, optional) – 布尔值，控制返回最大或最小值</li><li>sorted (bool, optional) – 布尔值，控制返回值是否排序</li><li>out (tuple, optional) – 可选输出张量 (Tensor, LongTensor) output buffers</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.arange(1, 6)&gt;&gt;&gt; x12345[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.topk(x, 3)(543[torch.FloatTensor of size 3],432[torch.LongTensor of size 3])&gt;&gt;&gt; torch.topk(x, 3, 0, largest=False)(123[torch.FloatTensor of size 3],012[torch.LongTensor of size 3])</code></pre><h4 id="其它操作-Other-Operations"><a href="#其它操作-Other-Operations" class="headerlink" title="其它操作 Other Operations"></a>其它操作 Other Operations</h4><hr><blockquote><p>torch.cross</p></blockquote><p>torch.cross(input, other, dim=-1, out=None) → Tensor<br>返回沿着维度dim上，两个张量input和other的向量积（叉积）。 input和other 必须有相同的形状，且指定的dim维上size必须为3。</p><p>如果不指定dim，则默认为第一个尺度为3的维。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>other (Tensor) – 第二个输入张量</li><li>dim (int, optional) – 沿着此维进行叉积操作</li><li>out (Tensor,optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4, 3)&gt;&gt;&gt; a-0.6652 -1.0116 -0.68570.2286  0.4446 -0.52720.0476  0.2321  1.99910.6199  1.1924 -0.9397[torch.FloatTensor of size 4x3]&gt;&gt;&gt; b = torch.randn(4, 3)&gt;&gt;&gt; b-0.1042 -1.1156  0.19470.9947  0.1149  0.4701-1.0108  0.8319 -0.07500.9045 -1.3754  1.0976[torch.FloatTensor of size 4x3]&gt;&gt;&gt; torch.cross(a, b, dim=1)-0.9619  0.2009  0.63670.2696 -0.6318 -0.4160-1.6805 -2.0171  0.27410.0163 -1.5304 -1.9311[torch.FloatTensor of size 4x3]&gt;&gt;&gt; torch.cross(a, b)-0.9619  0.2009  0.63670.2696 -0.6318 -0.4160-1.6805 -2.0171  0.27410.0163 -1.5304 -1.9311[torch.FloatTensor of size 4x3]</code></pre><ul><li>torch.diag</li></ul><p>torch.diag(input, diagonal=0, out=None) → Tensor<br>如果输入是一个向量(1D 张量)，则返回一个以input为对角线元素的2D方阵<br>如果输入是一个矩阵(2D 张量)，则返回一个包含input对角线元素的1D张量<br>参数diagonal指定对角线:</p><p>diagonal = 0, 主对角线<br>diagonal &gt; 0, 主对角线之上<br>diagonal &lt; 0, 主对角线之下  </p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>diagonal (int, optional) – 指定对角线</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><p>取得以input为对角线的方阵：</p><pre><code>&gt;&gt;&gt; a = torch.randn(3)&gt;&gt;&gt; a1.0480-2.3405-1.1138[torch.FloatTensor of size 3]&gt;&gt;&gt; torch.diag(a)1.0480  0.0000  0.00000.0000 -2.3405  0.00000.0000  0.0000 -1.1138[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.diag(a, 1)0.0000  1.0480  0.0000  0.00000.0000  0.0000 -2.3405  0.00000.0000  0.0000  0.0000 -1.11380.0000  0.0000  0.0000  0.0000[torch.FloatTensor of size 4x4]</code></pre><p>取得给定矩阵第k个对角线:</p><pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)&gt;&gt;&gt; a-1.5328 -1.3210 -1.52040.8596  0.0471 -0.2239-0.6617  0.0146 -1.0817[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.diag(a, 0)-1.53280.0471-1.0817[torch.FloatTensor of size 3]&gt;&gt;&gt; torch.diag(a, 1)-1.3210-0.2239[torch.FloatTensor of size 2]</code></pre><blockquote><p>torch.histc</p></blockquote><p>torch.histc(input, bins=100, min=0, max=0, out=None) → Tensor<br>计算输入张量的直方图。以min和max为range边界，将其均分成bins个直条，然后将排序好的数据划分到各个直条(bins)中。如果min和max都为0, 则利用数据中的最大最小值作为边界。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>bins (int) – 直方图 bins(直条)的个数(默认100个)</li><li>min (int) – range的下边界(包含)</li><li>max (int) – range的上边界(包含)</li><li>out (Tensor, optional) – 结果张量</li></ul><p>返回： 直方图 </p><p>返回类型：张量</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.histc(torch.FloatTensor([1, 2, 1]), bins=4, min=0, max=3)FloatTensor([0, 2, 1, 0])</code></pre><blockquote><p>torch.renorm</p></blockquote><p>torch.renorm(input, p, dim, maxnorm, out=None) → Tensor<br>返回一个张量，包含规范化后的各个子张量，使得沿着dim维划分的各子张量的p范数小于maxnorm。</p><font color="#ff0000" face="黑体">注意 如果p范数的值小于maxnorm，则当前子张量不需要修改。<br></font><font color="#ff0000" face="黑体">注意: 更详细解释参考torch7 以及Hinton et al. 2012, p. 2<br></font><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>p (float) – 范数的p</li><li>dim (int) – 沿着此维切片，得到张量子集</li><li>maxnorm (float) – 每个子张量的范数的最大值</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.ones(3, 3)&gt;&gt;&gt; x[1].fill_(2)&gt;&gt;&gt; x[2].fill_(3)&gt;&gt;&gt; x1  1  12  2  23  3  3[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.renorm(x, 1, 0, 5)1.0000  1.0000  1.00001.6667  1.6667  1.66671.6667  1.6667  1.6667[torch.FloatTensor of size 3x3]</code></pre><blockquote><p>torch.trace</p></blockquote><p>torch.trace(input) → float<br>返回输入2维矩阵对角线元素的和(迹)</p><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.arange(1, 10).view(3, 3)&gt;&gt;&gt; x1  2  34  5  67  8  9[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.trace(x)15.0</code></pre><blockquote><p>torch.tril</p></blockquote><p>torch.tril(input, k=0, out=None) → Tensor<br>返回一个张量out，包含输入矩阵(2D张量)的下三角部分，out其余部分被设为0。这里所说的下三角部分为矩阵指定对角线diagonal之上的元素。</p><p>参数k控制对角线:</p><ul><li>k = 0, 主对角线</li><li>k &gt; 0, 主对角线之上</li><li>k &lt; 0, 主对角线之下</li></ul><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>k (int, optional) – 指定对角线</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(3,3)&gt;&gt;&gt; a1.3225  1.7304  1.4573-0.3052 -0.3111 -0.18091.2469  0.0064 -1.6250[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.tril(a)1.3225  0.0000  0.0000-0.3052 -0.3111  0.00001.2469  0.0064 -1.6250[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.tril(a, k=1)1.3225  1.7304  0.0000-0.3052 -0.3111 -0.18091.2469  0.0064 -1.6250[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.tril(a, k=-1)0.0000  0.0000  0.0000-0.3052  0.0000  0.00001.2469  0.0064  0.0000[torch.FloatTensor of size 3x3]</code></pre><blockquote><p>torch.triu</p></blockquote><p>torch.triu(input, k=0, out=None) → Tensor<br>返回一个张量，包含输入矩阵(2D张量)的上三角部分，其余部分被设为0。这里所说的上三角部分为矩阵指定对角线diagonal之上的元素。</p><p>参数k控制对角线:</p><ul><li>k = 0, 主对角线</li><li>k &gt; 0, 主对角线之上</li><li>k &lt; 0, 主对角线之下</li></ul><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>k (int, optional) – 指定对角线</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(3,3)&gt;&gt;&gt; a1.3225  1.7304  1.4573-0.3052 -0.3111 -0.18091.2469  0.0064 -1.6250[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.triu(a)1.3225  1.7304  1.45730.0000 -0.3111 -0.18090.0000  0.0000 -1.6250[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.triu(a, k=1)0.0000  1.7304  1.45730.0000  0.0000 -0.18090.0000  0.0000  0.0000[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.triu(a, k=-1)1.3225  1.7304  1.4573-0.3052 -0.3111 -0.18090.0000  0.0064 -1.6250[torch.FloatTensor of size 3x3]</code></pre><h4 id="BLAS-and-LAPACK-Operations"><a href="#BLAS-and-LAPACK-Operations" class="headerlink" title="BLAS and LAPACK Operations"></a>BLAS and LAPACK Operations</h4><blockquote><p>torch.addbmm</p></blockquote><p>torch.addbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) → Tensor</p><p>对两个批batch1和batch2内存储的矩阵进行批矩阵乘操作，附带reduced add 步骤( 所有矩阵乘结果沿着第一维相加)。矩阵mat加到最终结果。 batch1和 batch2都为包含相同数量矩阵的3维张量。 如果batch1是形为b×n×m的张量，batch1是形为b×m×p的张量，则out和mat的形状都是n×p，即 res=(beta∗M)+(alpha∗sum(batch1i@batch2i,i=0,b))<br>对类型为 FloatTensor 或 DoubleTensor 的输入，alphaand beta必须为实数，否则两个参数须为整数。</p><p>参数：</p><ul><li>beta (Number, optional) – 用于mat的乘子</li><li>mat (Tensor) – 相加矩阵</li><li>alpha (Number, optional) – 用于batch1@batch2的乘子</li><li>batch1 (Tensor) – 第一批相乘矩阵</li><li>batch2 (Tensor) – 第二批相乘矩阵</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; M = torch.randn(3, 5)&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)&gt;&gt;&gt; torch.addbmm(M, batch1, batch2)-3.1162  11.0071   7.3102   0.1824  -7.68921.8265   6.0739   0.4589  -0.5641  -5.4283-9.3387  -0.1794  -1.2318  -6.8841  -4.7239[torch.FloatTensor of size 3x5]</code></pre><blockquote><p>torch.addmm</p></blockquote><p>torch.addmm(beta=1, mat, alpha=1, mat1, mat2, out=None) → Tensor</p><p>对矩阵mat1和mat2进行矩阵乘操作。矩阵mat加到最终结果。如果mat1 是一个 n×m张量，mat2 是一个 m×p张量，那么out和mat的形状为n×p。 alpha 和 beta 分别是两个矩阵 mat1@mat2和mat的比例因子，即， out=(beta∗M)+(alpha∗mat1@mat2)</p><p>对类型为 FloatTensor 或 DoubleTensor 的输入，betaand alpha必须为实数，否则两个参数须为整数。</p><p>参数 ：</p><ul><li>beta (Number, optional) – 用于mat的乘子</li><li>mat (Tensor) – 相加矩阵</li><li>alpha (Number, optional) – 用于mat1@mat2的乘子</li><li>mat1 (Tensor) – 第一个相乘矩阵</li><li>mat2 (Tensor) – 第二个相乘矩阵</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; M = torch.randn(2, 3)&gt;&gt;&gt; mat1 = torch.randn(2, 3)&gt;&gt;&gt; mat2 = torch.randn(3, 3)&gt;&gt;&gt; torch.addmm(M, mat1, mat2)-0.4095 -1.9703  1.35615.7674 -4.9760  2.7378[torch.FloatTensor of size 2x3]</code></pre><blockquote><p>torch.addmv</p></blockquote><p>torch.addmv(beta=1, tensor, alpha=1, mat, vec, out=None) → Tensor</p><p>对矩阵mat和向量vec对进行相乘操作。向量tensor加到最终结果。如果mat 是一个 n×m维矩阵，vec 是一个 m维向量，那么out和mat的为n元向量。 可选参数<em>alpha</em> 和 beta 分别是 mat∗vec和mat的比例因子，即， out=(beta∗tensor)+(alpha∗(mat@vec))</p><p>对类型为<em>FloatTensor</em>或<em>DoubleTensor</em>的输入，alphaand beta必须为实数，否则两个参数须为整数。</p><p>参数 ：</p><ul><li>beta (Number, optional) – 用于mat的乘子</li><li>mat (Tensor) – 相加矩阵</li><li>alpha (Number, optional) – 用于mat1@vec的乘子</li><li>mat (Tensor) – 相乘矩阵</li><li>vec (Tensor) – 相乘向量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; M = torch.randn(2)&gt;&gt;&gt; mat = torch.randn(2, 3)&gt;&gt;&gt; vec = torch.randn(3)&gt;&gt;&gt; torch.addmv(M, mat, vec)-2.0939-2.2950[torch.FloatTensor of size 2]</code></pre><blockquote><p>torch.addr</p></blockquote><p>torch.addr(beta=1, mat, alpha=1, vec1, vec2, out=None) → Tensor<br>对向量vec1和vec2对进行张量积操作。矩阵mat加到最终结果。如果vec1 是一个 n维向量，vec2 是一个 m维向量，那么矩阵mat的形状须为n×m。 可选参数<em>beta</em> 和 alpha 分别是两个矩阵 mat和 vec1@vec2的比例因子，即，resi=(beta∗Mi)+(alpha∗batch1i×batch2i)</p><p>对类型为<em>FloatTensor</em>或<em>DoubleTensor</em>的输入，alphaand beta必须为实数，否则两个参数须为整数。</p><p>参数 ：</p><ul><li>beta (Number, optional) – 用于mat的乘子</li><li>mat (Tensor) – 相加矩阵</li><li>alpha (Number, optional) – 用于两向量vec1，vec2外积的乘子</li><li>vec1 (Tensor) – 第一个相乘向量</li><li>vec2 (Tensor) – 第二个相乘向量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; vec1 = torch.arange(1, 4)&gt;&gt;&gt; vec2 = torch.arange(1, 3)&gt;&gt;&gt; M = torch.zeros(3, 2)&gt;&gt;&gt; torch.addr(M, vec1, vec2)1  22  43  6[torch.FloatTensor of size 3x2]</code></pre><blockquote><p>torch.baddbmm</p></blockquote><p>torch.baddbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) → Tensor</p><p>对两个批batch1和batch2内存储的矩阵进行批矩阵乘操作，矩阵mat加到最终结果。 batch1和  batch2都为包含相同数量矩阵的3维张量。 如果batch1是形为b×n×m的张量，batch1是形为b×m×p的张量，则out和mat的形状都是n×p，即 resi=(beta∗Mi)+(alpha∗batch1i×batch2i)<br>对类型为<em>FloatTensor</em>或<em>DoubleTensor</em>的输入，alphaand beta必须为实数，否则两个参数须为整数。</p><p>参数：</p><ul><li>beta (Number, optional) – 用于mat的乘子</li><li>mat (Tensor) – 相加矩阵</li><li>alpha (Number, optional) – 用于batch1@batch2的乘子</li><li>batch1 (Tensor) – 第一批相乘矩阵</li><li>batch2 (Tensor) – 第二批相乘矩阵</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; M = torch.randn(10, 3, 5)&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)&gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size()torch.Size([10, 3, 5])</code></pre><blockquote><p>torch.bmm</p></blockquote><p>torch.bmm(batch1, batch2, out=None) → Tensor<br>对存储在两个批batch1和batch2内的矩阵进行批矩阵乘操作。batch1和 batch2都为包含相同数量矩阵的3维张量。 如果batch1是形为b×n×m的张量，batch1是形为b×m×p的张量，则out和mat的形状都是n×p，即 res=(beta∗M)+(alpha∗sum(batch1i@batch2i,i=0,b))<br>对类型为 FloatTensor 或 DoubleTensor 的输入，alphaand beta必须为实数，否则两个参数须为整数。</p><p>参数：</p><ul><li>batch1 (Tensor) – 第一批相乘矩阵</li><li>batch2 (Tensor) – 第二批相乘矩阵</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)&gt;&gt;&gt; res = torch.bmm(batch1, batch2)&gt;&gt;&gt; res.size()torch.Size([10, 3, 5])</code></pre><blockquote><p>torch.btrifact</p></blockquote><p>torch.btrifact(A, info=None) → Tensor, IntTensor<br>返回一个元组，包含LU 分解和pivots 。 可选参数info决定是否对每个minibatch样本进行分解。info are from dgetrf and a non-zero value indicates an error occurred. 如果用CUDA的话，这个值来自于CUBLAS，否则来自LAPACK。</p><p>参数： </p><ul><li>A (Tensor) – 待分解张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)&gt;&gt;&gt; A_LU = A.btrifact()</code></pre><blockquote><p>torch.btrisolve</p></blockquote><p>torch.btrisolve(b, LU_data, LU_pivots) → Tensor<br>返回线性方程组Ax=b的LU解。</p><p>参数：</p><ul><li>b (Tensor) – RHS 张量.</li><li>LU_data (Tensor) – Pivoted LU factorization of A from btrifact.</li><li>LU_pivots (IntTensor) – LU 分解的Pivots.</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)&gt;&gt;&gt; b = torch.randn(2, 3)&gt;&gt;&gt; A_LU = torch.btrifact(A)&gt;&gt;&gt; x = b.btrisolve(*A_LU)&gt;&gt;&gt; torch.norm(A.bmm(x.unsqueeze(2)) - b)6.664001874625056e-08</code></pre><blockquote><p>torch.dot</p></blockquote><p>torch.dot(tensor1, tensor2) → float<br>计算两个张量的点乘(内乘),两个张量都为1-D 向量.</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.dot(torch.Tensor([2, 3]), torch.Tensor([2, 1]))7.0</code></pre><blockquote><p>torch.eig</p></blockquote><p>torch.eig(a, eigenvectors=False, out=None) -&gt; (Tensor, Tensor)<br>计算实方阵a 的特征值和特征向量</p><p>参数：</p><ul><li>a (Tensor) – 方阵，待计算其特征值和特征向量</li><li>eigenvectors (bool) – 布尔值，如果True，则同时计算特征值和特征向量，否则只计算特征值。</li><li>out (tuple, optional) – 输出元组</li></ul><p>返回值：<br>元组，包括：</p><ul><li>e (Tensor): a 的右特征向量</li><li>v (Tensor): 如果eigenvectors为True，则为包含特征向量的张量; 否则为空张量</li></ul><p>返回值类型： (Tensor, Tensor)</p><blockquote><p>torch.gels</p></blockquote><p>torch.gels(B, A, out=None) → Tensor<br>对形如m×n的满秩矩阵a计算其最小二乘和最小范数问题的解。 如果m&gt;=n,gels对最小二乘问题进行求解，即：<br>minimize∥AX−B∥F<br>如果m&lt;n,gels求解最小范数问题，即：<br>minimize∥X∥Fsubject toabAX=B<br>返回矩阵X的前n 行包含解。余下的行包含以下残差信息: 相应列从第n 行开始计算的每列的欧式距离。</p><font color="#ff0000" face="黑体">注意： 返回矩阵总是被转置，无论输入矩阵的原始布局如何，总会被转置；即，总是有 stride (1, m) 而不是 (m, 1).<br></font><p>参数：</p><ul><li>B (Tensor) – 矩阵B</li><li>A (Tensor) – m×n矩阵</li><li>out (tuple, optional) – 输出元组</li></ul><p>返回值： 元组，包括：</p><ul><li>X (Tensor): 最小二乘解</li><li>qr (Tensor): QR 分解的细节</li></ul><p>返回值类型： (Tensor, Tensor)</p><p>例子：</p><pre><code>&gt;&gt;&gt; A = torch.Tensor([[1, 1, 1],...                   [2, 3, 4],...                   [3, 5, 2],...                   [4, 2, 5],...                   [5, 4, 3]])&gt;&gt;&gt; B = torch.Tensor([[-10, -3],                    [ 12, 14],                    [ 14, 12],                    [ 16, 16],                    [ 18, 16]])&gt;&gt;&gt; X, _ = torch.gels(B, A)&gt;&gt;&gt; X2.0000  1.00001.0000  1.00001.0000  2.0000[torch.FloatTensor of size 3x2]</code></pre><blockquote><p>torch.geqrf</p></blockquote><p>torch.geqrf(input, out=None) -&gt; (Tensor, Tensor)</p><p>这是一个直接调用LAPACK的底层函数。 一般使用torch.qr()</p><p>计算输入的QR 分解，但是并不会分别创建Q,R两个矩阵，而是直接调用LAPACK 函数 Rather, this directly calls the underlying LAPACK function ?geqrf which produces a sequence of ‘elementary reflectors’.</p><p>参考 LAPACK文档获取更详细信息。</p><p>参数:</p><ul><li>input (Tensor) – 输入矩阵</li><li>out (tuple, optional) – 元组，包含输出张量 (Tensor, Tensor)</li></ul><blockquote><p>torch.ger</p></blockquote><p>torch.ger(vec1, vec2, out=None) → Tensor<br>计算两向量vec1,vec2的张量积。如果vec1的长度为n,vec2长度为m，则输出out应为形如n x m的矩阵。</p><p>参数:</p><ul><li>vec1 (Tensor) – 1D 输入向量</li><li>vec2 (Tensor) – 1D 输入向量</li><li>out (tuple, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; v1 = torch.arange(1, 5)&gt;&gt;&gt; v2 = torch.arange(1, 4)&gt;&gt;&gt; torch.ger(v1, v2)1   2   32   4   63   6   94   8  12[torch.FloatTensor of size 4x3]</code></pre><blockquote><p>torch.gesv</p></blockquote><p>torch.gesv(B, A, out=None) -&gt; (Tensor, Tensor)</p><p>X,LU=torch.gesv(B,A)，返回线性方程组AX=B的解。</p><p>LU 包含两个矩阵L，U。A须为非奇异方阵，如果A是一个m×m矩阵，B 是m×k矩阵，则LU 是m×m矩阵， X为m×k矩阵</p><p>参数：</p><ul><li>B (Tensor) – m×k矩阵</li><li>A (Tensor) – m×m矩阵</li><li>out (Tensor, optional) – 可选地输出矩阵X</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; A = torch.Tensor([[6.80, -2.11,  5.66,  5.97,  8.23],...                   [-6.05, -3.30,  5.36, -4.44,  1.08],...                   [-0.45,  2.58, -2.70,  0.27,  9.04],...                   [8.32,  2.71,  4.35,  -7.17,  2.14],...                   [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()&gt;&gt;&gt; B = torch.Tensor([[4.02,  6.19, -8.22, -7.57, -3.03],...                   [-1.56,  4.00, -8.67,  1.75,  2.86],...                   [9.81, -4.09, -4.57, -8.61,  8.99]]).t()&gt;&gt;&gt; X, LU = torch.gesv(B, A)&gt;&gt;&gt; torch.dist(B, torch.mm(A, X))9.250057093890353e-06</code></pre><blockquote><p>torch.inverse</p></blockquote><p>torch.inverse(input, out=None) → Tensor</p><p>对方阵输入input 取逆。</p><font color="#ff0000" face="黑体">注意： 返回矩阵总是被转置，无论输入矩阵的原始布局如何，总会被转置；即，总是有 stride (1, m) 而不是 (m, 1).<br></font><p>参数 ：</p><ul><li>input (Tensor) – 输入2维张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; x = torch.rand(10, 10)&gt;&gt;&gt; x0.7800  0.2267  0.7855  0.9479  0.5914  0.7119  0.4437  0.9131  0.1289  0.19820.0045  0.0425  0.2229  0.4626  0.6210  0.0207  0.6338  0.7067  0.6381  0.81960.8350  0.7810  0.8526  0.9364  0.7504  0.2737  0.0694  0.5899  0.8516  0.38830.6280  0.6016  0.5357  0.2936  0.7827  0.2772  0.0744  0.2627  0.6326  0.91530.7897  0.0226  0.3102  0.0198  0.9415  0.9896  0.3528  0.9397  0.2074  0.69800.5235  0.6119  0.6522  0.3399  0.3205  0.5555  0.8454  0.3792  0.4927  0.60860.1048  0.0328  0.5734  0.6318  0.9802  0.4458  0.0979  0.3320  0.3701  0.09090.2616  0.3485  0.4370  0.5620  0.5291  0.8295  0.7693  0.1807  0.0650  0.84970.1655  0.2192  0.6913  0.0093  0.0178  0.3064  0.6715  0.5101  0.2561  0.33960.4370  0.4695  0.8333  0.1180  0.4266  0.4161  0.0699  0.4263  0.8865  0.2578[torch.FloatTensor of size 10x10]&gt;&gt;&gt; x = torch.rand(10, 10)&gt;&gt;&gt; y = torch.inverse(x)&gt;&gt;&gt; z = torch.mm(x, y)&gt;&gt;&gt; z1.0000  0.0000  0.0000 -0.0000  0.0000  0.0000  0.0000  0.0000 -0.0000 -0.00000.0000  1.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000 -0.0000 -0.0000 -0.00000.0000  0.0000  1.0000 -0.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000 -0.00000.0000  0.0000  0.0000  1.0000  0.0000  0.0000  0.0000 -0.0000 -0.0000  0.00000.0000  0.0000 -0.0000 -0.0000  1.0000  0.0000  0.0000 -0.0000 -0.0000 -0.00000.0000  0.0000  0.0000 -0.0000  0.0000  1.0000 -0.0000 -0.0000 -0.0000 -0.00000.0000  0.0000  0.0000 -0.0000  0.0000  0.0000  1.0000  0.0000 -0.0000  0.00000.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000  1.0000 -0.0000  0.0000-0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000 -0.0000  1.0000 -0.0000-0.0000  0.0000 -0.0000 -0.0000 -0.0000  0.0000 -0.0000 -0.0000  0.0000  1.0000[torch.FloatTensor of size 10x10]&gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(10))) # Max nonzero5.096662789583206e-07</code></pre><blockquote><p>torch.mm</p></blockquote><p>torch.mm(mat1, mat2, out=None) → Tensor</p><p>对矩阵mat1和mat2进行相乘。 如果mat1 是一个n×m 张量，mat2 是一个 m×p 张量，将会输出一个 n×p 张量out。</p><p>参数 ：</p><ul><li>mat1 (Tensor) – 第一个相乘矩阵</li><li>mat2 (Tensor) – 第二个相乘矩阵</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; mat1 = torch.randn(2, 3)&gt;&gt;&gt; mat2 = torch.randn(3, 3)&gt;&gt;&gt; torch.mm(mat1, mat2)0.0519 -0.3304  1.22324.3910 -5.1498  2.7571[torch.FloatTensor of size 2x3]</code></pre><blockquote><p>torch.mv</p></blockquote><p>torch.mv(mat, vec, out=None) → Tensor</p><p>对矩阵mat和向量vec进行相乘。 如果mat 是一个n×m张量，vec 是一个m元 1维张量，将会输出一个n 元 1维张量。</p><p>参数 ：</p><ul><li>mat (Tensor) – 相乘矩阵</li><li>vec (Tensor) – 相乘向量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; mat = torch.randn(2, 3)&gt;&gt;&gt; vec = torch.randn(3)&gt;&gt;&gt; torch.mv(mat, vec)-2.0939-2.2950[torch.FloatTensor of size 2]</code></pre><hr><pre><code>torch.orgqrtorch.orgqr()torch.ormqrtorch.ormqr()torch.potrftorch.potrf()torch.potritorch.potri()torch.potrstorch.potrs()torch.pstrftorch.pstrf()</code></pre><hr><blockquote><p>torch.qr</p></blockquote><p>torch.qr(input, out=None) -&gt; (Tensor, Tensor)</p><p>计算输入矩阵的QR分解：返回两个矩阵q ,r， 使得 x=q∗r ，这里q 是一个半正交矩阵与 r 是一个上三角矩阵</p><p>本函数返回一个thin(reduced)QR分解。</p><font color="#ff0000" face="黑体">注意 如果输入很大，可能可能会丢失精度。<br></font><font color="#ff0000" face="黑体">注意 本函数依赖于你的LAPACK实现，虽然总能返回一个合法的分解，但不同平台可能得到不同的结果。<br></font><p>参数：</p><ul><li>input (Tensor) – 输入的2维张量</li><li>out (tuple, optional) – 输出元组tuple，包含Q和R</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; a = torch.Tensor([[12, -51, 4], [6, 167, -68], [-4, 24, -41]])&gt;&gt;&gt; q, r = torch.qr(a)&gt;&gt;&gt; q-0.8571  0.3943  0.3314-0.4286 -0.9029 -0.03430.2857 -0.1714  0.9429[torch.FloatTensor of size 3x3]&gt;&gt;&gt; r-14.0000  -21.0000   14.00000.0000 -175.0000   70.00000.0000    0.0000  -35.0000[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.mm(q, r).round()12  -51    46  167  -68-4   24  -41[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.mm(q.t(), q).round()1 -0  0-0  1  00  0  1[torch.FloatTensor of size 3x3]</code></pre><blockquote><p>torch.svd</p></blockquote><p>torch.svd(input, some=True, out=None) -&gt; (Tensor, Tensor, Tensor)</p><p>U,S,V=torch.svd(A)。 返回对形如 n×m的实矩阵 A 进行奇异值分解的结果，使得 A=USV’∗。 U 形状为 n×n，S 形状为 n×m ，V 形状为 m×m</p><p>some 代表了需要计算的奇异值数目。如果 some=True, it computes some and some=False computes all.</p><p>Irrespective of the original strides, the returned matrix U will be transposed, i.e. with strides (1, n) instead of (n, 1).</p><p>参数：</p><ul><li>input (Tensor) – 输入的2维张量</li><li>some (bool, optional) – 布尔值，控制需计算的奇异值数目</li><li>out (tuple, optional) – 结果tuple</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.Tensor([[8.79,  6.11, -9.15,  9.57, -3.49,  9.84],...                   [9.93,  6.91, -7.93,  1.64,  4.02,  0.15],...                   [9.83,  5.04,  4.86,  8.83,  9.80, -8.99],...                   [5.45, -0.27,  4.85,  0.74, 10.00, -6.02],...                   [3.16,  7.98,  3.01,  5.80,  4.27, -5.31]]).t()&gt;&gt;&gt; a8.7900   9.9300   9.8300   5.4500   3.16006.1100   6.9100   5.0400  -0.2700   7.9800-9.1500  -7.9300   4.8600   4.8500   3.01009.5700   1.6400   8.8300   0.7400   5.8000-3.4900   4.0200   9.8000  10.0000   4.27009.8400   0.1500  -8.9900  -6.0200  -5.3100[torch.FloatTensor of size 6x5]&gt;&gt;&gt; u, s, v = torch.svd(a)&gt;&gt;&gt; u-0.5911  0.2632  0.3554  0.3143  0.2299-0.3976  0.2438 -0.2224 -0.7535 -0.3636-0.0335 -0.6003 -0.4508  0.2334 -0.3055-0.4297  0.2362 -0.6859  0.3319  0.1649-0.4697 -0.3509  0.3874  0.1587 -0.51830.2934  0.5763 -0.0209  0.3791 -0.6526[torch.FloatTensor of size 6x5]&gt;&gt;&gt; s27.468722.64328.55845.98572.0149[torch.FloatTensor of size 5]&gt;&gt;&gt; v-0.2514  0.8148 -0.2606  0.3967 -0.2180-0.3968  0.3587  0.7008 -0.4507  0.1402-0.6922 -0.2489 -0.2208  0.2513  0.5891-0.3662 -0.3686  0.3859  0.4342 -0.6265-0.4076 -0.0980 -0.4932 -0.6227 -0.4396[torch.FloatTensor of size 5x5]&gt;&gt;&gt; torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))8.934150226306685e-06</code></pre><blockquote><p>torch.symeig</p></blockquote><p>torch.symeig(input, eigenvectors=False, upper=True, out=None) -&gt; (Tensor, Tensor)<br>e,V=torch.symeig(input) 返回实对称矩阵input的特征值和特征向量。</p><p>input 和 V 为 m×m 矩阵，e 是一个m 维向量。 此函数计算intput的所有特征值(和特征向量)，使得 input=Vdiag(e)V′<br>布尔值参数eigenvectors 规定是否只计算特征向量。如果为False，则只计算特征值；若设为True，则两者都会计算。 因为输入矩阵 input 是对称的，所以默认只需要上三角矩阵。如果参数upper为 False，下三角矩阵部分也被利用。</p><font color="#ff0000" face="黑体">注意： 返回矩阵总是被转置，无论输入矩阵的原始布局如何，总会被转置；即，总是有 stride (1, m) 而不是 (m, 1).<br></font><p>参数：</p><ul><li>input (Tensor) – 输入对称矩阵</li><li>eigenvectors (boolean, optional) – 布尔值（可选），控制是否计算特征向量</li><li>upper (boolean, optional) – 布尔值（可选），控制是否考虑上三角或下三角区域</li><li>out (tuple, optional) – 输出元组(Tensor, Tensor)</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.Tensor([[ 1.96,  0.00,  0.00,  0.00,  0.00],...                   [-6.49,  3.80,  0.00,  0.00,  0.00],...                   [-0.47, -6.39,  4.17,  0.00,  0.00],...                   [-7.20,  1.50, -1.51,  5.70,  0.00],...                   [-0.65, -6.34,  2.67,  1.80, -7.10]]).t()&gt;&gt;&gt; e, v = torch.symeig(a, eigenvectors=True)&gt;&gt;&gt; e-11.0656-6.22870.86408.865516.0948[torch.FloatTensor of size 5]&gt;&gt;&gt; v-0.2981 -0.6075  0.4026 -0.3745  0.4896-0.5078 -0.2880 -0.4066 -0.3572 -0.6053-0.0816 -0.3843 -0.6600  0.5008  0.3991-0.0036 -0.4467  0.4553  0.6204 -0.4564-0.8041  0.4480  0.1725  0.3108  0.1622[torch.FloatTensor of size 5x5]</code></pre><p>torch.trtrs<br> torch.trtrs()</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h3&gt;&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;font color=&quot;#00dddd&quot; size=&quot;4&quot;&gt;张量&lt;/font&gt;&lt;br&gt;  &lt;/li&gt;
      
    
    </summary>
    
      <category term="Pytorch框架" scheme="http://yoursite.com/categories/Pytorch%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>基于facenet的实时人脸检测</title>
    <link href="http://yoursite.com/2018/07/27/%E5%9F%BA%E4%BA%8Efacenet%E7%9A%84%E5%AE%9E%E6%97%B6%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/"/>
    <id>http://yoursite.com/2018/07/27/基于facenet的实时人脸检测/</id>
    <published>2018-07-27T09:33:00.000Z</published>
    <updated>2018-07-27T12:48:52.121Z</updated>
    
    <content type="html"><![CDATA[<p><strong>参考自</strong>  <a href="https://github.com/shanren7/real_time_face_recognition" target="_blank" rel="noopener">https://github.com/shanren7/real_time_face_recognition</a>  </p><p><strong>本人的项目代码</strong>  <a href="https://github.com/zouzhen/real_time_face_recognize" target="_blank" rel="noopener">https://github.com/zouzhen/real_time_face_recognize</a></p><p><del>虽然名字相同，但里面的内容可是有很大的不同</del><br>由于不能满足当前的tensorflow版本，以及未能满足设计要求，进行了优化与重新设计</p><h2 id="基于facenet的实时人脸检测"><a href="#基于facenet的实时人脸检测" class="headerlink" title="基于facenet的实时人脸检测"></a>基于facenet的实时人脸检测</h2><h3 id="工作环境"><a href="#工作环境" class="headerlink" title="工作环境"></a>工作环境</h3><ul><li>python 3.6</li><li>tensorflow==1.9.0(可运行在无gpu版)</li></ul><h3 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h3><p>real_time_face_recognize  </p><ul><li>|—— model_check_point（保存人脸识别模型）</li><li>|—— models（储存了facenet采用的神经网络模型）</li><li>|—— detect_face.py(主要实现人脸的检测，同时返回可能的人脸框)</li><li>|—— facenet.py（这里存储了facenet的主要函数）</li><li>|—— real_time_face_recognize.py(实现了实时人脸检测)  </li></ul><h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><ol><li>从 <a href="https://github.com/davidsandberg/facenet" target="_blank" rel="noopener">https://github.com/davidsandberg/facenet</a> 中下载预训练的分类模型，放在model_check_point下  </li><li>使用pip install requirements.txt安装需要的包，建议在virtualenv环境安装  </li><li>在目录下新建picture文件，将需要识别的人的图片放入其中，每人放入一张清晰的图片即可  </li><li>执行python real_time_face_recognize.py </li></ol><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>除可在facenet作者的github中下载模型外，我自己基于lfw训练集训练了一个模型，<a href="https://download.csdn.net/download/zouzhen_id/10568660" target="_blank" rel="noopener">点击</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;参考自&lt;/strong&gt;  &lt;a href=&quot;https://github.com/shanren7/real_time_face_recognition&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/
      
    
    </summary>
    
      <category term="项目" scheme="http://yoursite.com/categories/%E9%A1%B9%E7%9B%AE/"/>
    
    
      <category term="人脸识别" scheme="http://yoursite.com/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>MongoDB学习记录</title>
    <link href="http://yoursite.com/2018/07/16/MongoDB%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2018/07/16/MongoDB学习记录/</id>
    <published>2018-07-16T03:51:17.000Z</published>
    <updated>2018-07-17T01:25:30.547Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MongoDB"><a href="#MongoDB" class="headerlink" title="MongoDB"></a>MongoDB</h1><p>转载自<a href="https://github.com/zxhyJack/MyBlog/blob/master/mongodb/mongodb.md" target="_blank" rel="noopener">https://github.com/zxhyJack/MyBlog/blob/master/mongodb/mongodb.md</a></p><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li><p>文档<br>  是键值对的有序集合，这是MongDB的核心概念  </p></li><li><p>集合<br>  集合就是一组文档  </p><ul><li>动态模式<br>  集合是动态模式的，这意味着集合里面的文档可以是是各式各样的</li><li>命名<br>  集合使用名称进行命名  </li></ul></li><li><p>数据库<br>  由多个集合构成数据库，一个MongDB实例可以承载多个数据库，每个数据库拥有0个或多个集合  </p></li><li><p>基本操作<br>  在终端运行mongod命令，启动时，shell将自动连接MongDB数据库，需确保数据库已启动，可充分利用Javascript的标准库，还可定义和调用Javascript函数。</p><ul><li><p>创建数据库</p><pre><code>use database_name</code></pre><p>如果数据库存在，则进入指定数据库，否则，创建数据库<br>此时需要写入数据，数据库才能真正创建成功</p></li><li><p>查看所有数据库</p><pre><code>show databases | dbs</code></pre></li><li><p>创建集合</p><pre><code>db.createCollection(collection_name)</code></pre></li><li><p>删除数据库<br>先进入要删除的数据库，然后执行命令</p><pre><code>db.dropDatabase()</code></pre></li><li><p>删除集合</p><pre><code>db.collection_name.drop()</code></pre></li><li><p>增</p><pre><code>db.collection_name.insert(document)</code></pre><p>  exp:</p><pre><code>db.students.insert({name:&apos;James&apos;,age: 32,gender:&apos;man&apos;,career:&apos;player&apos;})</code></pre></li><li><p>查</p><pre><code>db.collection.find(&lt;query&gt;,&lt;projection&gt;)- query: 查询条件- projection: 投影操作</code></pre><p>  exp:</p><pre><code>db.students.find()</code></pre></li><li><p>改</p><pre><code>db.collection.updateOne(&lt;query&gt;,&lt;update&gt;) // 更新第一个符合条件的集合db.collection.updateMany(&lt;query&gt;,&lt;update&gt;)  // 更新所有符合条件的集合</code></pre><ul><li>query: 查询条件</li><li><p>update： 更新的内容</p><p>exp: </p><p>  db.students.update({name:’James’},{$set:{gender:’woman’}})</p></li></ul></li><li><p>删</p><pre><code>db.collection_name.deleteOne(&lt;query&gt;) // 删除第一个符合条件的集合db.collection_name.deleteMany(&lt;query&gt;) // 删除所有符合条件的集合</code></pre><p>  exp:</p><pre><code>db.students.deleteOne({name:&apos;James&apos;})</code></pre></li></ul></li></ul><h2 id="数据操作（重点）"><a href="#数据操作（重点）" class="headerlink" title="数据操作（重点）"></a>数据操作（重点）</h2><p>数据库的核心——CRUD，增加和删除较为简单，查询和修改较复杂</p><h3 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h3><h4 id="关系运算符"><a href="#关系运算符" class="headerlink" title="关系运算符"></a>关系运算符</h4><ul><li><p>$gt 大于</p></li><li><p>$lt 小于</p></li><li><p>$gte  大于等于</p></li><li><p>$lte  小于等于</p></li><li><p>$eq | (key: value)  等于</p></li><li><p>$ne 不等于</p></li></ul><p>先往数据库中添加一些数据</p><pre><code>db.students.insert({&apos;name&apos;:&apos;张三&apos;,&apos;sex&apos;:&apos;男&apos;,&apos;age&apos;:19,&apos;score&apos;: 89,&apos;address&apos;: &apos;海淀区&apos;})db.students.insert({&apos;name&apos;:&apos;李四&apos;,&apos;sex&apos;:&apos;女&apos;,&apos;age&apos;:20,&apos;score&apos;: 100,&apos;address&apos;: &apos;朝阳区&apos;})db.students.insert({&apos;name&apos;:&apos;王五&apos;,&apos;sex&apos;:&apos;男&apos;,&apos;age&apos;:22,&apos;score&apos;: 50,&apos;address&apos;: &apos;西城区&apos;})db.students.insert({&apos;name&apos;:&apos;赵六&apos;,&apos;sex&apos;:&apos;女&apos;,&apos;age&apos;:21,&apos;score&apos;: 60,&apos;address&apos;: &apos;东城区&apos;})db.students.insert({&apos;name&apos;:&apos;孙七&apos;,&apos;sex&apos;:&apos;男&apos;,&apos;age&apos;:19,&apos;score&apos;: 70,&apos;address&apos;: &apos;海淀区&apos;})db.students.insert({&apos;name&apos;:&apos;王八&apos;,&apos;sex&apos;:&apos;女&apos;,&apos;age&apos;:23,&apos;score&apos;: 90,&apos;address&apos;: &apos;海淀区&apos;})db.students.insert({&apos;name&apos;:&apos;刘九&apos;,&apos;sex&apos;:&apos;女&apos;,&apos;age&apos;:35,&apos;score&apos;: 56,&apos;address&apos;: &apos;朝阳区&apos;})db.students.insert({&apos;name&apos;:&apos;钱十&apos;,&apos;sex&apos;:&apos;男&apos;,&apos;age&apos;:27,&apos;score&apos;: 89,&apos;address&apos;: &apos;海淀区&apos;})</code></pre><p>exp:</p><ol><li><p>查询姓名是张三的学生信息</p><pre><code>db.students.find({name:’张三’}).pretty()</code></pre></li><li><p>查询性别是男的学生信息</p><pre><code>db.students.find({sex:’男’}).pretty()</code></pre></li><li><p>查询年龄大于19岁的学生</p><pre><code>db.students.find({age:{$gt:19}}).pretty()</code></pre></li><li><p>查询成绩大于等于60分的学生</p><pre><code>db.students.find({score:{$gte:60}}).pretty() </code></pre></li><li><p>查询姓名不是王五的信息</p><pre><code>db.students.find({name:{$ne:’王五’}}).pretty()</code></pre></li></ol><h4 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h4><ul><li><p><code>$and</code>   与</p></li><li><p><code>$or</code>   或</p></li><li><p><code>$not | $nor</code>  非</p></li></ul><p>exp:</p><ol><li><p>查询年龄在19 ~ 22岁的学生信息</p><pre><code>db.students.find({age:{$gte:19,$lte:22}}).pretty()</code></pre></li></ol><p>逻辑运算中与连接是最容易的，只需要利用<code>,</code>分割多个条件即可</p><ol start="2"><li><p>查询年龄小于20岁，或者成绩大于90分的学生信息</p><pre><code>db.students.find({$or:    [     {age:{$lt:20}},    {score:{$gt:90}}    ]}).pretty()</code></pre></li><li><p>查询年龄大于等于20岁，且成绩小于等于90分的学生信息</p><pre><code>db.students.find({$and:    [     {age:{$gte:20}},    {score:{$lte:90}}    ]}).pretty()</code></pre></li><li><p>查询年龄小于20岁的学生信息</p><pre><code>db.students.find({age:{$lt:20}}).pretty()db.students.find({age:{$not:{$gte:20}}}).pretty()</code></pre></li></ol><h4 id="取模"><a href="#取模" class="headerlink" title="取模"></a>取模</h4><p><code>$mod:[除数，余数]</code></p><p>exp: 查询年龄除以20余1的学生信息</p><pre><code>db.students.find({age:{$mod:[20,1]}}).pretty()</code></pre><h4 id="范围查询"><a href="#范围查询" class="headerlink" title="范围查询"></a>范围查询</h4><p>$in: 在范围之中<br>$nin: 不在范围之中</p><p>exp:</p><ol><li><p>查询姓名是”张三“、”李四、”王五“的学生</p><pre><code>db.students.find({name: {$in:[&apos;张三&apos;,&apos;李四&apos;,&apos;王五&apos;]}}).pret ty()</code></pre></li><li><p>查询姓名不是”张三“、”李四、”王五“的学生</p><pre><code>db.students.find({name: {$nin:[&apos;张三&apos;,&apos;李四&apos;,&apos;王五&apos;]}}).pretty()</code></pre></li></ol><h4 id="数组查询"><a href="#数组查询" class="headerlink" title="数组查询"></a>数组查询</h4><ul><li><p>$all </p></li><li><p>$size </p></li><li><p>$slice </p></li><li><p>$elemMatch</p></li></ul><p>首先在数据库中新增一些数据</p><pre><code>db.students.insert({name:&apos;a&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,course:[&apos;语文&apos;,&apos;数学&apos;,&apos;英语&apos;,&apos;音乐&apos;,&apos;政治&apos;]})db.students.insert({name:&apos;b&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,course:[&apos;语文&apos;,&apos;数学&apos;]})db.students.insert({name:&apos;c&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,course:[&apos;语文&apos;,&apos;数学&apos;,&apos;英语&apos;]})db.students.insert({name:&apos;d&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,course:[&apos;英语&apos;,&apos;音乐&apos;,&apos;政治&apos;]})db.students.insert({name:&apos;e&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,course:[&apos;语文&apos;,&apos;政治&apos;]})</code></pre><p><code>$all</code>: 表示全都包括，用法：</p><pre><code>{$all:[内容1,内容2]}</code></pre><p>exp:</p><p>查询同时参加语文和数学的学生</p><pre><code>db.students.find({course:{$all:[&apos;语文&apos;,&apos;数学&apos;]}}).pretty()</code></pre><p>数组的操作，可以利用索引，使用<code>key.index</code>的方式来定义索引</p><p>查询数组中第二个内容是数学的学生(sh)</p><pre><code>db.students.find({&apos;course.1&apos;:&apos;数学&apos;}).pretty()</code></pre><p><code>$size</code>: 控制数组元素数量</p><p>exp:</p><p>查询只有两门课程的学生</p><pre><code>db.students.find({course:{$size: 2}}).pretty()</code></pre><p><code>$slice</code>: 控制查询结果的返回数量</p><p>exp:</p><p>查询年龄是19岁的学生，要求之显示两门参加的课程</p><pre><code>db.students.find({age:19},{course:{$slice:2}}).pretty()</code></pre><p>此时查询返回的是前两门课程，可以设置参数来取出想要的内容</p><pre><code>$slice:-2   //后两门$slice: [1,2]   // 第一个参数表示跳过的数据量，第二个参数表示返回的数据量</code></pre><h4 id="嵌套集合运算"><a href="#嵌套集合运算" class="headerlink" title="嵌套集合运算"></a>嵌套集合运算</h4><p>对象里面套对象</p><p>在数据库中新增数据</p><pre><code>db.students.insert({    name:&apos;A&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,    course:[&apos;语文&apos;,&apos;数学&apos;,&apos;英语&apos;,&apos;音乐&apos;,&apos;政治&apos;],    parents:[        {name:&apos;A(father)&apos;,age:50,job:&apos;工人&apos;},        {name:&apos;A(mother)&apos;,age:50,job:&apos;职员&apos;}    ]})db.students.insert({    name:&apos;B&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,    course:[&apos;语文&apos;,&apos;数学&apos;],    parents:[        {name:&apos;B(father)&apos;,age:50,job:&apos;处长&apos;},        {name:&apos;B(mother)&apos;,age:50,job:&apos;局长&apos;}    ]})db.students.insert({    name:&apos;C&apos;,sex:&apos;男&apos;,age:19,score:89,address:&apos;海淀区&apos;,    course:[&apos;语文&apos;,&apos;数学&apos;,&apos;英语&apos;],    parents:[        {name:&apos;C(father)&apos;,age:50,job:&apos;工人&apos;},        {name:&apos;C(mother)&apos;,age:50,job:&apos;局长&apos;}        ]})</code></pre><p>对于嵌套的集合中数据的判断只能通过<code>$elemMatch</code>完成</p><p>语法：<code>{ &lt;field&gt;: { $elemMatch: { &lt;query1&gt;, &lt;query2&gt;, ... } } }</code></p><p>exp:</p><p>查询父母中有人是局长的信息</p><pre><code>db.students.find({parents: {$elemMatch: {job: &apos;局长&apos;}}}).pretty()</code></pre><h4 id="判断某个字段是否存在"><a href="#判断某个字段是否存在" class="headerlink" title="判断某个字段是否存在"></a>判断某个字段是否存在</h4><p><code>{$exists:flag}</code>  flag为true表示存在，false表示不存在</p><p>exp:</p><ol><li><p>查询具有parents成员的学生</p><pre><code>db.students.find({parents:{$exists: true}}).pretty()</code></pre></li><li><p>查询不具有course成员的学生</p><pre><code>db.students.find({course: {$exists: false}}).pretty()</code></pre></li></ol><h4 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h4><p><code>sort({ field: value })</code> value是1表示升序，-1表示降序</p><p>exp:</p><p>学生信息按照分数降序排列</p><pre><code>db.students.find().sort({score:-1}).pretty()</code></pre><h4 id="分页显示"><a href="#分页显示" class="headerlink" title="分页显示"></a>分页显示</h4><p><code>skip(n)</code>: 跳过n条数据</p><p><code>limit(n)</code>: 返回n条数据</p><p>exp:</p><ol><li><p>分页显示，第一页，每页显示5条数据</p><pre><code>db.students.find({}).skip(0).limit(5).pretty()</code></pre></li><li><p>分页显示，第二页，每页显示5条数据</p><pre><code>db.students.find({}).skip(5).limit(5).pretty()</code></pre></li></ol><h3 id="数据修改-更新"><a href="#数据修改-更新" class="headerlink" title="数据修改 | 更新"></a>数据修改 | 更新</h3><p><code>updateOne()</code>     修改匹配的第一条数据</p><p><code>updateMany()</code>    修改所有匹配的数据</p><p>格式：<code>updateOne(&lt;filter&gt;,&lt;update&gt;)</code></p><h4 id="修改器"><a href="#修改器" class="headerlink" title="修改器"></a>修改器</h4><p><code>$inc</code>: 操作数字字段的数据内容</p><p>语法: <code>{&quot;$inc&quot; : {成员 : 内容}}</code></p><p>exp: </p><p>将所有年龄为19岁的学生成绩一律减少30分，年龄增加1</p><pre><code>db.students.updateMany({age:19},{$inc:{score:-30,age:1}})</code></pre><p><code>$set</code>: 更新内容</p><p>语法：<code>{$set: :{属性: 新内容}}</code></p><p>exp: </p><p>将20岁学生的成绩修改为89</p><pre><code>db.students.updateMany({age: 20},{$set: {score: 89}})</code></pre><p><code>$unset</code>: 删除某个属性及其内容</p><p>语法：<code>{$unset: {属性: 1}}</code></p><p>exp:</p><p>删除张三的年龄和成绩信息</p><pre><code>db.students.updateOne({name:&apos;张三&apos;},{$unset: {age: 1,score: 1}})</code></pre><p><code>$push</code>: 向数组中添加数据</p><p>语法：<code>{$push: {属性: value}}</code></p><p>exp:</p><p>在李四的课程中添加语文</p><pre><code>db.students.updateOne({name: &apos;李四&apos;},{$push: {course: &apos;语文&apos;}})</code></pre><p>如果需要向数组中添加多个数据，则需要用到<code>$each</code></p><p>exp: </p><p>在李四的课程中添加数学、英语</p><pre><code>db.students.updateOne(    {name:&apos;李四&apos;},    {$push:        {            course:{$each: [&apos;数学&apos;,&apos;英语&apos;]}        }    })</code></pre><p><code>$addToSet</code>: 向数组里面添加一个新的数据</p><p>与<code>$push</code>的区别，<code>$push</code>添加的数据可能是重复的，<code>$addToSet</code>只有这个数据不存在时才会添加（去重）</p><p>语法：<code>{$addToSet: {属性：value}}</code></p><p>exp:</p><p>王五新增一门舞蹈课程</p><pre><code>db.students.updateOne(    {name:&apos;王五&apos;},    {$addToSet: {course:&apos;舞蹈&apos;}})</code></pre><p><code>$pop</code>: 删除数组内的数据</p><p>语法：<code>{$pop: {field: value}}</code>,value为-1表示删除第一个，value为1表示删除最后一个</p><p>exp:</p><p>删除王五的第一个课程</p><pre><code>db.students.updateOne({name:&apos;王五&apos;},{$pop:{course:-1}})</code></pre><p>只是删除属性的内容，属性还在</p><p><code>$pull</code>: 从数组中删除一个指定内容的数据</p><p>语法：<code>{$pull: {field：value}}</code> 进行数据比对，如果是该数据则删除</p><p>exp:</p><p>删除李四的语文课程</p><pre><code>db.students.updateOne({name: &apos;李四&apos;},{$pull:{course:&apos;语文&apos;}})</code></pre><p><code>$pullAll</code>: 一次删除多个数据</p><p>语法：<code>{$pullAll:{field:[value1,value2...]}}</code></p><p>exp:</p><p>删除a的语文数学英语课程</p><pre><code>db.students.updateOne({name:&apos;a&apos;},{$pullAll:{course:[&apos;语文&apos;,&apos;数学&apos;,&apos;英语&apos;]}})</code></pre><p><code>$rename</code>: 属性重命名</p><p>语法： <code>{$rename: {旧属性名：新属性名}}</code></p><p>exp:</p><p>把张三的name属性名改为姓名</p><pre><code>db.students.updateOne({name:&apos;张三&apos;},{$rename:{name:&apos;姓名&apos;}})</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;MongoDB&quot;&gt;&lt;a href=&quot;#MongoDB&quot; class=&quot;headerlink&quot; title=&quot;MongoDB&quot;&gt;&lt;/a&gt;MongoDB&lt;/h1&gt;&lt;p&gt;转载自&lt;a href=&quot;https://github.com/zxhyJack/MyBlog/blo
      
    
    </summary>
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="mongodb" scheme="http://yoursite.com/tags/mongodb/"/>
    
  </entry>
  
  <entry>
    <title>facenet详解</title>
    <link href="http://yoursite.com/2018/07/14/facenet%E8%AF%A6%E8%A7%A3/"/>
    <id>http://yoursite.com/2018/07/14/facenet详解/</id>
    <published>2018-07-14T08:35:26.000Z</published>
    <updated>2018-08-05T09:57:36.933Z</updated>
    
    <content type="html"><![CDATA[<h2 id="facenet算法初测"><a href="#facenet算法初测" class="headerlink" title="facenet算法初测"></a>facenet算法初测</h2><ul><li><p><a href="https://github.com/davidsandberg/facenet" target="_blank" rel="noopener">facenet代码地址</a></p></li><li><p><a href="https://www.cnblogs.com/bakari/archive/2012/08/27/2658956.html" target="_blank" rel="noopener">数据对齐详解</a></p></li><li><p><a href="https://drive.google.com/file/d/1R77HmFADxe87GmoLwzfgMu_HY0IhcyBz/view" target="_blank" rel="noopener">已训练模型下载(基于CASIA-WebFace)</a></p></li><li><p><a href="https://blog.csdn.net/u013044310/article/details/79556099" target="_blank" rel="noopener">主要参考博客</a></p></li></ul><hr><h3 id="算法代码结构"><a href="#算法代码结构" class="headerlink" title="算法代码结构"></a>算法代码结构</h3><p>结构如图：<br><img src="/2018/07/14/facenet详解/算法结构.png" alt="代码结构">  </p><ul><li><p>|—— contribute (包含对人脸进行处理的函数)  </p></li><li><p>|—— data (原算法进行训练或测试时使用的图片数据)  </p></li><li><p>|—— lfw (储存的lfw数据集)  </p></li><li><p>|—— lfw_mtcnnpy_160 (储存的经过对齐后的图片数据)  </p></li><li><p>|—— models (存储训练模型)  </p></li><li><p>|—— src (核心功能相关的代码)  </p></li><li><p>|—— test (算法、模型测试相关的代码)  </p></li><li><p>|—— tmp (暂不清楚)  </p></li><li><p>|—— 其他  </p></li></ul><hr><h3 id="功能测试"><a href="#功能测试" class="headerlink" title="功能测试"></a>功能测试</h3><ul><li><p>预训练模型测试：  </p><pre><code>python src/validate_on_lfw.py lfw_mtcnnpy_160 models\20180408-102900  </code></pre></li></ul><p><img src="/2018/07/14/facenet详解/算法测试.png" alt="算法测试">  </p><p>然而测试结果并不是特别好，可能是仅用CPU的缘故  </p><ul><li><p>相似人脸对比结果：  </p><pre><code>python src\compare.py models\20180408-102900 data\images\Anthony_Hopkins_0001.jpg data\images\Anthony_Hopkins_0002.jpg  </code></pre></li></ul><p><img src="/2018/07/14/facenet详解/相似人脸对比结果.png" alt="相似人脸对比结果">  </p><ul><li><p>不相似人脸对比结果：  </p><pre><code>python src\compare.py models\20180408-102900 data\images\Anthony_Hopkins_0001.jpg lfw_mtcnnpy_160\Aaron_Eckhart\Aaron_Eckhart_0001.png</code></pre></li></ul><p><img src="/2018/07/14/facenet详解/不相似人脸对比结果.png" alt="不相似人脸对比结果"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;facenet算法初测&quot;&gt;&lt;a href=&quot;#facenet算法初测&quot; class=&quot;headerlink&quot; title=&quot;facenet算法初测&quot;&gt;&lt;/a&gt;facenet算法初测&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.c
      
    
    </summary>
    
      <category term="算法" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="facenet" scheme="http://yoursite.com/tags/facenet/"/>
    
  </entry>
  
  <entry>
    <title>JavaScript使用记录</title>
    <link href="http://yoursite.com/2018/07/12/JavaScript%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2018/07/12/JavaScript使用记录/</id>
    <published>2018-07-12T06:20:08.000Z</published>
    <updated>2018-07-14T12:42:53.197Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Promise"><a href="#Promise" class="headerlink" title="Promise"></a>Promise</h3><p>在JavaScript的世界中，所有代码都是单线程执行的。</p><p>由于这个“缺陷”，导致JavaScript的所有网络操作，浏览器事件，都必须是异步执行。异步执行可以用回调函数实现：  </p><pre><code>function callback() {    console.log(&apos;Done&apos;);}console.log(&apos;before setTimeout()&apos;);setTimeout(callback, 1000); // 1秒钟后调用callback函数console.log(&apos;after setTimeout()&apos;);</code></pre><p>观察上述代码执行，在Chrome的控制台输出可以看到：</p><pre><code>before setTimeout()after setTimeout()(等待1秒后)Done  </code></pre><p>可见，异步操作会在将来的某个时间点触发一个函数调用。</p><p>AJAX就是典型的异步操作。以之前的代码为例：</p><pre><code>request.onreadystatechange = function () {    if (request.readyState === 4) {        if (request.status === 200) {            return success(request.responseText);        } else {            return fail(request.status);        }    }}  </code></pre><p>把回调函数success(request.responseText)和fail(request.status)写到一个AJAX操作里很正常，但是不好看，而且不利于代码复用。</p><p>有没有更好的写法？比如写成这样：</p><pre><code>var ajax = ajaxGet(&apos;http://...&apos;);ajax.ifSuccess(success)    .ifFail(fail);</code></pre><p>这种链式写法的好处在于，先统一执行AJAX逻辑，不关心如何处理结果，然后，根据结果是成功还是失败，在将来的某个时候调用success函数或fail函数。</p><p>古人云：“君子一诺千金”，这种“承诺将来会执行”的对象在JavaScript中称为Promise对象。<br>Promise有各种开源实现，在ES6中被统一规范，由浏览器直接支持。  </p><p>我们先看一个最简单的Promise例子：生成一个0-2之间的随机数，如果小于1，则等待一段时间后返回成功，否则返回失败：</p><pre><code>function test(resolve, reject) {    var timeOut = Math.random() * 2;    log(&apos;set timeout to: &apos; + timeOut + &apos; seconds.&apos;);    setTimeout(function () {        if (timeOut &lt; 1) {            log(&apos;call resolve()...&apos;);            resolve(&apos;200 OK&apos;);        }        else {            log(&apos;call reject()...&apos;);            reject(&apos;timeout in &apos; + timeOut + &apos; seconds.&apos;);        }    }, timeOut * 1000);}</code></pre><p>这个test()函数有两个参数，这两个参数都是函数，如果执行成功，我们将调用resolve(‘200 OK’)，如果执行失败，我们将调用reject(‘timeout in ‘ + timeOut + ‘ seconds.’)。可以看出，test()函数只关心自身的逻辑，并不关心具体的resolve和reject将如何处理结果。</p><p>有了执行函数，我们就可以用一个Promise对象来执行它，并在将来某个时刻获得成功或失败的结果：</p><pre><code>var p1 = new Promise(test);var p2 = p1.then(function (result) {    console.log(&apos;成功：&apos; + result);});var p3 = p2.catch(function (reason) {    console.log(&apos;失败：&apos; + reason);});</code></pre><p>变量p1是一个Promise对象，它负责执行test函数。由于test函数在内部是异步执行的，当test函数执行成功时，我们告诉Promise对象：</p><p>// 如果成功，执行这个函数：  </p><pre><code>p1.then(function (result) {    console.log(&apos;成功：&apos; + result);});</code></pre><p>当test函数执行失败时，我们告诉Promise对象：</p><pre><code>p2.catch(function (reason) {    console.log(&apos;失败：&apos; + reason);});</code></pre><p>Promise对象可以串联起来，所以上述代码可以简化为：</p><pre><code>new Promise(test).then(function (result) {    console.log(&apos;成功：&apos; + result);}).catch(function (reason) {    console.log(&apos;失败：&apos; + reason);}); </code></pre><p>可见Promise最大的好处是在异步执行的流程中，把执行代码和处理结果的代码清晰地分离了：<br><img src="/2018/07/12/JavaScript使用记录/l.png" alt="图片">  </p><p>Promise还可以做更多的事情，比如，有若干个异步任务，需要先做任务1，如果成功后再做任务2，任何任务失败则不再继续并执行错误处理函数。</p><p>要串行执行这样的异步任务，不用Promise需要写一层一层的嵌套代码。有了Promise，我们只需要简单地写：</p><pre><code>job1.then(job2).then(job3).catch(handleError);  </code></pre><p>其中，job1、job2和job3都是Promise对象。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Promise&quot;&gt;&lt;a href=&quot;#Promise&quot; class=&quot;headerlink&quot; title=&quot;Promise&quot;&gt;&lt;/a&gt;Promise&lt;/h3&gt;&lt;p&gt;在JavaScript的世界中，所有代码都是单线程执行的。&lt;/p&gt;
&lt;p&gt;由于这个“缺陷”，导致Ja
      
    
    </summary>
    
      <category term="编程语言" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="js" scheme="http://yoursite.com/tags/js/"/>
    
  </entry>
  
  <entry>
    <title>Git使用</title>
    <link href="http://yoursite.com/2018/07/12/Git%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2018/07/12/Git使用/</id>
    <published>2018-07-12T03:27:13.000Z</published>
    <updated>2018-07-12T03:32:08.825Z</updated>
    
    <content type="html"><![CDATA[<p>转载自<a href="https://blog.csdn.net/zwhfyy/article/details/8625228，如有侵权，请联系删除" target="_blank" rel="noopener">https://blog.csdn.net/zwhfyy/article/details/8625228，如有侵权，请联系删除</a></p><h1 id="出错信息"><a href="#出错信息" class="headerlink" title="出错信息"></a>出错信息</h1><p>Your local changes to the following files would be overwritten by merge<br>error: Your local changes to the following files would be overwritten by merge:<br>        123.txt<br>Please, commit your changes or stash them before you can merge.</p><p>如果希望保留生产服务器上所做的改动,仅仅并入新配置项, 处理方法如下:  </p><pre><code>git stashgit pullgit stash pop  </code></pre><p>然后可以使用git diff -w +文件名 来确认代码自动合并的情况.</p><p>反过来,如果希望用代码库中的文件完全覆盖本地工作版本. 方法如下:  </p><pre><code>git reset --hardgit pull其中git reset是针对版本,如果想针对文件回退本地修改,使用</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载自&lt;a href=&quot;https://blog.csdn.net/zwhfyy/article/details/8625228，如有侵权，请联系删除&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/zwhfyy/
      
    
    </summary>
    
      <category term="工具" scheme="http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Git" scheme="http://yoursite.com/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>人脸识别算法发展情况</title>
    <link href="http://yoursite.com/2018/07/10/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%AE%97%E6%B3%95%E5%8F%91%E5%B1%95%E6%83%85%E5%86%B5/"/>
    <id>http://yoursite.com/2018/07/10/人脸识别算法发展情况/</id>
    <published>2018-07-10T09:09:03.000Z</published>
    <updated>2018-07-21T07:25:30.449Z</updated>
    
    <content type="html"><![CDATA[<p>转载自<a href="https://zhuanlan.zhihu.com/p/36416906，如有侵权，请联系删除。" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/36416906，如有侵权，请联系删除。</a></p><h1 id="人脸识别概述"><a href="#人脸识别概述" class="headerlink" title="人脸识别概述"></a>人脸识别概述</h1><p>人脸识别的目标是确定一张人脸图像的身份，即这个人是谁，这是机器学习和模式识别中的分类问题。它主要应用在身份识别和身份验证中。</p><h2 id="人脸识别系统的组成"><a href="#人脸识别系统的组成" class="headerlink" title="人脸识别系统的组成"></a>人脸识别系统的组成</h2><pre><code>人脸检测（Face Detection）人脸对齐（Face Alignment）人脸特征表征（Feature Representation）</code></pre><ul><li><p>人脸检测</p><p>  人脸检测用于确定人脸在图像中的大小和位置，即解决“人脸在哪里”的问题，把真正的人脸区域从图像中裁剪出来，便于后续的人脸特征分析和识别。  </p></li><li><p>人脸对齐</p><p>  同一个人在不同的图像序列中可能呈现出不同的姿态和表情，这种情况是不利于人脸识别的。所以有必要将人脸图像都变换到一个统一的角度和姿态，这就是人脸对齐。它的原理是找到人脸的若干个关键点（基准点，如眼角，鼻尖，嘴角等），然后利用这些对应的关键点通过相似变换（Similarity Transform，旋转、缩放和平移）将人脸尽可能变换到标准人脸。  </p></li><li><p>人脸特征表征</p><p>  第三个模块是本文重点要讲的人脸识别算法，它接受的输入是标准化的人脸图像，通过特征建模得到向量化的人脸特征，最后通过分类器判别得到识别的结果。这里的关键是怎样得到对不同人脸有区分度的特征，通常我们在识别一个人时会看它的眉形、脸轮廓、鼻子形状、眼睛的类型等，人脸识别算法引擎要通过练习（训练）得到类似这样的有区分度的特征。本系列文章主要围绕人脸识别中的人脸特征表征进行展开，人脸检测和人脸对齐方法会在其它专题系列文章中进行介绍。</p></li></ul><h2 id="人脸识别算法的三个阶段"><a href="#人脸识别算法的三个阶段" class="headerlink" title="人脸识别算法的三个阶段"></a>人脸识别算法的三个阶段</h2><p>人脸识别算法经历了早期算法，人工特征+分类器，深度学习3个阶段。目前深度学习算法是主流，极大的提高了人脸识别的精度。  </p><ul><li><p>早期算法  </p><p>  早期的算法有基于几何特征的算法，基于模板匹配的算法，子空间算法等多种类型。子空间算法将人脸图像当成一个高维的向量，将向量投影到低维空间中，投影之后得到的低维向量达到对不同的人具有良好的区分度。</p><p>  子空间算法的典型代表是PCA（主成分分析，也称为特征脸EigenFace）[1]和LDA（线性判别分析，FisherFace）[2]。PCA的核心思想是在进行投影之后尽量多的保留原始数据的主要信息，降低数据的冗余信息，以利于后续的识别。LDA的核心思想是最大化类间差异，最小化类内差异，即保证同一个人的不同人脸图像在投影之后聚集在一起，不同人的人脸图像在投影之后被用一个大的间距分开。PCA和LDA最后都归结于求解矩阵的特征值和特征向量，这有成熟的数值算法可以实现。</p><p>  PCA和LDA都是线性降维技术，但人脸在高维空间中的分布显然是非线性的，因此可以使用非线性降维算法，典型的代表是流形学习[3]和核（kernel）技术。流形学习假设向量点在高维空间中的分布具有某些几何形状，然后在保持这些几何形状约束的前提下将向量投影到低维空间中，这种投影是通过非线性变换完成的。  </p></li><li><p>人工特征 + 分类器  </p><p>  第二阶段的人脸识别算法普遍采用了人工特征 + 分类器的思路。分类器有成熟的方案，如神经网络，支持向量机[7]，贝叶斯[8]等。这里的关键是人工特征的设计，它要能有效的区分不同的人。</p><p>  描述图像的很多特征都先后被用于人脸识别问题，包括HOG、SIFT、Gabor、LBP等。它们中的典型代表是LBP（局部二值模式）特征[9]，这种特征简单却有效。LBP特征计算起来非常简单，部分解决了光照敏感问题，但还是存在姿态和表情的问题。</p><p>  联合贝叶斯是对贝叶斯人脸的改进方法[8]，选用LBP和LE作为基础特征，将人脸图像的差异表示为相同人因姿态、表情等导致的差异以及不同人间的差异两个因素，用潜在变量组成的协方差，建立两张人脸的关联。文章的创新点在于将两个人脸表示进行联合建模，在人脸联合建模的时候，又使用了人脸的先验知识，将两张人脸的建模问题变为单张人脸图片的统计计算，更好的验证人脸的相关性，该方法在LFW上取得了92.4%的准确率。</p><p>  人工特征的巅峰之作是出自CVPR 2013年MSRA的”Blessing of Dimisionality: High Dimensional Feature and Its Efficient Compression for Face Verification” [10]，一篇关于如何使用高维度特征在人脸验证中的文章，作者主要以LBP（Local Binary Pattern，局部二值特征）为例子，论述了高维特征和验证性能存在着正相关的关系，即人脸维度越高，验证的准确度就越高。</p></li><li><p>深度学习  </p><p>  第三个阶段是基于深度学习的方法，自2012年深度学习在ILSVRC-2012大放异彩后，很多研究者都在尝试将其应用在自己的方向，这极大的推动了深度学习的发展。卷积神经网络在图像分类中显示出了巨大的威力，通过学习得到的卷积核明显优于人工设计的特征+分类器的方案。在人脸识别的研究者利用卷积神经网络（CNN）对海量的人脸图片进行学习，然后对输入图像提取出对区分不同人的脸有用的特征向量，替代人工设计的特征。</p><p>  在前期，研究人员在网络结构、输入数据的设计等方面尝试了各种方案，然后送入卷积神经网络进行经典的目标分类模型训练；在后期，主要的改进集中在损失函数上，即迫使卷积网络学习得到对分辨不同的人更有效的特征，这时候人脸识别领域彻底被深度学习改造了！</p><p>  DeepFace[11]是CVPR2014上由Facebook提出的方法，是深度卷积神经网络在人脸识别领域的奠基之作，文中使用了3D模型来做人脸对齐任务，深度卷积神经网络针对对齐后的人脸Patch进行多类的分类学习，使用的是经典的交叉熵损失函数（Softmax）进行问题优化，最后通过特征嵌入（Feature Embedding）得到固定长度的人脸特征向量。Backbone网络使用了多层局部卷积结构（Local Convolution），原因是希望网络的不同卷积核能学习人脸不同区域的特征，但会导致参数量增大，要求数据量很大，回过头去看该策略并不是十分必要。<br>  DeepFace在LFW上取得了97.35%的准确率，已经接近了人类的水平。之后Google推出FaceNet（<a href="https://arxiv.org/abs/1503.03832" target="_blank" rel="noopener">Facenet论文地址</a>），使用三元组损失函数(Triplet Loss)代替常用的Softmax交叉熵损失函数，在一个超球空间上进行优化使类内距离更紧凑，类间距离更远，最后得到了一个紧凑的128维人脸特征，其网络使用GoogLeNet的Inception模型，模型参数量较小，精度更高，在LFW上取得了99.63%的准确率，这种损失函数的思想也可以追溯到早期的LDA算法。<br>  CVPR2014、CVPR2015香港中文大学汤晓鸥团队提出的DeepID系列是一组非常有代表性的工作，其中DeepID1[12]使用四层卷积，最后一层为Softmax，中间为Deep Hidden Identity Features，是学习到的人脸特征表示，并使用Multi-patch分别训练模型最后组合成高维特征，人脸验证阶段使用联合贝叶斯的方法；通过学习一个多类（10000类，每个类大约有20个实例）人脸识别任务来学习特征，文中指出，随着训练时要预测的人脸类越多，DeepID的泛化能力就越强。</p></li></ul><h1 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h1><h2 id="算法仓库"><a href="#算法仓库" class="headerlink" title="算法仓库"></a><strong>算法仓库</strong></h2><ul><li><p>ageitgey/face_recognition:  </p><p>  <a href="https://github.com/ageitgey/face_recognition" target="_blank" rel="noopener">https://github.com/ageitgey/face_recognition</a>  </p></li><li><p>davidsandberg/facenet:  </p><p>  <a href="https://github.com/davidsandberg/facenet" target="_blank" rel="noopener">https://github.com/davidsandberg/facenet</a>  </p></li><li><p>cmusatyalab/openface:  </p><p>  <a href="https://github.com/cmusatyalab/openface" target="_blank" rel="noopener">https://github.com/cmusatyalab/openface</a> </p></li><li><p>kpzhang93/MTCNN_face_detection_alignment(人脸检测):  </p><p>  <a href="https://github.com/kpzhang93/MTCNN_face_detection_alignment" target="_blank" rel="noopener">https://github.com/kpzhang93/MTCNN_face_detection_alignment</a>  </p></li><li><p>deepinsight/insightface:  </p><p>  <a href="https://github.com/deepinsight/insightface" target="_blank" rel="noopener">https://github.com/deepinsight/insightface</a>  </p></li><li><p>nyoki-mtl/keras-facenet:  </p><p>  <a href="https://github.com/nyoki-mtl/keras-facenet" target="_blank" rel="noopener">https://github.com/nyoki-mtl/keras-facenet</a>  </p></li><li><p>yuyang-huang/keras-inception-resnet-v2(网络结构):</p><p>  <a href="https://github.com/yuyang-huang/keras-inception-resnet-v2" target="_blank" rel="noopener">https://github.com/yuyang-huang/keras-inception-resnet-v2</a>  </p></li><li><p>yobibyte/yobiface:<br>  <a href="https://github.com/yobibyte/yobiface/tree/master/src" target="_blank" rel="noopener">https://github.com/yobibyte/yobiface/tree/master/src</a></p></li></ul><p><strong>相关博客</strong></p><ul><li><p>应用一个基于Python的开源人脸识别库，face_recognition:<br><a href="https://blog.csdn.net/hongbin_xu/article/details/76284134" target="_blank" rel="noopener">https://blog.csdn.net/hongbin_xu/article/details/76284134</a>  </p></li><li><p>TensorFlow–实现人脸识别实验精讲 （Face Recognition using Tensorflow）:<br><a href="https://blog.csdn.net/niutianzhuang/article/details/79191167" target="_blank" rel="noopener">https://blog.csdn.net/niutianzhuang/article/details/79191167</a>  </p></li><li><p>基于卷积神经网络和tensorflow实现的人脸识别:<br><a href="https://blog.csdn.net/hy13684802853/article/details/79780805" target="_blank" rel="noopener">https://blog.csdn.net/hy13684802853/article/details/79780805</a></p></li><li><p>keras/构建卷积神经网络人脸识别:</p><p>  <a href="https://blog.csdn.net/szj_huhu/article/details/75202254" target="_blank" rel="noopener">https://blog.csdn.net/szj_huhu/article/details/75202254</a></p></li><li><p>人脸识别–(opencv、dlib、keras-TensorFlow）:<br><a href="https://blog.csdn.net/u014258362/article/details/80688224" target="_blank" rel="noopener">https://blog.csdn.net/u014258362/article/details/80688224</a>  </p></li><li><p>TensorFlow实现人脸识别(5)——-利用训练好的模型实时进行人脸检测:<br><a href="https://blog.csdn.net/yunge812/article/details/79447584" target="_blank" rel="noopener">https://blog.csdn.net/yunge812/article/details/79447584</a>  </p></li><li><p>基于keras的人脸识别:<br><a href="https://blog.csdn.net/Julymycin/article/details/79182222" target="_blank" rel="noopener">https://blog.csdn.net/Julymycin/article/details/79182222</a>  </p></li><li><p>史上最全的FaceNet源码使用方法和讲解（一）（附预训练模型下载）:<br><a href="https://blog.csdn.net/u013044310/article/details/79556099" target="_blank" rel="noopener">https://blog.csdn.net/u013044310/article/details/79556099</a><br><a href="https://github.com/boyliwensheng/understand_facenet(作者整理代码)" target="_blank" rel="noopener">https://github.com/boyliwensheng/understand_facenet(作者整理代码)</a>  </p></li><li><p>基于 MTCNN/TensorFlow 实现人脸检测:<br><a href="https://blog.csdn.net/Mr_EvanChen/article/details/77650883" target="_blank" rel="noopener">https://blog.csdn.net/Mr_EvanChen/article/details/77650883</a></p></li><li><p>计算机视觉实时目标检测 TensorFlow Object Detection API<br><a href="https://blog.csdn.net/chenhaifeng2016/article/details/74205717" target="_blank" rel="noopener">https://blog.csdn.net/chenhaifeng2016/article/details/74205717</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载自&lt;a href=&quot;https://zhuanlan.zhihu.com/p/36416906，如有侵权，请联系删除。&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/36416906，如有侵权，
      
    
    </summary>
    
      <category term="算法" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="人脸识别" scheme="http://yoursite.com/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>Nodejs与Django的跨域问题</title>
    <link href="http://yoursite.com/2018/07/08/Nodejs%E4%B8%8EDjango%E7%9A%84%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2018/07/08/Nodejs与Django的跨域问题/</id>
    <published>2018-07-08T12:23:10.000Z</published>
    <updated>2018-07-09T00:38:12.861Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Nodejs与Django的跨域问题"><a href="#Nodejs与Django的跨域问题" class="headerlink" title="Nodejs与Django的跨域问题"></a>Nodejs与Django的跨域问题</h2><p>由于采用前后端分离的编程方式，Django的csrf_token验证失效，出现跨域问题，在此记录一下解决方法。</p><ul><li><p>1 安装django-cors-headers  </p><p>  pip install django-cors-headers</p></li></ul><ul><li>2 配置settings.py文件<br><img src="/2018/07/08/Nodejs与Django的跨域问题/跨域问题模块.png" alt="Nodejs与Django的跨域问题">  </li></ul><p><img src="/2018/07/08/Nodejs与Django的跨域问题/跨域问题设置.png" alt="Nodejs与Django的跨域问题">  </p><p>OK！问题解决！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Nodejs与Django的跨域问题&quot;&gt;&lt;a href=&quot;#Nodejs与Django的跨域问题&quot; class=&quot;headerlink&quot; title=&quot;Nodejs与Django的跨域问题&quot;&gt;&lt;/a&gt;Nodejs与Django的跨域问题&lt;/h2&gt;&lt;p&gt;由于采用前后
      
    
    </summary>
    
      <category term="Web开发" scheme="http://yoursite.com/categories/Web%E5%BC%80%E5%8F%91/"/>
    
    
      <category term="Django" scheme="http://yoursite.com/tags/Django/"/>
    
      <category term="Nodejs" scheme="http://yoursite.com/tags/Nodejs/"/>
    
  </entry>
  
  <entry>
    <title>Django使用MongoDB数据库</title>
    <link href="http://yoursite.com/2018/07/08/Django%E4%BD%BF%E7%94%A8MongoDB%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    <id>http://yoursite.com/2018/07/08/Django使用MongoDB数据库/</id>
    <published>2018-07-08T12:22:47.000Z</published>
    <updated>2018-08-05T10:11:43.125Z</updated>
    
    <content type="html"><![CDATA[<p>之前学习使用Django搭建在线教育平台使用的是Mysql数据库，现在考虑到公司以后的发展及当前技术需求，更换为MongoDB数据库。在此记录一下更改操作：  </p><ul><li><p>1 首先安装mongoengine，并在setting中设置对应的位置  </p><p>  pip instal mongoengine</p></li></ul><p><img src="/2018/07/08/Django使用MongoDB数据库/installs安装内容.png" alt="数据库设置"></p><ul><li>2 设置默认的数据库信息  </li></ul><p><img src="/2018/07/08/Django使用MongoDB数据库/数据库设置.png" alt="数据库设置">  </p><ul><li>3 设置Model</li></ul><p><img src="/2018/07/08/Django使用MongoDB数据库/model设置.png" alt="数据库设置">  </p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>之前因为使用Django自带的admin后台管理系统，所以在像Mysql一样迁移数据库时出现错误。后来分析发现，目前使用Django所做的工作不需要用到后台管理系统，仅仅是作为一个后台服务，因此可直接运行。至此，设置完成。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;之前学习使用Django搭建在线教育平台使用的是Mysql数据库，现在考虑到公司以后的发展及当前技术需求，更换为MongoDB数据库。在此记录一下更改操作：  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1 首先安装mongoengine，并在setting中设置对应的位置  &lt;/p
      
    
    </summary>
    
      <category term="Web开发" scheme="http://yoursite.com/categories/Web%E5%BC%80%E5%8F%91/"/>
    
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>CentOS7为firewalld添加开放端口及相关操作</title>
    <link href="http://yoursite.com/2018/07/05/CentOS7%E4%B8%BAfirewalld%E6%B7%BB%E5%8A%A0%E5%BC%80%E6%94%BE%E7%AB%AF%E5%8F%A3%E5%8F%8A%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/"/>
    <id>http://yoursite.com/2018/07/05/CentOS7为firewalld添加开放端口及相关操作/</id>
    <published>2018-07-05T13:32:17.000Z</published>
    <updated>2018-07-08T12:39:48.527Z</updated>
    
    <content type="html"><![CDATA[<h3 id="firewalld的基本使用"><a href="#firewalld的基本使用" class="headerlink" title="firewalld的基本使用"></a>firewalld的基本使用</h3><pre><code>启动： systemctl start firewalld查看状态： systemctl status firewalld 停止： systemctl disable firewalld禁用： systemctl stop firewalld</code></pre><h3 id="systemctl是CentOS7的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。"><a href="#systemctl是CentOS7的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。" class="headerlink" title="systemctl是CentOS7的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。"></a>systemctl是CentOS7的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。</h3><pre><code>启动一个服务：systemctl start firewalld.service关闭一个服务：systemctlstop firewalld.service重启一个服务：systemctlrestart firewalld.service显示一个服务的状态：systemctlstatus firewalld.service在开机时启用一个服务：systemctlenable firewalld.service在开机时禁用一个服务：systemctldisable firewalld.service查看服务是否开机启动：systemctlis-enabled firewalld.service查看已启动的服务列表：systemctllist-unit-files|grep enabled查看启动失败的服务列表：systemctl--failed</code></pre><h3 id="配置firewalld-cmd"><a href="#配置firewalld-cmd" class="headerlink" title="配置firewalld-cmd"></a>配置firewalld-cmd</h3><pre><code>查看版本： firewall-cmd --version查看帮助： firewall-cmd --help显示状态： firewall-cmd --state查看所有打开的端口： firewall-cmd--zone=public --list-ports更新防火墙规则： firewall-cmd --reload查看区域信息:  firewall-cmd--get-active-zones查看指定接口所属区域： firewall-cmd--get-zone-of-interface=eth0拒绝所有包：firewall-cmd --panic-on取消拒绝状态： firewall-cmd --panic-off查看是否拒绝： firewall-cmd --query-panic</code></pre><h4 id="添加"><a href="#添加" class="headerlink" title="添加"></a>添加</h4><pre><code>firewall-cmd --zone=public --add-port=80/tcp --permanent   （--permanent永久生效，没有此参数重启后失效）</code></pre><h4 id="重新载入"><a href="#重新载入" class="headerlink" title="重新载入"></a>重新载入</h4><pre><code>firewall-cmd --reload</code></pre><h4 id="查看"><a href="#查看" class="headerlink" title="查看"></a>查看</h4><pre><code>firewall-cmd --zone=public --query-port=80/tcp</code></pre><h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4><pre><code>firewall-cmd --zone=public --remove-port=80/tcp --permanent</code></pre><h3 id="查看firewall是否运行-下面两个命令都可以"><a href="#查看firewall是否运行-下面两个命令都可以" class="headerlink" title="查看firewall是否运行,下面两个命令都可以"></a>查看firewall是否运行,下面两个命令都可以</h3><pre><code>systemctl status firewalld.servicefirewall-cmd --state</code></pre><h3 id="查看当前开了哪些端口"><a href="#查看当前开了哪些端口" class="headerlink" title="查看当前开了哪些端口"></a>查看当前开了哪些端口</h3><p>其实一个服务对应一个端口，每个服务对应/usr/lib/firewalld/services下面一个xml文件。</p><pre><code>firewall-cmd --list-services</code></pre><h3 id="查看还有哪些服务可以打开"><a href="#查看还有哪些服务可以打开" class="headerlink" title="查看还有哪些服务可以打开"></a>查看还有哪些服务可以打开</h3><pre><code>firewall-cmd --get-services</code></pre><h3 id="查看所有打开的端口："><a href="#查看所有打开的端口：" class="headerlink" title="查看所有打开的端口："></a>查看所有打开的端口：</h3><pre><code>firewall-cmd --zone=public --list-ports</code></pre><h3 id="更新防火墙规则："><a href="#更新防火墙规则：" class="headerlink" title="更新防火墙规则："></a>更新防火墙规则：</h3><pre><code>firewall-cmd --reload</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;firewalld的基本使用&quot;&gt;&lt;a href=&quot;#firewalld的基本使用&quot; class=&quot;headerlink&quot; title=&quot;firewalld的基本使用&quot;&gt;&lt;/a&gt;firewalld的基本使用&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;启动： systemctl 
      
    
    </summary>
    
      <category term="笔记" scheme="http://yoursite.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CentOS7" scheme="http://yoursite.com/tags/CentOS7/"/>
    
      <category term="firewalld" scheme="http://yoursite.com/tags/firewalld/"/>
    
  </entry>
  
</feed>
