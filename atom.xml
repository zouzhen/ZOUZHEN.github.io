<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZOUZHEN_BLOG</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-04-01T02:27:52.666Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ZOUZHEN</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>搭建python沙盒环境</title>
    <link href="http://yoursite.com/2019/04/01/%E6%90%AD%E5%BB%BApython%E6%B2%99%E7%9B%92%E7%8E%AF%E5%A2%83/"/>
    <id>http://yoursite.com/2019/04/01/搭建python沙盒环境/</id>
    <published>2019-04-01T02:23:52.000Z</published>
    <updated>2019-04-01T02:27:52.666Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一-先update-sudo-apt-update"><a href="#一-先update-sudo-apt-update" class="headerlink" title="一. 先update    sudo apt update"></a>一. 先update    sudo apt update</h3><h3 id="二-安装pip-sudo-apt-install-python-pip"><a href="#二-安装pip-sudo-apt-install-python-pip" class="headerlink" title="二. 安装pip       sudo apt install python-pip"></a>二. 安装pip       sudo apt install python-pip</h3><h3 id="三-安装virtualenv-sudo-pip-install-virtualenv-virtualenvwrapper"><a href="#三-安装virtualenv-sudo-pip-install-virtualenv-virtualenvwrapper" class="headerlink" title="三.  安装virtualenv    sudo pip install virtualenv  virtualenvwrapper"></a>三.  安装virtualenv    sudo pip install virtualenv  virtualenvwrapper</h3><h3 id="四-创建目录用来存放虚拟环境-sudo-mkdir-p-WORKON-HOME"><a href="#四-创建目录用来存放虚拟环境-sudo-mkdir-p-WORKON-HOME" class="headerlink" title="四. 创建目录用来存放虚拟环境    sudo mkdir -p $WORKON_HOME"></a>四. 创建目录用来存放虚拟环境    sudo mkdir -p $WORKON_HOME</h3><h3 id="五-在用户目录下中的-bashrc-中添加以下内容并保存-通过ll-可查看到-bashrc-文件"><a href="#五-在用户目录下中的-bashrc-中添加以下内容并保存-通过ll-可查看到-bashrc-文件" class="headerlink" title="五. 在用户目录下中的 .bashrc 中添加以下内容并保存(通过ll 可查看到 .bashrc 文件)"></a>五. 在用户目录下中的 .bashrc 中添加以下内容并保存(通过ll 可查看到 .bashrc 文件)</h3><p>if [ -f /usr/local/bin/virtualenvwrapper.sh ]; then<br>    export WORKON_HOME=$HOME/.virtualenvs<br>    source /usr/local/bin/virtualenvwrapper.sh<br>fi  </p><h3 id="六-运行-source-bashrc-重新加载到环境变量中"><a href="#六-运行-source-bashrc-重新加载到环境变量中" class="headerlink" title="六. 运行  source .bashrc   重新加载到环境变量中"></a>六. 运行  source .bashrc   重新加载到环境变量中</h3><h3 id="七-创建虚拟环境-mkvirtualenv-test1-关于虚拟环境的命令如下"><a href="#七-创建虚拟环境-mkvirtualenv-test1-关于虚拟环境的命令如下" class="headerlink" title="七. 创建虚拟环境     mkvirtualenv  test1             关于虚拟环境的命令如下"></a>七. 创建虚拟环境     mkvirtualenv  test1             关于虚拟环境的命令如下</h3><p>mkvirtualenv wxhpython01：创建运行环境wxhpython01<br>workon wxhpython01: 工作在 zqxt 环境 或 从其它环境切换到 wxhpython01环境<br>deactivate: 退出终端环境<br>rmvirtualenv ENV：删除运行环境ENV<br>mkproject mic：创建mic项目和运行环境mic<br>mktmpenv：创建临时运行环境<br>lsvirtualenv: 列出可用的运行环境<br>lssitepackages: 列出当前环境安装了的包  </p><p>八.在虚拟环境中可以通过pip安装其他的所需包  例如 pip install django==1.9.8</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;一-先update-sudo-apt-update&quot;&gt;&lt;a href=&quot;#一-先update-sudo-apt-update&quot; class=&quot;headerlink&quot; title=&quot;一. 先update    sudo apt update&quot;&gt;&lt;/a&gt;一. 先upd
      
    
    </summary>
    
      <category term="环境配置" scheme="http://yoursite.com/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="沙盒" scheme="http://yoursite.com/tags/%E6%B2%99%E7%9B%92/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>常用的损失函数</title>
    <link href="http://yoursite.com/2019/03/30/%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2019/03/30/常用的损失函数/</id>
    <published>2019-03-30T07:04:20.000Z</published>
    <updated>2019-03-30T07:18:25.829Z</updated>
    
    <content type="html"><![CDATA[<h3 id="MAE：平均绝对误差-mean-absolute-error-，对应位置差值的绝对值之和"><a href="#MAE：平均绝对误差-mean-absolute-error-，对应位置差值的绝对值之和" class="headerlink" title="MAE：平均绝对误差(mean absolute error)，对应位置差值的绝对值之和"></a>MAE：平均绝对误差(mean absolute error)，对应位置差值的绝对值之和</h3><p>$$J(\theta) = \frac{1}{mn}\sum_{i=1}^m\sum_{j=1}^n|\hat y_{ij}-y_{ij}|$$ </p><h3 id="MSE：均方误差-mean-squared-error-，对应位置差值的平方之和"><a href="#MSE：均方误差-mean-squared-error-，对应位置差值的平方之和" class="headerlink" title="MSE：均方误差(mean squared error)，对应位置差值的平方之和"></a>MSE：均方误差(mean squared error)，对应位置差值的平方之和</h3><p>$$J(\theta) = \frac{1}{mn}\sum_{i=1}^m\sum_{j=1}^n(\hat y_{ij}-y_{ij})^2$$</p><h3 id="两种损失函数的性质"><a href="#两种损失函数的性质" class="headerlink" title="两种损失函数的性质"></a>两种损失函数的性质</h3><h4 id="异常值"><a href="#异常值" class="headerlink" title="异常值"></a>异常值</h4><p>MSE对异常值敏感，因为它的惩罚是平方的，所以异常值的loss会非常大。<br>MAE对异常之不敏感，</p><p>不妨设拟合函数为常数，那么MSE就相当于所有数据的均值（列出loss对c求导即可），而MAE相当于所有数据的中位数，所以会对异常值不敏感。</p><h4 id="优化效率"><a href="#优化效率" class="headerlink" title="优化效率"></a>优化效率</h4><p>MAE不可导而且所有的导数的绝对值都相同，优化时无法确定更新速度，<br>MSE可导，有closed-form解，只需要令偏导数为0即可。</p><h4 id="如何选择"><a href="#如何选择" class="headerlink" title="如何选择"></a>如何选择</h4><p>如果想要检测异常值则使用MSE，如果想学习一个预测模型则建议使用MAE，或者先进行异常值处理再使用MSE</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;MAE：平均绝对误差-mean-absolute-error-，对应位置差值的绝对值之和&quot;&gt;&lt;a href=&quot;#MAE：平均绝对误差-mean-absolute-error-，对应位置差值的绝对值之和&quot; class=&quot;headerlink&quot; title=&quot;MAE：
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="损失函数" scheme="http://yoursite.com/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>ssh传输文件</title>
    <link href="http://yoursite.com/2019/03/28/ssh%E4%BC%A0%E8%BE%93%E6%96%87%E4%BB%B6/"/>
    <id>http://yoursite.com/2019/03/28/ssh传输文件/</id>
    <published>2019-03-28T03:27:27.000Z</published>
    <updated>2019-04-01T02:12:32.014Z</updated>
    
    <content type="html"><![CDATA[<h2 id="利用ssh传输文件"><a href="#利用ssh传输文件" class="headerlink" title="利用ssh传输文件"></a>利用ssh传输文件</h2><p>在linux下一般用scp这个命令来通过ssh传输文件。</p><h3 id="1、从服务器上下载文件"><a href="#1、从服务器上下载文件" class="headerlink" title="1、从服务器上下载文件"></a>1、从服务器上下载文件</h3><p>scp username@servername:/path/filename /var/www/local_dir（本地目录）</p><p> 例如scp <a href="mailto:root@192.168.0.101" target="_blank" rel="noopener">root@192.168.0.101</a>:/var/www/test.txt  把192.168.0.101上的/var/www/test.txt 的文件下载到/var/www/local_dir（本地目录）</p><h3 id="2、上传本地文件到服务器"><a href="#2、上传本地文件到服务器" class="headerlink" title="2、上传本地文件到服务器"></a>2、上传本地文件到服务器</h3><p>scp /path/filename username@servername:/path   </p><p>例如scp /var/www/test.php  <a href="mailto:root@192.168.0.101" target="_blank" rel="noopener">root@192.168.0.101</a>:/var/www/  把本机/var/www/目录下的test.php文件上传到192.168.0.101这台服务器上的/var/www/目录中</p><h3 id="3、从服务器下载整个目录"><a href="#3、从服务器下载整个目录" class="headerlink" title="3、从服务器下载整个目录"></a>3、从服务器下载整个目录</h3><p>scp -r username@servername:/var/www/remote_dir/（远程目录） /var/www/local_dir（本地目录）</p><p>例如:scp -r <a href="mailto:root@192.168.0.101" target="_blank" rel="noopener">root@192.168.0.101</a>:/var/www/test  /var/www/  </p><h3 id="4、上传目录到服务器"><a href="#4、上传目录到服务器" class="headerlink" title="4、上传目录到服务器"></a>4、上传目录到服务器</h3><p>scp  -r local_dir username@servername:remote_dir<br>例如：scp -r test  <a href="mailto:root@192.168.0.101" target="_blank" rel="noopener">root@192.168.0.101</a>:/var/www/   把当前目录下的test目录上传到服务器的/var/www/ 目录</p><p>注：目标服务器要开启写入权限。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;利用ssh传输文件&quot;&gt;&lt;a href=&quot;#利用ssh传输文件&quot; class=&quot;headerlink&quot; title=&quot;利用ssh传输文件&quot;&gt;&lt;/a&gt;利用ssh传输文件&lt;/h2&gt;&lt;p&gt;在linux下一般用scp这个命令来通过ssh传输文件。&lt;/p&gt;
&lt;h3 id=&quot;1
      
    
    </summary>
    
      <category term="远程操作" scheme="http://yoursite.com/categories/%E8%BF%9C%E7%A8%8B%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="ssh" scheme="http://yoursite.com/tags/ssh/"/>
    
  </entry>
  
  <entry>
    <title>深度学习框架简介</title>
    <link href="http://yoursite.com/2019/03/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E7%AE%80%E4%BB%8B/"/>
    <id>http://yoursite.com/2019/03/18/深度学习框架简介/</id>
    <published>2019-03-18T10:02:24.000Z</published>
    <updated>2019-03-28T03:30:28.991Z</updated>
    
    <content type="html"><![CDATA[<h2 id="theano"><a href="#theano" class="headerlink" title="theano"></a>theano</h2><p>theano最初于2008年开始开发，是第一个有较大影响力的框架。<br>theano是一个Python库，可用于定义、优化和计算数学表达式，特别是多维数组（numpy.ndarray）<br>theano诞生于研究机构，服务于研究人员，其设计具有较浓厚的学术气息，但在工业设计上有较大的缺陷。</p><h2 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h2><p>Tendorflow是Google于2015年11月10日推出的全新的机器学习开源框架。Tensorflow在很大程度上可以看作是Theano的后继者，都是基于计算图实现自动微分系统。Tensoflow支持多种语言，Java、Go、R和Haskell的alpha版本也被支持。<br>但是Tensorflow也存在一些问题：</p><ul><li>过于复杂的系统设计</li><li>频繁变动的接口</li><li>接口设计过于晦涩难懂</li><li>文档混乱脱节</li></ul><p>直接使用Tensorflow的生产力很低下，因而出现了Keras等第三方封装库（PS：我不认为Keras等第三方封装库可以被称之为深度学习框架）<br>虽然不完美，但是最流行的深度学习框架，社区强大，适合生产环境</p><h2 id="Caffe-Caffe2"><a href="#Caffe-Caffe2" class="headerlink" title="Caffe/Caffe2"></a>Caffe/Caffe2</h2><p>caffe是一个清晰、高效的深度学习框架，可信语言是C++，既可以在CPU上运行，也可以在GPU上运行。其优点为简洁快速、缺点是缺少灵活性。Caffe灵活性的缺失主要是因为它的设计。caffe的作者在FAIR担任主管的时候，开发了Caffe2，Caffe2的设计追求轻量级，在保有扩展性和高性能的同时，Caffe2也强调了便携性。<br>文档不够完善，但性能优异，几乎全平台支持Caffe2，适合生产环境。</p><h2 id="MXNet"><a href="#MXNet" class="headerlink" title="MXNet"></a>MXNet</h2><p>MXNet是一个深度学习库，支持C++、Python、JavaScript等语言；支持命令和符号编程；可以运行在CPU、GPU、集群、服务器、台式机或者移动设备上。<br>MXNet以其超强的分布式支持，明显的内存、显存优化为人所称道。<br>文档略混乱、但分布式性能强大、语言支持最多、适合AWS平台开发。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;theano&quot;&gt;&lt;a href=&quot;#theano&quot; class=&quot;headerlink&quot; title=&quot;theano&quot;&gt;&lt;/a&gt;theano&lt;/h2&gt;&lt;p&gt;theano最初于2008年开始开发，是第一个有较大影响力的框架。&lt;br&gt;theano是一个Python库，
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>框架梳理——RetinaNet/keras</title>
    <link href="http://yoursite.com/2019/01/17/%E6%A1%86%E6%9E%B6%E6%A2%B3%E7%90%86%E2%80%94%E2%80%94RetinaNet-keras/"/>
    <id>http://yoursite.com/2019/01/17/框架梳理——RetinaNet-keras/</id>
    <published>2019-01-17T06:28:36.000Z</published>
    <updated>2019-03-28T03:30:28.985Z</updated>
    
    <content type="html"><![CDATA[<h1 id="框架梳理——RetinaNet-keras"><a href="#框架梳理——RetinaNet-keras" class="headerlink" title="框架梳理——RetinaNet/keras"></a>框架梳理——RetinaNet/keras</h1><p>代码地址：<a href="https://travis-ci.org/fizyr/keras-retinanet" target="_blank" rel="noopener">https://travis-ci.org/fizyr/keras-retinanet</a></p><h2 id="文件目录"><a href="#文件目录" class="headerlink" title="文件目录"></a>文件目录</h2><ul><li><p>keras-retinanet  </p><p>  │<br>  ├── examples(例图和测试Notebook)<br>  │<br>  ├── images(测试图片)<br>  │<br>  ├── keras_retinanet(主要的框架代码)<br>  │<br>  ├── snapshots(模型保存地址)<br>  │<br>  └── tests(测试文件夹，包含对模型的各种测试)</p></li><li><p>keras_retinanet(主要的框架代码)<br>  │<br>  ├── backend(tf或者th相关的设置)<br>  │<br>  ├── bin(包含模型转换、调试、评价、训练等文件)<br>  │<br>  ├── callbacks(与召回相关的函数)<br>  │<br>  ├── layers(基础网络层之上的采样层处理，过滤预选框)<br>  │<br>  ├── callbacks(与召回相关的函数)<br>  │<br>  ├── models(基础网络文件夹，包括densenet、mobilenet、resnet、retinanet、vgg等网络)<br>  │<br>  ├── preprocessing(数据集相关，包括各种数据集的获取、解析等)<br>  │<br>  ├── utils(附属工具，包括锚点框、非最大值抑制等)<br>  │<br>  └── losses.py/initializers.py(损失和初始化等)</p></li></ul><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>关于模型的训练文件，在bin文件夹下，其训练文件的流程大致如下</p><p><img src="/2019/01/17/框架梳理——RetinaNet-keras/keras-retinanet解析.jpg" alt="图片"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;框架梳理——RetinaNet-keras&quot;&gt;&lt;a href=&quot;#框架梳理——RetinaNet-keras&quot; class=&quot;headerlink&quot; title=&quot;框架梳理——RetinaNet/keras&quot;&gt;&lt;/a&gt;框架梳理——RetinaNet/keras&lt;/
      
    
    </summary>
    
      <category term="框架实现" scheme="http://yoursite.com/categories/%E6%A1%86%E6%9E%B6%E5%AE%9E%E7%8E%B0/"/>
    
    
      <category term="RetinaNet" scheme="http://yoursite.com/tags/RetinaNet/"/>
    
      <category term="keras" scheme="http://yoursite.com/tags/keras/"/>
    
  </entry>
  
  <entry>
    <title>keras-RetinaNet问题记录</title>
    <link href="http://yoursite.com/2019/01/16/keras-RetinaNet%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2019/01/16/keras-RetinaNet问题记录/</id>
    <published>2019-01-16T07:39:30.000Z</published>
    <updated>2019-03-28T03:30:28.971Z</updated>
    
    <content type="html"><![CDATA[<h2 id="框架搭建"><a href="#框架搭建" class="headerlink" title="框架搭建"></a>框架搭建</h2><h3 id="获取代码"><a href="#获取代码" class="headerlink" title="获取代码"></a>获取代码</h3><ul><li><p>克隆代码库。  </p><p>  git clone <a href="https://github.com/fizyr/keras-retinanet.git" target="_blank" rel="noopener">https://github.com/fizyr/keras-retinanet.git</a></p></li></ul><h3 id="编译支持"><a href="#编译支持" class="headerlink" title="编译支持"></a>编译支持</h3><p>编译Cython代码</p><ul><li>python setup.py build_ext –inplace</li></ul><h2 id="Retinanet训练Pascal-VOC-2007"><a href="#Retinanet训练Pascal-VOC-2007" class="headerlink" title="Retinanet训练Pascal VOC 2007"></a>Retinanet训练Pascal VOC 2007</h2><h3 id="train"><a href="#train" class="headerlink" title="train"></a>train</h3><pre><code># trainpython3 keras_retinanet/bin/train.py pascal /path/to/VOCdevkit/VOC2007# 使用 --backbone=xxx 选择网络结构，默认是resnet50# xxx可以是resnet模型（`resnet50`，`resnet101`，`resnet152`）# 或`mobilenet`模型（`mobilenet128_1.0`，`mobilenet128_0.75`，`mobilenet160_1.0`等）# 也可以使用models目录下的 resnet.py，mobilenet.py等来自定义网络</code></pre><h3 id="test"><a href="#test" class="headerlink" title="test"></a>test</h3><p>1 首先需要进行模型转换，将训练好的模型转换为测试所需模型，<br>keras-retinanet的训练程序与训练模型一起使用。 与测试模型相比，这些是精简版本，仅包含培训所需的层（回归和分类值）。 如果您希望对模型进行测试（对图像执行对象检测），则需要将训练模型转换为测试模型。</p><pre><code># Running directly from the repository:keras_retinanet/bin/convert_model.py /path/to/training/model.h5 /path/to/save/inference/model.h5# Using the installed script:retinanet-convert-model /path/to/training/model.h5 /path/to/save/inference/model.h5</code></pre><p>2 测试代码</p><pre><code># import kerasimport keras# import keras_retinanetfrom keras_retinanet import modelsfrom keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_imagefrom keras_retinanet.utils.visualization import draw_box, draw_captionfrom keras_retinanet.utils.colors import label_color# import miscellaneous modulesimport matplotlib.pyplot as pltimport cv2import osimport numpy as npimport time# set tf backend to allow memory to grow, instead of claiming everythingimport tensorflow as tfdef get_session():    config = tf.ConfigProto()    config.gpu_options.allow_growth = True    return tf.Session(config=config)# use this environment flag to change which GPU to use#os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;1&quot;# set the modified tf session as backend in keraskeras.backend.tensorflow_backend.set_session(get_session())# adjust this to point to your downloaded/trained model# models can be downloaded here: https://github.com/fizyr/keras-retinanet/releasesmodel_path = os.path.join(&apos;..&apos;, &apos;snapshots&apos;, &apos;resnet50_coco_best_v2.1.0.h5&apos;)# load retinanet modelmodel = models.load_model(model_path, backbone_name=&apos;resnet50&apos;)# if the model is not converted to an inference model, use the line below# see: https://github.com/fizyr/keras-retinanet#converting-a-training-model-to-inference-model#model = models.convert_model(model)#print(model.summary())# load label to names mapping for visualization purposeslabels_to_names = {0: &apos;person&apos;, 1: &apos;bicycle&apos;, 2: &apos;car&apos;, 3: &apos;motorcycle&apos;, 4: &apos;airplane&apos;, 5: &apos;bus&apos;, 6: &apos;train&apos;, 7: &apos;truck&apos;, 8: &apos;boat&apos;, 9: &apos;traffic light&apos;, 10: &apos;fire hydrant&apos;, 11: &apos;stop sign&apos;, 12: &apos;parking meter&apos;, 13: &apos;bench&apos;, 14: &apos;bird&apos;, 15: &apos;cat&apos;, 16: &apos;dog&apos;, 17: &apos;horse&apos;, 18: &apos;sheep&apos;, 19: &apos;cow&apos;, 20: &apos;elephant&apos;, 21: &apos;bear&apos;, 22: &apos;zebra&apos;, 23: &apos;giraffe&apos;, 24: &apos;backpack&apos;, 25: &apos;umbrella&apos;, 26: &apos;handbag&apos;, 27: &apos;tie&apos;, 28: &apos;suitcase&apos;, 29: &apos;frisbee&apos;, 30: &apos;skis&apos;, 31: &apos;snowboard&apos;, 32: &apos;sports ball&apos;, 33: &apos;kite&apos;, 34: &apos;baseball bat&apos;, 35: &apos;baseball glove&apos;, 36: &apos;skateboard&apos;, 37: &apos;surfboard&apos;, 38: &apos;tennis racket&apos;, 39: &apos;bottle&apos;, 40: &apos;wine glass&apos;, 41: &apos;cup&apos;, 42: &apos;fork&apos;, 43: &apos;knife&apos;, 44: &apos;spoon&apos;, 45: &apos;bowl&apos;, 46: &apos;banana&apos;, 47: &apos;apple&apos;, 48: &apos;sandwich&apos;, 49: &apos;orange&apos;, 50: &apos;broccoli&apos;, 51: &apos;carrot&apos;, 52: &apos;hot dog&apos;, 53: &apos;pizza&apos;, 54: &apos;donut&apos;, 55: &apos;cake&apos;, 56: &apos;chair&apos;, 57: &apos;couch&apos;, 58: &apos;potted plant&apos;, 59: &apos;bed&apos;, 60: &apos;dining table&apos;, 61: &apos;toilet&apos;, 62: &apos;tv&apos;, 63: &apos;laptop&apos;, 64: &apos;mouse&apos;, 65: &apos;remote&apos;, 66: &apos;keyboard&apos;, 67: &apos;cell phone&apos;, 68: &apos;microwave&apos;, 69: &apos;oven&apos;, 70: &apos;toaster&apos;, 71: &apos;sink&apos;, 72: &apos;refrigerator&apos;, 73: &apos;book&apos;, 74: &apos;clock&apos;, 75: &apos;vase&apos;, 76: &apos;scissors&apos;, 77: &apos;teddy bear&apos;, 78: &apos;hair drier&apos;, 79: &apos;toothbrush&apos;}# load imageimage = read_image_bgr(&apos;000000008021.jpg&apos;)# copy to draw ondraw = image.copy()draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)# preprocess image for networkimage = preprocess_image(image)image, scale = resize_image(image)# process imagestart = time.time()boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))print(&quot;processing time: &quot;, time.time() - start)# correct for image scaleboxes /= scale# visualize detectionsfor box, score, label in zip(boxes[0], scores[0], labels[0]):    # scores are sorted so we can break    if score &lt; 0.5:        break    color = label_color(label)    b = box.astype(int)    draw_box(draw, b, color=color)    caption = &quot;{} {:.3f}&quot;.format(labels_to_names[label], score)    draw_caption(draw, b, caption)plt.figure(figsize=(15, 15))plt.axis(&apos;off&apos;)plt.imshow(draw)plt.show()</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;框架搭建&quot;&gt;&lt;a href=&quot;#框架搭建&quot; class=&quot;headerlink&quot; title=&quot;框架搭建&quot;&gt;&lt;/a&gt;框架搭建&lt;/h2&gt;&lt;h3 id=&quot;获取代码&quot;&gt;&lt;a href=&quot;#获取代码&quot; class=&quot;headerlink&quot; title=&quot;获取代码&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="算法实现" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/"/>
    
    
      <category term="RetinaNet" scheme="http://yoursite.com/tags/RetinaNet/"/>
    
  </entry>
  
  <entry>
    <title>FPN——CNN特征提取</title>
    <link href="http://yoursite.com/2019/01/14/FPN%E2%80%94%E2%80%94CNN%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    <id>http://yoursite.com/2019/01/14/FPN——CNN特征提取/</id>
    <published>2019-01-14T09:22:16.000Z</published>
    <updated>2019-03-28T03:30:28.944Z</updated>
    
    <content type="html"><![CDATA[<p>转载自：<a href="https://www.jianshu.com/p/5a28ae9b365d" target="_blank" rel="noopener">https://www.jianshu.com/p/5a28ae9b365d</a> 如有侵权，请联系删除</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>FPN是一种利用常规CNN模型来高效提取图片中各维度特征的方法。在计算机视觉学科中，多维度的目标检测一直以来都是通过将缩小或扩大后的不同维度图片作为输入来生成出反映不同维度信息的特征组合。这种办法确实也能有效地表达出图片之上的各种维度特征，但却对硬件计算能力及内存大小有较高要求，因此只能在有限的领域内部使用。<br>FPN通过利用常规CNN模型内部从底至上各个层对同一scale图片不同维度的特征表达结构，提出了一种可有效在单一图片视图下生成对其的多维度特征表达的方法。它可以有效地赋能常规CNN模型，从而可以生成出表达能力更强的feature maps以供下一阶段计算机视觉任务像object detection/semantic segmentation等来使用。本质上说它是一种加强主干网络CNN特征表达的方法。</p><h2 id="Featurized-image-pyramid"><a href="#Featurized-image-pyramid" class="headerlink" title="Featurized image pyramid"></a>Featurized image pyramid</h2><p>下图中描述了四种不同的得到一张图片多维度特征组合的方法。  </p><p><img src="/2019/01/14/FPN——CNN特征提取/1.JPG" alt="图片"><br>上图(a)中的方法即为常规的生成一张图片的多维度特征组合的经典方法。即对某一输入图片我们通过压缩或放大从而形成不同维度的图片作为模型输入，使用同一模型对这些不同维度的图片分别处理后，最终再将这些分别得到的特征（feature maps）组合起来就得到了我们想要的可反映多维度信息的特征集。此种方法缺点在于需要对同一图片在更改维度后输入处理多次，因此对计算机的算力及内存大小都有较高要求。<br>图(b)中的方法则只拿单一维度的图片做为输入，然后经CNN模型处理后，拿最终一层的feature maps作为最终的特征集。显然此种方法只能得到单一维度的信息。优点是计算简单，对计算机算力及内存大小都无过高需求。此方法为大多数R-CNN系列目标检测方法所用像R-CNN/Fast-RCNN/Faster-RCNN等。因此最终这些模型对小维度的目标检测性能不是很好。<br>图(c)中的方法同样是拿单一维度的图片做为输入，不过最终选取用于接下来分类或检测任务时的特征组合时，此方法不只选用了最后一层的high level feature maps，同样也会选用稍靠下的反映图片low level 信息的feature maps。然后将这些不同层次（反映不同level的图片信息）的特征简单合并起来（一般为concat处理），用于最终的特征组合输出。此方法可见于SSD当中。不过SSD在选取层特征时都选用了较高层次的网络。比如在它以VGG16作为主干网络的检测模型里面所选用的最低的Convolution的层为Conv4，这样一些具有更低级别信息的层特征像Conv2/Conv3就被它给漏掉了，于是它对更小维度的目标检测效果就不大好。<br>图(d)中的方法同图(c)中的方法有些类似，也是拿单一维度的图片作为输入，然后它会选取所有层的特征来处理然后再联合起来做为最终的特征输出组合。（作者在论文中拿Resnet为实例时并没选用Conv1层，那是为了算力及内存上的考虑，毕竟Conv1层的size还是比较大的，所包含的特征跟直接的图片像素信息也过于接近）。另外还对这些反映不同级别图片信息的各层自上向下进行了再处理以能更好地组合从而形成较好的特征表达（详细过程会在下面章节中进一步介绍）。而此方法正是我们本文中要讲的FPN CNN特征提取方法。</p><h2 id="FPN基本架构"><a href="#FPN基本架构" class="headerlink" title="FPN基本架构"></a>FPN基本架构</h2><p>FPN会使用CNN网络中每一层的信息来生成最后的表达特征组合。下图是它的基本架构。从中我们能看到FPN会模型每个CNN层的特征输出进行处理以生成反映此维度信息的特征。而自上至下处理后所生成出的特征之间也有个关联关系，即上层high level的特征会影响下一层次的low level特征表达。最终所有的特征一起用来作为下一步的目标检测或类别分析等任务的输入。</p><p><img src="/2019/01/14/FPN——CNN特征提取/2.JPG" alt="图片"></p><h2 id="FPN详细介绍"><a href="#FPN详细介绍" class="headerlink" title="FPN详细介绍"></a>FPN详细介绍</h2><p>FPN是传统CNN网络对图片信息进行表达输出的一种增强。它目的是为了改进CNN网络的特征提取方式，从而可以使最终输出的特征更好地表示出输入图片各个维度的信息。它的基本过程有三个分别为：自下至上的通路即自下至上的不同维度特征生成；自上至下的通路即自上至下的特征补充增强；CNN网络层特征与最终输出的各维度特征之间的关联表达。<br>我们在下图中能看出这三个过程的细粒度表示。</p><p><img src="/2019/01/14/FPN——CNN特征提取/3.JPG" alt="图片"></p><ul><li><p>自下至上的通路（Bottom-top pathway）：这个没啥奇怪就是指的普通CNN特征自底至上逐层浓缩表达特征的一个过程。此过程很早即被认识到了即较底的层反映较浅层次的图片信息特征像边缘等；较高的层则反映较深层次的图片特征像物体轮廓、乃至类别等；</p></li><li><p>自上至下的通路（Top-bottome pathway）：上层的特征输出一般其feature map size比较小，但却能表示更大维度（同时也是更加high level）的图片信息。此类high level信息经实验证明能够对后续的目标检测、物体分类等任务发挥关键作用。因此我们在处理每一层信息时会参考上一层的high level信息做为其输入（这里只是在将上层feature map等比例放大后再与本层的feature maps做element wise相加）;</p></li><li><p>CNN层特征与每一级别输出之间的表达关联：在这里作者实验表明使用1x1的Conv即可生成较好的输出特征，它可有效地降低中间层次的channels 数目。最终这些1x1的Convs使得我们输出不同维度的各个feature maps有着相同的channels数目（本文用到的Resnet-101主干网络中，各个层次特征的最终输出channels数目为256）。</p></li></ul><h2 id="FPN在目标检测中的实际应用"><a href="#FPN在目标检测中的实际应用" class="headerlink" title="FPN在目标检测中的实际应用"></a>FPN在目标检测中的实际应用</h2><p>以下为一个FPN特征提取方法在RCNN目标检测框架中应用的例子。从中我们可以更加详细地了解到它的具体实现。</p><p><img src="/2019/01/14/FPN——CNN特征提取/4.JPG" alt="图片"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载自：&lt;a href=&quot;https://www.jianshu.com/p/5a28ae9b365d&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.jianshu.com/p/5a28ae9b365d&lt;/a&gt; 如有侵权，请联系删除
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="图像识别" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/"/>
    
      <category term="FPN" scheme="http://yoursite.com/tags/FPN/"/>
    
  </entry>
  
  <entry>
    <title>目标检测——RetinaNet</title>
    <link href="http://yoursite.com/2019/01/14/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94RetinaNet/"/>
    <id>http://yoursite.com/2019/01/14/目标检测——RetinaNet/</id>
    <published>2019-01-14T08:23:53.000Z</published>
    <updated>2019-03-28T03:30:28.993Z</updated>
    
    <content type="html"><![CDATA[<p>转载自：<a href="https://www.jianshu.com/p/8e501a159b28" target="_blank" rel="noopener">https://www.jianshu.com/p/8e501a159b28</a> 如有侵权，请联系删除</p><h1 id="RetinaNet-Focal-loss在目标检测网络中的应用"><a href="#RetinaNet-Focal-loss在目标检测网络中的应用" class="headerlink" title="RetinaNet: Focal loss在目标检测网络中的应用"></a>RetinaNet: Focal loss在目标检测网络中的应用</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>RetinaNet是2018年Facebook AI团队在目标检测领域新的贡献。它的重要作者名单中Ross Girshick与Kaiming He赫然在列。来自Microsoft的Sun Jian团队与现在Facebook的Ross/Kaiming团队在当前视觉目标分类、检测领域有着北乔峰、南慕容一般的独特地位。这两个实验室的文章多是行业里前进方向的提示牌。<br>RetinaNet只是原来FPN网络与FCN网络的组合应用，因此在目标网络检测框架上它并无特别亮眼创新。文章中最大的创新来自于Focal loss的提出及在单阶段目标检测网络RetinaNet（实质为Resnet + FPN + FCN）的成功应用。Focal loss是一种改进了的交叉熵(cross-entropy, CE)loss，它通过在原有的CE loss上乘了个使易检测目标对模型训练贡献削弱的指数式，从而使得Focal loss成功地解决了在目标检测时，正负样本区域极不平衡而目标检测loss易被大批量负样本所左右的问题。此问题是单阶段目标检测框架（如SSD/Yolo系列）与双阶段目标检测框架（如Faster-RCNN/R-FCN等）accuracy gap的最大原因。在Focal loss提出之前，已有的目标检测网络都是通过像Boot strapping/Hard example mining等方法来解决此问题的。作者通过后续实验成功表明Focal loss可在单阶段目标检测网络中成功使用，并最终能以更快的速率实现与双阶段目标检测网络近似或更优的效果。</p><h2 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h2><p>常规的单阶段目标检测网络像SSD一般在模型训练时会先大密度地在模型终端的系列feature maps上生成出10,000甚至100,0000个目标候选区域。然后再分别对这些候选区域进行分类与位置回归识别。而在这些生成的数万个候选区域中，绝大多数都是不包含待检测目标的图片背景，这样就造成了机器学习中经典的训练样本正负不平衡的问题。它往往会造成最终算出的training loss为占绝对多数但包含信息量却很少的负样本所支配，少样正样本提供的关键信息却不能在一般所用的training loss中发挥正常作用，从而无法得出一个能对模型训练提供正确指导的loss。<br>常用的解决此问题的方法就是负样本挖掘。或其它更复杂的用于过滤负样本从而使正负样本数维持一定比率的样本取样方法。而在此篇文章中作者提出了可通过候选区域包含潜在目标概率进而对最终的training loss进行较正的方法。实验表明这种新提出的focal loss在单阶段目标检测任务上表现突出，有效地解决了此领域里面潜在的类别不平衡问题。</p><h2 id="Focal-loss"><a href="#Focal-loss" class="headerlink" title="Focal loss"></a>Focal loss</h2><ul><li><p>CE(cross-entropy) loss<br>  以下为典型的交叉熵loss，它广泛用于当下的图像分类、检测CNN网络当中。</p><p>  <img src="/2019/01/14/目标检测——RetinaNet/1.JPG" alt="图片"></p></li><li><p>Balanced CE loss<br>  考虑到上节中提到的类别不平衡问题对最终training loss的不利影响，我们自然会想到可通过在loss公式中使用与目标存在概率成反比的系数对其进行较正。如下公式即是此朴素想法的体现。它也是作者最终Focus loss的baseline。</p><p>  <img src="/2019/01/14/目标检测——RetinaNet/2.JPG" alt="图片"></p></li><li><p>Focal loss定义<br>  以下是作者提出的focal loss的想法。</p><p>  <img src="/2019/01/14/目标检测——RetinaNet/3.JPG" alt="图片"><br>  下图为focal loss与常规CE loss的对比。从中，我们易看出focal loss所加的指数式系数可对正负样本对loss的贡献自动调节。当某样本类别比较明确些，它对整体loss的贡献就比较少；而若某样本类别不易区分，则对整体loss的贡献就相对偏大。这样得到的loss最终将集中精力去诱导模型去努力分辨那些难分的目标类别，于是就有效提升了整体的目标检测准度。不过在此focus loss计算当中，我们引入了一个新的hyper parameter即γ。一般来说新参数的引入，往往会伴随着模型使用难度的增加。在本文中，作者有试者对其进行调节，线性搜索后得出将γ设为2时，模型检测效果最好。</p><p>  <img src="/2019/01/14/目标检测——RetinaNet/4.JPG" alt="图片"></p><p>  在最终所用的focal loss上，作者还引入了α系数，它能够使得focal loss对不同类别更加平衡。实验表明它会比原始的focal loss效果更好。</p><p>  <img src="/2019/01/14/目标检测——RetinaNet/5.JPG" alt="图片"></p></li></ul><h2 id="模型的初始化参数选择"><a href="#模型的初始化参数选择" class="headerlink" title="模型的初始化参数选择"></a>模型的初始化参数选择</h2><p>一般我们初始化CNN网络模型时都会使用无偏的参数对其初始化，比如Conv的kernel 参数我们会以bias 为0，variance为0.01的某分布来对其初始化。但是如果我们的模型要去处理类别极度不平衡的情况，那么就会考虑到这样对训练数据分布无任选先验假设的初始化会使得在训练过程中，我们的参数更偏向于拥有更多数量的负样本的情况去进化。作者观察下来发现它在训练时会出现极度的不稳定。于是作者在初始化模型最后一层参数时考虑了数据样本分布的不平衡性，这样使得初始训练时最终得出的loss不会对过多的负样本数量所惊讶到，从而有效地规避了初始训练时模型的震荡与不稳定</p><h2 id="RetinaNet检测框架"><a href="#RetinaNet检测框架" class="headerlink" title="RetinaNet检测框架"></a>RetinaNet检测框架</h2><p>RetinaNet本质上是Resnet + FPN + 两个FCN子网络。<br>以下为RetinaNet目标框架框架图。有了之前blog里面提到的FPN与FCN的知识后，我们很容易理解此框架的设计含义。</p><p><img src="/2019/01/14/目标检测——RetinaNet/6.JPG" alt="图片"></p><p>一般主干网络可选用任一有效的特征提取网络如vgg16或resnet系列，此处作者分别尝试了resnet-50与resnet-101。而FPN则是对resnet-50里面自动形成的多尺度特征进行了强化利用，从而得到了表达力更强、包含多尺度目标区域信息的feature maps集合。最后在FPN所吐出的feature maps集合上，分别使用了两个FCN子网络（它们有着相同的网络结构却各自独立，并不share参数）用来完成目标框类别分类与位置回归任务。</p><h2 id="模型的推理与训练"><a href="#模型的推理与训练" class="headerlink" title="模型的推理与训练"></a>模型的推理与训练</h2><ul><li><p>模型推理 </p><p>  一旦我们有了训练好的模型，在正式部署时，只需对其作一次forward，然后对最终生成的目标区域进行过渡。然后只对每个FPN level上目标存在概率最高的前1000个目标框进一步地decoding处理。接下来再将所有FPN level上得到的目标框汇集起来，统一使用极大值抑制的方法进一步过渡（其中极大值抑制时所用的阈值为0.5）。这样，我们就得到了最终的目标与其位置框架。</p></li><li><p>模型训练</p><p>  模型训练中主要在后端Loss计算时采用了Focal loss，另外也在模型初始化时考虑到了正负样本极度不平衡的情况进而对模型最后一个conv layer的bias参数作了有偏初始化。 </p><p>  训练时用了SGD，mini batch size为16，在8个GPU上一块训练，每个GPU上local batch size为2。最大iterations数目为90,000；模型初始lr为0.01,接下来随着训练进行分step wisely 降低。真正的training loss则为表达目标类别的focus loss与表达目标框位置回归信息的L1 loss的和。  </p><p>  下图为RetinaNet模型的检测准度与性能<br>  <img src="/2019/01/14/目标检测——RetinaNet/7.JPG" alt="图片"></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载自：&lt;a href=&quot;https://www.jianshu.com/p/8e501a159b28&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.jianshu.com/p/8e501a159b28&lt;/a&gt; 如有侵权，请联系删除
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="RetinaNet" scheme="http://yoursite.com/tags/RetinaNet/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>项目整理</title>
    <link href="http://yoursite.com/2019/01/14/%E9%A1%B9%E7%9B%AE%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2019/01/14/项目整理/</id>
    <published>2019-01-14T01:25:39.000Z</published>
    <updated>2019-03-28T03:30:29.015Z</updated>
    
    <content type="html"><![CDATA[<h1 id="渝黔高速——隧道"><a href="#渝黔高速——隧道" class="headerlink" title="渝黔高速——隧道"></a>渝黔高速——隧道</h1><p>开发工具：JavaScript + Git + Docker + Postgresql<br>IDE：Vscode</p><p>表结构设计：</p><ul><li>隧道表</li><li>衬砌表</li><li>工法分区表</li><li>衬砌工程量表</li><li>分区工程量表</li><li>模型表</li><li>进度表</li><li>施工劳务记录表</li><li>劳务合同清单表</li></ul><h2 id="业务分析"><a href="#业务分析" class="headerlink" title="业务分析"></a>业务分析</h2><h3 id="模型的创建"><a href="#模型的创建" class="headerlink" title="模型的创建"></a>模型的创建</h3><ul><li>隧道的创建以及获取信息</li><li>衬砌的创建以及信息获取</li><li>工法分区的创建以及信息获取</li><li>衬砌工程量的录入</li><li>隧道段的创建、查询、修改</li><li>在段下面创建模型（根据衬砌类型和分区批量创建）</li><li>分区工程量参数设置（根据已经创建的模型，对每一个分区进行分区工程量参数设置）、修改</li><li>分区工程量参数</li></ul><h3 id="进度"><a href="#进度" class="headerlink" title="进度"></a>进度</h3><ul><li>根据模型获取相关的工程量，起止桩号</li><li>创建进尺</li><li>修改完成状态</li><li>查看已完成进尺</li></ul><h3 id="工程量"><a href="#工程量" class="headerlink" title="工程量"></a>工程量</h3><ul><li>统计当前施工完成的工程量</li><li>根据时间筛选当前完成的工程量</li></ul><h3 id="产值"><a href="#产值" class="headerlink" title="产值"></a>产值</h3><ul><li>根据工程量计算得到当前施工的总产值以及单个隧道产值</li></ul><h3 id="分包"><a href="#分包" class="headerlink" title="分包"></a>分包</h3><ul><li>合同工程量清单的创建、获取</li><li>劳务成本的计算</li></ul><h3 id="业务逻辑"><a href="#业务逻辑" class="headerlink" title="业务逻辑"></a>业务逻辑</h3><p><img src="/2019/01/14/项目整理/1.png" alt="图片"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;渝黔高速——隧道&quot;&gt;&lt;a href=&quot;#渝黔高速——隧道&quot; class=&quot;headerlink&quot; title=&quot;渝黔高速——隧道&quot;&gt;&lt;/a&gt;渝黔高速——隧道&lt;/h1&gt;&lt;p&gt;开发工具：JavaScript + Git + Docker + Postgresql&lt;br
      
    
    </summary>
    
      <category term="笔记" scheme="http://yoursite.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="项目" scheme="http://yoursite.com/tags/%E9%A1%B9%E7%9B%AE/"/>
    
      <category term="隧道" scheme="http://yoursite.com/tags/%E9%9A%A7%E9%81%93/"/>
    
  </entry>
  
  <entry>
    <title>基于Pytorch的特征图提取</title>
    <link href="http://yoursite.com/2018/11/13/%E5%9F%BA%E4%BA%8EPytorch%E7%9A%84%E7%89%B9%E5%BE%81%E5%9B%BE%E6%8F%90%E5%8F%96/"/>
    <id>http://yoursite.com/2018/11/13/基于Pytorch的特征图提取/</id>
    <published>2018-11-13T02:49:13.000Z</published>
    <updated>2018-11-13T03:19:20.839Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h3><p>为了方便理解卷积神经网络的运行过程，需要对卷积神经网络的运行结果进行可视化的展示。  </p><p>大致可分为如下步骤：</p><ul><li>单个图片的提取</li><li>神经网络的构建</li><li>特征图的提取</li><li>可视化展示</li></ul><h4 id="单个图片的提取"><a href="#单个图片的提取" class="headerlink" title="单个图片的提取"></a>单个图片的提取</h4><p>根据目标要求，需要对单个图片进行卷积运算，但是Pytorch中读取数据主要用到torch.utils.data.DataLoader类，因此我们需要编写单个图片的读取程序</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_picture</span><span class="params">(picture_dir, transform)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    该算法实现了读取图片，并将其类型转化为Tensor</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    tmp = []</span><br><span class="line">    img = skimage.io.imread(picture_dir)</span><br><span class="line">    tmp.append(img)</span><br><span class="line">    img = skimage.io.imread(<span class="string">'./picture/4.jpg'</span>)</span><br><span class="line">    tmp.append(img)</span><br><span class="line">    img256 = [skimage.transform.resize(img, (<span class="number">256</span>, <span class="number">256</span>)) <span class="keyword">for</span> img <span class="keyword">in</span> tmp]</span><br><span class="line">    img256 = np.asarray(img256)</span><br><span class="line">    img256 = img256.astype(np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transform(img256[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p><strong>注意：</strong> 神经网络的输入是四维形式，我们返回的图片是三维形式，需要使用unsqueeze()插入一个维度</p><h4 id="神经网络的构建"><a href="#神经网络的构建" class="headerlink" title="神经网络的构建"></a>神经网络的构建</h4><p>网络的基于LeNet构建，不过为了方便展示，将其中的参数按照256<em>256</em>3进行的参数的修正  </p><p>网络构建如下：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    该类继承了torch.nn.Modul类</span></span><br><span class="line"><span class="string">    构建LeNet神经网络模型</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LeNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第一层神经网络，包括卷积层、线性激活函数、池化层</span></span><br><span class="line">        self.conv1 = nn.Sequential( </span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),   <span class="comment"># input_size=(3*256*256)，padding=2</span></span><br><span class="line">            nn.ReLU(),                  <span class="comment"># input_size=(32*256*256)</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),  <span class="comment"># output_size=(32*128*128)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二层神经网络，包括卷积层、线性激活函数、池化层</span></span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),  <span class="comment"># input_size=(32*128*128)</span></span><br><span class="line">            nn.ReLU(),            <span class="comment"># input_size=(64*128*128)</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)    <span class="comment"># output_size=(64*64*64)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 全连接层(将神经网络的神经元的多维输出转化为一维)</span></span><br><span class="line">        self.fc1 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">64</span> * <span class="number">64</span> * <span class="number">64</span>, <span class="number">128</span>),  <span class="comment"># 进行线性变换</span></span><br><span class="line">            nn.ReLU()                    <span class="comment"># 进行ReLu激活</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出层(将全连接层的一维输出进行处理)</span></span><br><span class="line">        self.fc2 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">84</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将输出层的数据进行分类(输出预测值)</span></span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">62</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向传播过程，输入为x</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="comment"># nn.Linear()的输入输出都是维度为一的值，所以要把多维度的tensor展平成一维</span></span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p><h4 id="特征图的提取"><a href="#特征图的提取" class="headerlink" title="特征图的提取"></a>特征图的提取</h4><p>直接上代码：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureExtractor</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, submodule, extracted_layers)</span>:</span></span><br><span class="line">        super(FeatureExtractor, self).__init__()</span><br><span class="line">        self.submodule = submodule</span><br><span class="line">        self.extracted_layers = extracted_layers</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        outputs = []</span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.submodule._modules.items():</span><br><span class="line">        <span class="comment"># 目前不展示全连接层</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">"fc"</span> <span class="keyword">in</span> name: </span><br><span class="line">                x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">            print(module)</span><br><span class="line">            x = module(x)</span><br><span class="line">            print(name)</span><br><span class="line">            <span class="keyword">if</span> name <span class="keyword">in</span> self.extracted_layers:</span><br><span class="line">                outputs.append(x)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure></p><h4 id="可视化展示"><a href="#可视化展示" class="headerlink" title="可视化展示"></a>可视化展示</h4><p>可视化展示使用matplotlib</p><p>代码如下：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征输出可视化</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">    ax = plt.subplot(<span class="number">6</span>, <span class="number">6</span>, i + <span class="number">1</span>)</span><br><span class="line">    ax.set_title(<span class="string">'Feature &#123;&#125;'</span>.format(i))</span><br><span class="line">    ax.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.imshow(x[<span class="number">0</span>].data.numpy()[<span class="number">0</span>,i,:,:],cmap=<span class="string">'jet'</span>)</span><br><span class="line">plt.plot()</span><br></pre></td></tr></table></figure></p><h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><p>在此贴上完整代码</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="keyword">as</span> tv</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> skimage.data</span><br><span class="line"><span class="keyword">import</span> skimage.io</span><br><span class="line"><span class="keyword">import</span> skimage.transform</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义是否使用GPU</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load training and testing datasets.</span></span><br><span class="line">pic_dir = <span class="string">'./picture/3.jpg'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据预处理方式(将输入的类似numpy中arrary形式的数据转化为pytorch中的张量（tensor）)</span></span><br><span class="line">transform = transforms.ToTensor()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_picture</span><span class="params">(picture_dir, transform)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    该算法实现了读取图片，并将其类型转化为Tensor</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    img = skimage.io.imread(picture_dir)</span><br><span class="line">    img256 = skimage.transform.resize(img, (<span class="number">256</span>, <span class="number">256</span>))</span><br><span class="line">    img256 = np.asarray(img256)</span><br><span class="line">    img256 = img256.astype(np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transform(img256)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_picture_rgb</span><span class="params">(picture_dir)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    该函数实现了显示图片的RGB三通道颜色</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    img = skimage.io.imread(picture_dir)</span><br><span class="line">    img256 = skimage.transform.resize(img, (<span class="number">256</span>, <span class="number">256</span>))</span><br><span class="line">    skimage.io.imsave(<span class="string">'./picture/4.jpg'</span>,img256)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 取单一通道值显示</span></span><br><span class="line">    <span class="comment"># for i in range(3):</span></span><br><span class="line">    <span class="comment">#     img = img256[:,:,i]</span></span><br><span class="line">    <span class="comment">#     ax = plt.subplot(1, 3, i + 1)</span></span><br><span class="line">    <span class="comment">#     ax.set_title('Feature &#123;&#125;'.format(i))</span></span><br><span class="line">    <span class="comment">#     ax.axis('off')</span></span><br><span class="line">    <span class="comment">#     plt.imshow(img)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># r = img256.copy()</span></span><br><span class="line">    <span class="comment"># r[:,:,0:2]=0</span></span><br><span class="line">    <span class="comment"># ax = plt.subplot(1, 4, 1)</span></span><br><span class="line">    <span class="comment"># ax.set_title('B Channel')</span></span><br><span class="line">    <span class="comment"># # ax.axis('off')</span></span><br><span class="line">    <span class="comment"># plt.imshow(r)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># g = img256.copy()</span></span><br><span class="line">    <span class="comment"># g[:,:,0]=0</span></span><br><span class="line">    <span class="comment"># g[:,:,2]=0</span></span><br><span class="line">    <span class="comment"># ax = plt.subplot(1, 4, 2)</span></span><br><span class="line">    <span class="comment"># ax.set_title('G Channel')</span></span><br><span class="line">    <span class="comment"># # ax.axis('off')</span></span><br><span class="line">    <span class="comment"># plt.imshow(g)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># b = img256.copy()</span></span><br><span class="line">    <span class="comment"># b[:,:,1:3]=0</span></span><br><span class="line">    <span class="comment"># ax = plt.subplot(1, 4, 3)</span></span><br><span class="line">    <span class="comment"># ax.set_title('R Channel')</span></span><br><span class="line">    <span class="comment"># # ax.axis('off')</span></span><br><span class="line">    <span class="comment"># plt.imshow(b)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># img = img256.copy()</span></span><br><span class="line">    <span class="comment"># ax = plt.subplot(1, 4, 4)</span></span><br><span class="line">    <span class="comment"># ax.set_title('image')</span></span><br><span class="line">    <span class="comment"># # ax.axis('off')</span></span><br><span class="line">    <span class="comment"># plt.imshow(img)</span></span><br><span class="line"></span><br><span class="line">    img = img256.copy()</span><br><span class="line">    ax = plt.subplot()</span><br><span class="line">    ax.set_title(<span class="string">'image'</span>)</span><br><span class="line">    <span class="comment"># ax.axis('off')</span></span><br><span class="line">    plt.imshow(img)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    该类继承了torch.nn.Modul类</span></span><br><span class="line"><span class="string">    构建LeNet神经网络模型</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LeNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第一层神经网络，包括卷积层、线性激活函数、池化层</span></span><br><span class="line">        self.conv1 = nn.Sequential( </span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),   <span class="comment"># input_size=(3*256*256)，padding=2</span></span><br><span class="line">            nn.ReLU(),                  <span class="comment"># input_size=(32*256*256)</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),  <span class="comment"># output_size=(32*128*128)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二层神经网络，包括卷积层、线性激活函数、池化层</span></span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),  <span class="comment"># input_size=(32*128*128)</span></span><br><span class="line">            nn.ReLU(),            <span class="comment"># input_size=(64*128*128)</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)    <span class="comment"># output_size=(64*64*64)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 全连接层(将神经网络的神经元的多维输出转化为一维)</span></span><br><span class="line">        self.fc1 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">64</span> * <span class="number">64</span> * <span class="number">64</span>, <span class="number">128</span>),  <span class="comment"># 进行线性变换</span></span><br><span class="line">            nn.ReLU()                    <span class="comment"># 进行ReLu激活</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出层(将全连接层的一维输出进行处理)</span></span><br><span class="line">        self.fc2 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">84</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将输出层的数据进行分类(输出预测值)</span></span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">62</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向传播过程，输入为x</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="comment"># nn.Linear()的输入输出都是维度为一的值，所以要把多维度的tensor展平成一维</span></span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中间特征提取</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureExtractor</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, submodule, extracted_layers)</span>:</span></span><br><span class="line">        super(FeatureExtractor, self).__init__()</span><br><span class="line">        self.submodule = submodule</span><br><span class="line">        self.extracted_layers = extracted_layers</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        outputs = []</span><br><span class="line">        print(self.submodule._modules.items())</span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.submodule._modules.items():</span><br><span class="line">            <span class="keyword">if</span> <span class="string">"fc"</span> <span class="keyword">in</span> name: </span><br><span class="line">                print(name)</span><br><span class="line">                x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">            print(module)</span><br><span class="line">            x = module(x)</span><br><span class="line">            print(name)</span><br><span class="line">            <span class="keyword">if</span> name <span class="keyword">in</span> self.extracted_layers:</span><br><span class="line">                outputs.append(x)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_feature</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 输入数据</span></span><br><span class="line">    img = get_picture(pic_dir, transform)</span><br><span class="line">    <span class="comment"># 插入维度</span></span><br><span class="line">    img = img.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    img = img.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 特征输出</span></span><br><span class="line">    net = LeNet().to(device)</span><br><span class="line">    <span class="comment"># net.load_state_dict(torch.load('./model/net_050.pth'))</span></span><br><span class="line">    exact_list = [<span class="string">"conv1"</span>，<span class="string">"conv2"</span>]</span><br><span class="line">    myexactor = FeatureExtractor(net, exact_list)</span><br><span class="line">    x = myexactor(img)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 特征输出可视化</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">        ax = plt.subplot(<span class="number">6</span>, <span class="number">6</span>, i + <span class="number">1</span>)</span><br><span class="line">        ax.set_title(<span class="string">'Feature &#123;&#125;'</span>.format(i))</span><br><span class="line">        ax.axis(<span class="string">'off'</span>)</span><br><span class="line">        plt.imshow(x[<span class="number">0</span>].data.numpy()[<span class="number">0</span>,i,:,:],cmap=<span class="string">'jet'</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    get_picture_rgb(pic_dir)</span><br><span class="line">    <span class="comment"># get_feature()</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;简述&quot;&gt;&lt;a href=&quot;#简述&quot; class=&quot;headerlink&quot; title=&quot;简述&quot;&gt;&lt;/a&gt;简述&lt;/h3&gt;&lt;p&gt;为了方便理解卷积神经网络的运行过程，需要对卷积神经网络的运行结果进行可视化的展示。  &lt;/p&gt;
&lt;p&gt;大致可分为如下步骤：&lt;/p&gt;
&lt;ul&gt;
      
    
    </summary>
    
      <category term="Pytorch框架" scheme="http://yoursite.com/categories/Pytorch%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="feature map" scheme="http://yoursite.com/tags/feature-map/"/>
    
  </entry>
  
  <entry>
    <title>基于pytorch的交通标志识别</title>
    <link href="http://yoursite.com/2018/11/11/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E4%BA%A4%E9%80%9A%E6%A0%87%E5%BF%97%E8%AF%86%E5%88%AB/"/>
    <id>http://yoursite.com/2018/11/11/基于pytorch的交通标志识别/</id>
    <published>2018-11-11T03:57:49.000Z</published>
    <updated>2018-11-11T07:07:34.337Z</updated>
    
    <content type="html"><![CDATA[<p>本文是在<a href="https://www.jianshu.com/p/d8feaddc7bdf文章的基础上用Pytorch实现的" target="_blank" rel="noopener">https://www.jianshu.com/p/d8feaddc7bdf文章的基础上用Pytorch实现的</a></p><p>话不多说，直接上代码，具体的可以看代码中的解释</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="keyword">as</span> tv</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> skimage.data</span><br><span class="line"><span class="keyword">import</span> skimage.transform</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义是否使用GPU</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">使得我们能够手动输入命令行参数，就是让风格变得和Linux命令行差不多</span></span><br><span class="line"><span class="string">argparse是python的一个包，用来解析输入的参数</span></span><br><span class="line"><span class="string">如：</span></span><br><span class="line"><span class="string">    python mnist.py --outf model  </span></span><br><span class="line"><span class="string">    （意思是将训练的模型保存到model文件夹下，当然，你也可以不加参数，那样的话代码最后一行</span></span><br><span class="line"><span class="string">      torch.save()就需要注释掉了）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    python mnist.py --net model/net_005.pth</span></span><br><span class="line"><span class="string">    （意思是加载之前训练好的网络模型，前提是训练使用的网络和测试使用的网络是同一个网络模型，保证权重参数矩阵相等）</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">'--outf'</span>, default=<span class="string">'./model/'</span>, help=<span class="string">'folder to output images and model checkpoints'</span>)  <span class="comment"># 模型保存路径</span></span><br><span class="line">parser.add_argument(<span class="string">'--net'</span>, default=<span class="string">'./model/net.pth'</span>, help=<span class="string">"path to netG (to continue training)"</span>)  <span class="comment"># 模型加载路径</span></span><br><span class="line">opt = parser.parse_args()  <span class="comment"># 解析得到你在路径中输入的参数，比如 --outf 后的"model"或者 --net 后的"model/net_005.pth"，是作为字符串形式保存的</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load training and testing datasets.</span></span><br><span class="line">ROOT_PATH = <span class="string">"./traffic"</span></span><br><span class="line">train_data_dir = os.path.join(ROOT_PATH, <span class="string">"datasets/BelgiumTS/Training"</span>)</span><br><span class="line">test_data_dir = os.path.join(ROOT_PATH, <span class="string">"datasets/BelgiumTS/Testing"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">定义LeNet神经网络，进一步的理解可查看Pytorch入门，里面很详细，代码本质上是一样的，这里做了一些封装</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    该类继承了torch.nn.Modul类</span></span><br><span class="line"><span class="string">    构建LeNet神经网络模型</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LeNet, self).__init__()  <span class="comment"># 这一个是python中的调用父类LeNet的方法，因为LeNet继承了nn.Module，如果不加这一句，无法使用导入的torch.nn中的方法，这涉及到python的类继承问题，你暂时不用深究</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第一层神经网络，包括卷积层、线性激活函数、池化层</span></span><br><span class="line">        self.conv1 = nn.Sequential(     <span class="comment"># input_size=(1*28*28)：输入层图片的输入尺寸，我看了那个文档，发现不需要天，会自动适配维度</span></span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),   <span class="comment"># padding=2保证输入输出尺寸相同：采用的是两个像素点进行填充，用尺寸为5的卷积核，保证了输入和输出尺寸的相同</span></span><br><span class="line">            nn.ReLU(),                  <span class="comment"># input_size=(6*28*28)：同上，其中的6是卷积后得到的通道个数，或者叫特征个数，进行ReLu激活</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), <span class="comment"># output_size=(6*14*14)：经过池化层后的输出</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二层神经网络，包括卷积层、线性激活函数、池化层</span></span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>),  <span class="comment"># input_size=(6*14*14)：  经过上一层池化层后的输出,作为第二层卷积层的输入，不采用填充方式进行卷积</span></span><br><span class="line">            nn.ReLU(),            <span class="comment"># input_size=(16*10*10)： 对卷积神经网络的输出进行ReLu激活</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)    <span class="comment"># output_size=(16*5*5)：  池化层后的输出结果</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 全连接层(将神经网络的神经元的多维输出转化为一维)</span></span><br><span class="line">        self.fc1 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">64</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">128</span>),  <span class="comment"># 进行线性变换</span></span><br><span class="line">            nn.ReLU()                    <span class="comment"># 进行ReLu激活</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出层(将全连接层的一维输出进行处理)</span></span><br><span class="line">        self.fc2 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">84</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将输出层的数据进行分类(输出预测值)</span></span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">62</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向传播过程，输入为x</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="comment"># nn.Linear()的输入输出都是维度为一的值，所以要把多维度的tensor展平成一维</span></span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数设置</span></span><br><span class="line">EPOCH = <span class="number">20</span>   <span class="comment"># 遍历数据集次数(训练模型的轮数)</span></span><br><span class="line">BATCH_SIZE = <span class="number">3</span>     <span class="comment"># 批处理尺寸(batch_size)：关于为何进行批处理，文档中有不错的介绍</span></span><br><span class="line">LR = <span class="number">0.001</span>        <span class="comment"># 学习率：模型训练过程中每次优化的幅度</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据预处理方式(将输入的类似numpy中arrary形式的数据转化为pytorch中的张量（tensor）)</span></span><br><span class="line">transform = transforms.ToTensor()</span><br><span class="line"><span class="comment"># transform = torch.FloatTensor</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练数据集(此处是加载MNIST手写数据集)</span></span><br><span class="line">trainset = tv.datasets.Traffic(</span><br><span class="line">    root=train_data_dir, <span class="comment"># 如果从本地加载数据集，对应的加载路径</span></span><br><span class="line">    train=<span class="keyword">True</span>,     <span class="comment"># 训练模型</span></span><br><span class="line">    download=<span class="keyword">True</span>,  <span class="comment"># 是否从网络下载训练数据集</span></span><br><span class="line">    transform=transform  <span class="comment"># 数据的转换形式</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练批处理数据</span></span><br><span class="line">trainloader = torch.utils.data.DataLoader(</span><br><span class="line">    trainset,                <span class="comment"># 加载测试集</span></span><br><span class="line">    batch_size=BATCH_SIZE,   <span class="comment"># 最小批处理尺寸</span></span><br><span class="line">    shuffle=<span class="keyword">True</span>,            <span class="comment"># 标识进行数据迭代时候将数据打乱</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义测试数据集</span></span><br><span class="line">testset = tv.datasets.Traffic(</span><br><span class="line">    root=test_data_dir, <span class="comment"># 如果从本地加载数据集，对应的加载路径</span></span><br><span class="line">    train=<span class="keyword">True</span>,     <span class="comment"># 训练模型</span></span><br><span class="line">    download=<span class="keyword">True</span>,  <span class="comment"># 是否从网络下载训练数据集</span></span><br><span class="line">    transform=transform  <span class="comment"># 数据的转换形式</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义测试批处理数据</span></span><br><span class="line">testloader = torch.utils.data.DataLoader(</span><br><span class="line">    testset,                 <span class="comment"># 加载测试集</span></span><br><span class="line">    batch_size=BATCH_SIZE,   <span class="comment"># 最小批处理尺寸</span></span><br><span class="line">    shuffle=<span class="keyword">False</span>,           <span class="comment"># 标识进行数据迭代时候不将数据打乱</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_train</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 定义损失函数loss function 和优化方式（采用SGD）</span></span><br><span class="line">    net = LeNet().to(device)</span><br><span class="line">    criterion = nn.CrossEntropyLoss()  <span class="comment"># 交叉熵损失函数，通常用于多分类问题上</span></span><br><span class="line">    optimizer = optim.SGD(net.parameters(), lr=LR, momentum=<span class="number">0.9</span>)  <span class="comment"># 优化函数</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">        sum_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 数据读取（采用python的枚举方法获得标签和数据，这一部分可能和numpy相关）</span></span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader):</span><br><span class="line">            inputs, labels = data</span><br><span class="line">            <span class="comment"># labels = [torch.LongTensor(label) for label in labels]</span></span><br><span class="line">            <span class="comment"># 将输入数据和标签放入构建的图中 注：图的概念可在pytorch入门中查</span></span><br><span class="line">            inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 梯度清零</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># forward + backward  注: 这一部分是训练神经网络的核心</span></span><br><span class="line">            outputs = net(inputs)</span><br><span class="line">            loss = criterion(outputs, labels)</span><br><span class="line">            loss.backward() <span class="comment"># 反向自动求导</span></span><br><span class="line">            optimizer.step() <span class="comment"># 进行优化</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 每训练100个batch打印一次平均loss</span></span><br><span class="line">            sum_loss += loss.item()</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">48</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">'[%d, %d] loss: %.03f'</span></span><br><span class="line">                      % (epoch + <span class="number">1</span>, i + <span class="number">1</span>, sum_loss / <span class="number">100</span>))</span><br><span class="line">                sum_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 每跑完一次epoch测试一下准确率</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            correct = <span class="number">0</span></span><br><span class="line">            total = <span class="number">0</span></span><br><span class="line">            <span class="comment"># for i, data in enumerate(testloader):</span></span><br><span class="line">            <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">                images, labels = data</span><br><span class="line">                images, labels = images.to(device), labels.to(device)</span><br><span class="line">                outputs = net(images)</span><br><span class="line">                <span class="comment"># 取得分最高的那个类</span></span><br><span class="line">                _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">                total += labels.size(<span class="number">0</span>)</span><br><span class="line">                correct += (predicted == labels).sum()</span><br><span class="line">            print(<span class="string">'第%d个epoch的识别准确率为：%d%%'</span> % (epoch + <span class="number">1</span>, (<span class="number">100</span> * correct / total)))</span><br><span class="line">    torch.save(net.state_dict(), <span class="string">'%s/net_%03d.pth'</span> % (opt.outf, epoch + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    model_train()</span><br></pre></td></tr></table></figure><h3 id="主要问题——数据读取"><a href="#主要问题——数据读取" class="headerlink" title="主要问题——数据读取"></a>主要问题——数据读取</h3><p>PyTorch中数据读取的一个重要接口是torch.utils.data.DataLoader，该接口定义在dataloader.py脚本中，只要是用PyTorch来训练模型基本都会用到该接口，为了满足pytorch的数据读取要求，写了一个tv.datasets.Traffic的读取文件，是基于mnist数据集的读取进行编写的：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> data</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"><span class="keyword">import</span> errno</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> skimage.data</span><br><span class="line"><span class="keyword">import</span> skimage.transform</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(data_dir, train=True)</span>:</span></span><br><span class="line">    <span class="string">"""Loads a data set and returns two lists:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    images: a list of Numpy arrays, each representing an image.</span></span><br><span class="line"><span class="string">    labels: a list of numbers that represent the images labels.</span></span><br><span class="line"><span class="string">    仅仅只是加载图片到数组，并没有对图片进行缩放比例</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        <span class="comment"># Get all subdirectories of data_dir. Each represents a label.</span></span><br><span class="line">        directories = [d <span class="keyword">for</span> d <span class="keyword">in</span> os.listdir(data_dir)</span><br><span class="line">                    <span class="keyword">if</span> os.path.isdir(os.path.join(data_dir, d))]</span><br><span class="line">        <span class="comment"># Loop through the label directories and collect the data in</span></span><br><span class="line">        <span class="comment"># two lists, labels and images.</span></span><br><span class="line">        labels = []</span><br><span class="line">        images = []</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> directories:</span><br><span class="line">            label_dir = os.path.join(data_dir, d)</span><br><span class="line">            file_names = [os.path.join(label_dir, f)</span><br><span class="line">                        <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(label_dir) <span class="keyword">if</span> f.endswith(<span class="string">".ppm"</span>)]</span><br><span class="line">            <span class="comment"># For each label, load it's images and add them to the images list.</span></span><br><span class="line">            <span class="comment"># And add the label number (i.e. directory name) to the labels list.</span></span><br><span class="line">            <span class="keyword">for</span> f <span class="keyword">in</span> file_names:</span><br><span class="line">                images.append(skimage.data.imread(f))</span><br><span class="line">                labels.append(int(d))  <span class="comment"># 为每一个图片加上标签</span></span><br><span class="line">    images28 = [skimage.transform.resize(image, (<span class="number">28</span>, <span class="number">28</span>)) <span class="keyword">for</span> image <span class="keyword">in</span> images]</span><br><span class="line">    labels_a = np.asarray(labels)</span><br><span class="line">    images_a = np.asarray(images28)</span><br><span class="line">    <span class="keyword">return</span> images_a, labels_a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Traffic</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Traffic Dataset.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, train=True, transform=None, target_transform=None, download=False)</span>:</span></span><br><span class="line">        self.root = os.path.expanduser(root)</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line">        self.train = train  <span class="comment"># training set or test set</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            self.train_data, self.train_labels = load_data(root,train)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.train_data, self.train_labels = load_data(root,train)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            index (int): Index</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            tuple: (image, target) where target is index of the target class.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            img, target = self.train_data[index], self.train_labels[index]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img, target = self.test_data[index], self.test_labels[index]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># doing this so that it is consistent with all other datasets</span></span><br><span class="line">        img = img.astype(np.float32)</span><br><span class="line">        target = torch.LongTensor([target])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.target_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            target = self.target_transform(target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, target</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            <span class="keyword">return</span> len(self.train_data)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> len(self.test_data)</span><br></pre></td></tr></table></figure><p><strong>注意:</strong> 在返回标签的时候，由于数据格式的问题，需要将标签放入一个list中，之后再转换为LongTensor，并取其第一个数据。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文是在&lt;a href=&quot;https://www.jianshu.com/p/d8feaddc7bdf文章的基础上用Pytorch实现的&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.jianshu.com/p/d8feaddc7b
      
    
    </summary>
    
      <category term="Pytorch框架" scheme="http://yoursite.com/categories/Pytorch%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="交通标志识别" scheme="http://yoursite.com/tags/%E4%BA%A4%E9%80%9A%E6%A0%87%E5%BF%97%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>机器学习的基本概念</title>
    <link href="http://yoursite.com/2018/10/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <id>http://yoursite.com/2018/10/29/机器学习的基本概念/</id>
    <published>2018-10-29T07:17:56.000Z</published>
    <updated>2018-10-30T12:58:21.733Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习和深度学习的关系在此不做赘述，本文主要说明机器学习的基本知识</p><h2 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h2><p>机器学习算法是一种能够从数据中学习的算法，Mitchell（1997）提供了一个简洁的定义：“对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。”</p><h3 id="任务T"><a href="#任务T" class="headerlink" title="任务T"></a><strong>任务T</strong></h3><p>通常机器学习任务定义为机器学习系统应该如何处理样本（example）。样本是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的特征（feature）的集合。  </p><p>一些非常常见的机器学习的任务列举如下：</p><ul><li>分类</li><li>输入缺失分类</li><li>回归</li><li>转录</li><li>机器翻译</li><li>结构化输出</li><li>异常检测</li><li>合成和采样</li><li>确实值填补</li><li>去噪</li><li>密度估计或概率质量函数估计  </li></ul><p>当然，还有很多其他同类型或其他类型的任务，这里我们列举的任务类型只是用来介绍机器学习可以做哪些任务。</p><h3 id="性能度量P"><a href="#性能度量P" class="headerlink" title="性能度量P"></a><strong>性能度量P</strong></h3><p>为了评估机器学习算法的能力，引入了对其性能的定量度量。通常性能度量P是特定与系统执行的任务T而言的。</p><p>这就需要我们针对不同的的任务来寻找相对应的性能度量标准。</p><h3 id="经验E"><a href="#经验E" class="headerlink" title="经验E"></a><strong>经验E</strong></h3><p>根据学习过程中的不同经验，机器学习算法可以大致分类为无监督（unsupervised）算法和监督（supervised）算法。  </p><ul><li>无监督学习算法（unsupervised learning algorithm）：训练包含有很多特征的数据集，然后学习出这个数据集上有用的结构性质。</li><li>监督学习算法（supervised learning algorithm）：训练含有很多特征的数据集，不过数据集中的样本都有一个标签（label）或者目标（target）。</li></ul><p>大部分学习算法可以被理解为在整个 <strong>数据集（dataset）</strong> 上获取经验。数据集可以用很多不同方式来表示，在所有的情况下，数据集都是样本的集合，而样本是特征的集合。</p><ul><li>表示数据集的常用方法是 <strong>设计矩阵（design matrix）</strong></li></ul><h2 id="容量、过拟合和欠拟合"><a href="#容量、过拟合和欠拟合" class="headerlink" title="容量、过拟合和欠拟合"></a>容量、过拟合和欠拟合</h2><p>机器学习的主要挑战是我们的算法必须能够在先前未观测到的新输入上表现良好，而不只是在训练集上表现良好。在先前未观测到的输入上表现良好的能力被称为 <strong>泛化（generalization）</strong>。</p><ul><li>训练误差（training error）：使用训练集，进行性能度量的一个量。来优化模型</li><li>泛化误差（generalization errot）：（也被称为测试误差（test error）），使用测试集，进行性能度量的一个量。</li></ul><p>训练集和测试集通过数据集上被称为数据生成过程（data generating process）的概率分部生成。在生成数据集时，不会提前固定参数，然后采样得到两个数据集，一般是先采样得到训练集，然后挑选参数去降低训练集误差，然后采样得到测试集。在这个过程中，测试误差会大于或等于训练误差期望。以下是决定机器学习算法效果是否好的因素：  </p><ul><li>降低训练误差</li><li>缩小训练误差和测试误差的差距  </li></ul><p>这两个因素对应机器学习的两个主要挑战：<strong>欠拟合（underfitting）和过拟合（overfitting）</strong>。</p><p>通过调整模型的容量（capacity）可以控制模型是否偏向于过拟合或者欠拟合。通俗来讲，模型的容量是指其拟合各种函数的能力。容量低的模型可能很难拟合训练集，容量高的模型可能会过拟合，因为记住了不适用于测试集的训练集性质。一种控制训练算法容量的方法是选择假设空间（hypothesis space），即学习算法可以选择为解决方法的函数集。容量不仅取决与模型的选择，模型规定了调整参数降低训练目标时，学习算法可以从哪些函数中选择函数。这被称为模型的表示容量（representational capacity）。在很多情况下，由于额外的限制因素，比如优化算法的不完美，意味着学习算法的有效容量（effective capacity）可能小于模型的表示容量。</p><h3 id="没有免费午餐定理"><a href="#没有免费午餐定理" class="headerlink" title="没有免费午餐定理"></a><strong>没有免费午餐定理</strong></h3><p>机器学习的没有免费午餐定理（no free lunch theorem）表明（Wolpert，1996），在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率。换言之，在某种意义上，没有一个机器学习算法总是比其他的要好。</p><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a><strong>正则化</strong></h3><p>正则化是指修改学习算法，使其降低泛化误差而非训练误差。正则化是机器学习领域领域的中心问题之一，只有优化能够与其重要性想提并论。</p><h2 id="超参数和验证集"><a href="#超参数和验证集" class="headerlink" title="超参数和验证集"></a>超参数和验证集</h2><p>大多数机器学习算法都有超参数，可以设置来控制算法行为。超参数的值不是通过学习算法本身学习出来的（尽管我们可以设置一个嵌套的学习过程，一个学习算法为另一个学习算法学习出最有超参数）<br>用于挑选超参数的数据子集被称为验证集</p><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a><strong>交叉验证</strong></h3><p>当数据集太小时，也有替代方法允许我们使用所有的样本估计平均测试误差，代价是增加了计算量。最常用的方法是K-折交叉验证过程。</p><ul><li>K-折交叉验证：将数据集分成k个不重合的子集，测试误差可以估计为k次计算后的平均测试误差。在第i次测试时，数据的第i个自己用于测试集，其他的数据用于训练集，但是带来的一个问题是不存在平均误差方差的无偏估计（Bengio and Grandvalet，2004）</li></ul><h2 id="估计、偏差和方差"><a href="#估计、偏差和方差" class="headerlink" title="估计、偏差和方差"></a>估计、偏差和方差</h2><p>统计领域为我们提供了很多工具来实现机器学习目标，不仅可以解决训练集上的任务，还可以泛化。基本的概念，例如参数估计、偏差和方差，对于正式的刻画泛化、欠拟合和过拟合都非常有帮助。</p><h3 id="点估计"><a href="#点估计" class="headerlink" title="点估计"></a><strong>点估计</strong></h3><p>点估计试图为一些感兴趣的量提供单个“最优”预测。一般地，感兴趣的量可以是单个参数，或是某些参数模型中的一个向量参数。</p><ul><li>函数估计：有时我们会关注函数估计（或函数近似）。</li></ul><h3 id="偏差"><a href="#偏差" class="headerlink" title="偏差"></a><strong>偏差</strong></h3><p>估计的偏差有 <strong>无偏（unbiased）和渐近无偏（asymptotically unbiased）</strong>。</p><ul><li>伯努利分布</li><li>均值的高斯分布估计</li><li><p>高斯分布方差估计</p><ul><li>样本方差</li></ul></li><li><p>无偏样本方差</p></li></ul><h3 id="方差和标准差"><a href="#方差和标准差" class="headerlink" title="方差和标准差"></a><strong>方差和标准差</strong></h3><p>我们有时会考虑估计量的另一个性质是它作为数据样本的函数，期望的变化程度是多少。正如我们可以计算估计量的期望来决定它的偏差，我们也可以计算它的方差。估计量的方差就是一个方差，另外，方差的平方根被称为标准差（standard error）。</p><h3 id="权衡偏差和方差以最小化均方误差"><a href="#权衡偏差和方差以最小化均方误差" class="headerlink" title="权衡偏差和方差以最小化均方误差"></a><strong>权衡偏差和方差以最小化均方误差</strong></h3><p>偏差和方差度量着估计量的两个不同误差来源。偏差度量着偏离真实函数或参数的误差期望，而方差度量这数据上任意特定采样可能导致的估计期望的偏差。<br>当面对一个偏差更大的估计和一个方差更大的估计时，我们如何进行选择。判断这种权衡最常用的方法是<strong>交叉验证</strong>，我们还可以比较这些估计的<strong>均方误差（mean squared error， MSE）</strong>。MSE度量着估计和真实值之间平方误差的总体期望偏差。</p><h3 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a><strong>一致性</strong></h3><p>一致性保证了估计量的偏差会随着样本数目的增多而减少。然而，反过来是不正确的——渐进无偏并不意味着一致性。</p><h2 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h2><p>之前，我们已经看到过常用估计的定义，并分析了它们的性质。但是这些估计是从哪里来的呢？我们希望有些准则可以让我们从不同模型中得到特定函数作为好的估计，而不是猜测某些函数可能是好的估计，然后分析其偏差和方差。<br>最常用的准则是最大似然估计。</p><h3 id="条件对数似然和均方误差"><a href="#条件对数似然和均方误差" class="headerlink" title="条件对数似然和均方误差"></a><strong>条件对数似然和均方误差</strong></h3><ul><li>线性回归作为最大似然</li></ul><h3 id="最大似然的性质"><a href="#最大似然的性质" class="headerlink" title="最大似然的性质"></a><strong>最大似然的性质</strong></h3><p>最大似然估计最吸引人的地方在于，它被证明当样本数目趋向于无穷大时，就收敛率而言是最好的渐进估计。<br>在合适的条件下，最大似然估计具有一致性，意味着训练样本数目趋向于无穷大时，参数的最大似然估计会收敛到参数的真实值。</p><h2 id="贝叶斯统计"><a href="#贝叶斯统计" class="headerlink" title="贝叶斯统计"></a>贝叶斯统计</h2><p>前面讨论的属于<strong>频率派统计（frequentist statistics）</strong> 方法和基于估计单一值的方法，然后基于该估计作所有的预测。另一种方法是在做预测时会考虑所有可能的值。后者属于<strong>贝叶斯统计（Bayesian statistics）</strong>的范畴。  </p><ul><li>贝叶斯线性回归</li></ul><h3 id="最大后验（MAP）估计"><a href="#最大后验（MAP）估计" class="headerlink" title="最大后验（MAP）估计"></a><strong>最大后验（MAP）估计</strong></h3><p> MAP贝叶斯推断提供了一个直观的方法来设计复杂但可解释的正则化项。例如，更复杂的惩罚项可以通过混合高斯分部作为先验得到，而不是一个单独的高斯分布（Nowlan and Hinton，1992）</p><h2 id="监督学习算法"><a href="#监督学习算法" class="headerlink" title="监督学习算法"></a>监督学习算法</h2><p>粗略的说，监督学习算法是给定一组输入x和输出y的训练集，学习如何关联输入和输出。在许多情况下，输出y很难自动收集，必须由人来提供“监督”，不过该术语仍然适用于训练集目标可以被自动收集的情况。  </p><h3 id="概率监督学习"><a href="#概率监督学习" class="headerlink" title="概率监督学习"></a><strong>概率监督学习</strong></h3><h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a><strong>支持向量机</strong></h3><p>支持向量机（support vector machine，SVM）是监督学习中最有影响力的方法之一。类似于逻辑回归，但支持向量机不输出概率，只输出类别。<br>支持向量机的一个重要创新是核技巧（kernal trick）。核技巧观察到许多机器学习算法都可以写成样本间点积的形式。<br>最常用的核函数是高斯核（Gaussian kernel），这个核也被称为径向基函数（radial basis function，RBF）核。<br>支持向量机不是唯一可以使用核技巧来增强的算法，许多其他的线性模型也可以通过这种方式来增强。使用核技巧的算法类别被称为和机器（kernel machine）或核方法（kernel method）。核机器的一个主要缺点是计算决策函数的成本关于训练样本的数目是线性的。而训练的样本则被称做支持向量（support vector）。</p><h3 id="其他简单的机器学习算法"><a href="#其他简单的机器学习算法" class="headerlink" title="其他简单的机器学习算法"></a><strong>其他简单的机器学习算法</strong></h3><ul><li>决策树</li></ul><h2 id="无监督学习算法"><a href="#无监督学习算法" class="headerlink" title="无监督学习算法"></a>无监督学习算法</h2><p>无监督算法只处理“特征”，不操作监督信号。监督和无监督算法之间的区别没有规范严格的定义，因为没有客观的判断来区分监督者提供的值是特征还是目标。通俗的说，无监督学习的大多数尝试是指从不需要人为注释的样本的分布中抽取信息。该术语通常与密度估计相关，学习从分布中采样、学习从分布中去噪、寻找数据分布的流形或是将数据中相关的样本聚类。</p><h3 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a><strong>主成分分析</strong></h3><h3 id="k-均值聚类"><a href="#k-均值聚类" class="headerlink" title="k-均值聚类"></a><strong>k-均值聚类</strong></h3><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>几乎所有的深度学习算法都用到了一个非常重要的算法：梯度下降算法（stochastic gradient,SGD）。<br>机器学习中反复出现的一个问题是好的泛化需要大的训练集，但大的计算集的计算代价也更大。机器学习算法中的代价函数通常可以分解成每个样本的代价函数的总和。<br>随机梯度下降的核心是，梯度是期望。期望可使用小规模的样本近似估计。</p><h2 id="构建机器学习算法"><a href="#构建机器学习算法" class="headerlink" title="构建机器学习算法"></a>构建机器学习算法</h2><h2 id="构建机器学习算法-1"><a href="#构建机器学习算法-1" class="headerlink" title="构建机器学习算法"></a>构建机器学习算法</h2><h2 id="促使深度学习发展的挑战"><a href="#促使深度学习发展的挑战" class="headerlink" title="促使深度学习发展的挑战"></a>促使深度学习发展的挑战</h2><ul><li>维度灾难</li><li>局部不变性和平滑正则化</li><li>流形学习</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;机器学习和深度学习的关系在此不做赘述，本文主要说明机器学习的基本知识&lt;/p&gt;
&lt;h2 id=&quot;学习算法&quot;&gt;&lt;a href=&quot;#学习算法&quot; class=&quot;headerlink&quot; title=&quot;学习算法&quot;&gt;&lt;/a&gt;学习算法&lt;/h2&gt;&lt;p&gt;机器学习算法是一种能够从数据中学习的算法
      
    
    </summary>
    
      <category term="机器学习算法" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="概念" scheme="http://yoursite.com/tags/%E6%A6%82%E5%BF%B5/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch--线性回归和逻辑回归</title>
    <link href="http://yoursite.com/2018/10/20/Pytorch-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2018/10/20/Pytorch-线性回归和逻辑回归/</id>
    <published>2018-10-20T05:53:49.000Z</published>
    <updated>2018-11-05T13:34:59.932Z</updated>
    
    <content type="html"><![CDATA[<h3 id="代码如下"><a href="#代码如下" class="headerlink" title="代码如下"></a><strong>代码如下</strong></h3><p>利用torch中的线性回归和逻辑回归模块实现</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">torch 一维线性回归算法</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成训练数据</span></span><br><span class="line">np.random.seed(<span class="number">10</span>)</span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">30</span>, <span class="number">20</span>)</span><br><span class="line">y = x * <span class="number">3</span> + np.random.normal(<span class="number">0</span>, <span class="number">5</span>, <span class="number">20</span>)</span><br><span class="line">x = np.array(x, dtype=np.float32).reshape([<span class="number">20</span>, <span class="number">1</span>])</span><br><span class="line">y = np.array(y, dtype=np.float32).reshape([<span class="number">20</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为torch中的张量形式</span></span><br><span class="line">x_train = torch.from_numpy(x)</span><br><span class="line">y_train = torch.from_numpy(y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    线性回归模型：一维线性回归</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LinearRegression, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model  = LinearRegression().cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model = LinearRegression()</span><br><span class="line"></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        inputs = Variable(x_train).cuda()</span><br><span class="line">        target = Variable(y_train).cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        inputs = Variable(x_train)</span><br><span class="line">        target = Variable(y_train)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    out = model(inputs)</span><br><span class="line">    loss = criterion(out, target)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Epoch[&#123;&#125;/&#123;&#125;], loss:&#123;:.6f&#125;'</span>.format(epoch+<span class="number">1</span>, num_epochs, loss.data[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">model.eval()</span><br><span class="line">predict = model(Variable(x_train))</span><br><span class="line">predict = predict.data.numpy()</span><br><span class="line"></span><br><span class="line">plt.plot(x_train.numpy(), y_train.numpy(), <span class="string">'ro'</span>, label=<span class="string">'Original data'</span>)</span><br><span class="line">plt.plot(x_train.numpy(), predict, label=<span class="string">'predict data'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">torch 一维线性回归算法(多项式回归)</span></span><br><span class="line"><span class="string">'</span><span class="string">''</span></span><br><span class="line"><span class="comment"># 生成训练数据</span></span><br><span class="line"></span><br><span class="line">def make_features(x):</span><br><span class="line">    <span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">    建立多项式特征</span></span><br><span class="line"><span class="string">    '</span><span class="string">''</span></span><br><span class="line">    x = x.unsqueeze(1)</span><br><span class="line">    <span class="built_in">return</span> torch.cat([x ** i <span class="keyword">for</span> i <span class="keyword">in</span> range(1, 4)], 1)</span><br><span class="line"></span><br><span class="line">W_target = torch.FloatTensor([0.5, 3, 2.4]).unsqueeze(1)</span><br><span class="line">b_target = torch.FloatTensor([0.9])</span><br><span class="line"></span><br><span class="line">def f(x):</span><br><span class="line">    <span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">    实际函数</span></span><br><span class="line"><span class="string">    '</span><span class="string">''</span></span><br><span class="line">    <span class="built_in">return</span> x.mm(W_target) + b_target[0]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_batch(batch_size=32):</span><br><span class="line">    <span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">    生成训练数据</span></span><br><span class="line"><span class="string">    '</span><span class="string">''</span></span><br><span class="line">    random = torch.randn(batch_size)</span><br><span class="line">    x = make_features(random)</span><br><span class="line">    y = f(x)</span><br><span class="line">    <span class="comment"># print(random,x)</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        <span class="built_in">return</span> Variable(x).cuda(), Variable(y).cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">return</span> Variable(random), Variable(x), Variable(y)</span><br><span class="line"></span><br><span class="line">class poly_model(nn.Module):</span><br><span class="line">    <span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">    多项式线性回归（三维）</span></span><br><span class="line"><span class="string">    '</span><span class="string">''</span></span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(poly_model, self).__init__()</span><br><span class="line">        self.poly = nn.Linear(3, 1)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        out = self.poly(x)</span><br><span class="line">        <span class="built_in">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model = poly_model().cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model = poly_model()</span><br><span class="line"></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=0.001)</span><br><span class="line"></span><br><span class="line">epoch = 0</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> True:</span><br><span class="line">    _, batch_x, batch_y = get_batch()</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    out = model(batch_x)</span><br><span class="line">    loss = criterion(out, batch_y)</span><br><span class="line">    print_loss = loss.data[0]</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(print_loss)</span><br><span class="line">    epoch += 1 </span><br><span class="line">    <span class="keyword">if</span> print_loss &lt; 0.001:</span><br><span class="line">        <span class="built_in">break</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">逻辑回归</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LogisticRegression, self).__init__()</span><br><span class="line">        self.lr = nn.Linear(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">        self.sm = nn.Sigmoid()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.lr(x)</span><br><span class="line">        x = self.sm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">logistic_model = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    logistic_model.cuda()</span><br><span class="line"></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line">optimezer = torch.optim.SGD(logistic_model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">50000</span>):</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        x = Variable(x_data).cuda()</span><br><span class="line">        y = Variable(y_data).cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = Variable(x_data)</span><br><span class="line">        y = Variable(y_data)</span><br><span class="line">    </span><br><span class="line">    out = logistic_model(x)</span><br><span class="line">    loss = criterion(out, y)</span><br><span class="line">    print_loss = loss.data[<span class="number">0</span>]</span><br><span class="line">    mask = out.ge(<span class="number">0.5</span>).float()</span><br><span class="line">    correct = (mask == y).sum()</span><br><span class="line">    acc = correct.data[<span class="number">0</span>] / x.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'*'</span>*<span class="number">10</span>)</span><br><span class="line">        print(<span class="string">'epoch &#123;&#125;'</span>.format(epoch+<span class="number">1</span>))</span><br><span class="line">        print(<span class="string">'loss is &#123;:.4f&#125;'</span>.format(print_loss))</span><br><span class="line">        print(<span class="string">'acc is &#123;:.4f&#125;'</span>.format(acc))</span><br><span class="line">        <span class="number">0</span></span><br><span class="line"></span><br><span class="line">w0, w1 = logistic_model.lr.weight[<span class="number">0</span>]</span><br><span class="line">w0 = w0.data[<span class="number">0</span>]</span><br><span class="line">w1 = w1.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">b = logistic_model.lr.bias.data[<span class="number">0</span>]</span><br><span class="line">plot_x = np.arrange(<span class="number">30</span>,<span class="number">100</span>,<span class="number">0.1</span>)</span><br><span class="line">plot_y = (-w0 * plot_x -b)/ w1</span><br><span class="line">plot.plot(plot_x, plot_y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;代码如下&quot;&gt;&lt;a href=&quot;#代码如下&quot; class=&quot;headerlink&quot; title=&quot;代码如下&quot;&gt;&lt;/a&gt;&lt;strong&gt;代码如下&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;利用torch中的线性回归和逻辑回归模块实现&lt;/p&gt;
&lt;figure class=&quot;hig
      
    
    </summary>
    
      <category term="Pytorch框架" scheme="http://yoursite.com/categories/Pytorch%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="线性回归" scheme="http://yoursite.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="逻辑回归" scheme="http://yoursite.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>机器学习——线性回归和逻辑回归</title>
    <link href="http://yoursite.com/2018/10/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2018/10/17/机器学习——线性回归和逻辑回归/</id>
    <published>2018-10-17T10:01:24.000Z</published>
    <updated>2018-10-20T05:52:40.969Z</updated>
    
    <content type="html"><![CDATA[<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a><strong>线性回归</strong></h2><h3 id="简述"><a href="#简述" class="headerlink" title="简述"></a><strong>简述</strong></h3><p>在统计学中，线性回归（Linear Regression）是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合（自变量都是一次方）。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。简单来说，就是找到一条直线去拟合数据点。如下图：<br><img src="/2018/10/17/机器学习——线性回归和逻辑回归/Figure_1.png" alt="图片"></p><p>优点：结果易于理解，计算上不复杂。<br>缺点：对非线性数据拟合不好。<br>适用数据类型：数值型和标称型数据。<br>算法类型：回归算法</p><p>线性回归的模型函数如下：  </p><p>$$h_\theta = \theta^Tx$$  </p><p>它的损失函数如下：  </p><p>$$J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2$$  </p><p>通过训练数据集寻找参数的最优解，即求解可以得到$minJ(θ)$的参数向量$θ$,其中这里的参数向量也可以分为参数和$w$和$b$,分别表示权重和偏置值。<br>求解最优解的方法有最小二乘法和梯度下降法。</p><ul><li><p><strong>梯度下降法</strong><br>  梯度下降算法的思想如下(这里以一元线性回归为例)：</p><p>  首先，我们有一个代价函数，假设是$J(θ_0,θ_1)$，我们的目标是$minθ_0,θ_1 J(θ_0,θ_1)$。<br>  接下来的做法是：</p><ul><li>首先是随机选择一个参数的组合$(θ_0,θ_1)$,一般是设$θ_0=0,θ_1=0$;</li><li><p>然后是不断改变$(θ_0,θ_1)$，并计算代价函数，直到一个局部最小值。之所以是局部最小值，是因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值，选择不同的初始参数组合，可能会找到不同的局部最小值。<br>下面给出梯度下降算法的公式：<br>repeat until convergence{</p><p>  $$\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(θ_0,θ_1)(for\ j =0\ and\ j=1)$$<br>}<br>也就是在梯度下降中，不断重复上述公式直到收敛，也就是找到局部最小值局部最小值。其中符号$:=$是赋值符号的意思。</p></li><li><p>而应用梯度下降法到线性回归，则公式如下：  </p><p>$$\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{i})-y^i)$$<br>$$\theta_1 := \theta_1 - \alpha\frac{1}{m}\sum_{i=1}^m((h_\theta(x^i)-y^i)\cdot x^i)$$  </p><p>公式中的$\alpha$称为学习率(learning rate)，它决定了我们沿着能让代价函数下降程度最大的<br>方向向下迈进的步子有多大。<br>在梯度下降中，还涉及都一个参数更新的问题，即更新$(\theta_0,\theta_1)$,一般我们的做法是同步更新.<br>最后，上述梯度下降算法公式实际上是一个叫<strong>批量梯度下降(batch gradient descent)</strong>，即它在每次梯度下降中都是使用整个训练集的数据，<br>所以公式中是带有$ \sum_{i=1}^m $.</p></li></ul></li><li><p><strong>岭回归（ridge regression）</strong><br>  岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价，获得回归系数更为符合实际、更可靠的回归方法，对病态数据的耐受性远远强于最小二乘法。</p><p>  岭回归分析法是从根本上消除复共线性影响的统计方法。岭回归模型通过在相关矩阵中引入一个很小的岭参数$K（1&gt;K&gt;0）$，并将它加到主对角线元素上，从而降低参数的最小二乘估计中复共线特征向量的影响，减小复共线变量系数最小二乘估计的方法，以保证参数估计更接近真实情况。岭回归分析将所有的变量引入模型中，比逐步回归分析提供更多的信息。</p></li></ul><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a><strong>代码实现</strong></h3><p>线性回归的相关数据及代码<a href="https://github.com/zouzhen/machine-learning-algorithms-in-python" target="_blank" rel="noopener">点此</a></p><ul><li><p>使用sklearn包中的线性回归算法</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets, linear_model</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error, r2_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the diabetes dataset</span></span><br><span class="line">diabetes = datasets.load_diabetes()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use only one feature</span></span><br><span class="line">diabetes_X = diabetes.data[:, np.newaxis, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the data into training/testing sets</span></span><br><span class="line">diabetes_X_train = diabetes_X[:<span class="number">-20</span>]</span><br><span class="line">diabetes_X_test = diabetes_X[<span class="number">-20</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the targets into training/testing sets</span></span><br><span class="line">diabetes_y_train = diabetes.target[:<span class="number">-20</span>]</span><br><span class="line">diabetes_y_test = diabetes.target[<span class="number">-20</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create linear regression object</span></span><br><span class="line">regr = linear_model.LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model using the training sets</span></span><br><span class="line">regr.fit(diabetes_X_train, diabetes_y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions using the testing set</span></span><br><span class="line">diabetes_y_pred = regr.predict(diabetes_X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The coefficients</span></span><br><span class="line">print(<span class="string">'Coefficients: \n'</span>, regr.coef_)</span><br><span class="line"><span class="comment"># The mean squared error</span></span><br><span class="line">print(<span class="string">"Mean squared error: %.2f"</span></span><br><span class="line">      % mean_squared_error(diabetes_y_test, diabetes_y_pred))</span><br><span class="line"><span class="comment"># Explained variance score: 1 is perfect prediction</span></span><br><span class="line">print(<span class="string">'Variance score: %.2f'</span> % r2_score(diabetes_y_test, diabetes_y_pred))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot outputs</span></span><br><span class="line">plt.scatter(diabetes_X_test, diabetes_y_test,  color=<span class="string">'black'</span>)</span><br><span class="line">plt.plot(diabetes_X_test, diabetes_y_pred, color=<span class="string">'blue'</span>, linewidth=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">plt.xticks(())</span><br><span class="line">plt.yticks(())</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li><li><p>使用代码实现算法</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">path = os.path.dirname(os.getcwd()) + <span class="string">'\data\ex1data1.txt'</span></span><br><span class="line">data = pd.read_csv(path, header=<span class="keyword">None</span>, names=[<span class="string">'Population'</span>, <span class="string">'Profit'</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(X, y, theta)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    损失函数</span></span><br><span class="line"><span class="string">    X: 自变量</span></span><br><span class="line"><span class="string">    y: 因变量</span></span><br><span class="line"><span class="string">    theta: 参数向量</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    inner = np.power(((X * theta.T) - y), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.sum(inner) / (<span class="number">2</span> * len(X))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X, y, theta, alpha, iters)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    梯度下降算法</span></span><br><span class="line"><span class="string">    X: 自变量</span></span><br><span class="line"><span class="string">    y: 因变量</span></span><br><span class="line"><span class="string">    theta: 参数向量</span></span><br><span class="line"><span class="string">    alpha: 学习率</span></span><br><span class="line"><span class="string">    iters: 计算次数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 暂存参数向量</span></span><br><span class="line">    temp = np.matrix(np.zeros(theta.shape))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将参数向量降为一维，返回视图，可以修改原始的参数向量</span></span><br><span class="line">    parameters = int(theta.ravel().shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 损失值消耗记录</span></span><br><span class="line">    cost = np.zeros(iters)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度下降的计算</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">        error = (X * theta.T) - y</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(parameters):</span><br><span class="line">            term = np.multiply(error, X[:, j])</span><br><span class="line">            temp[<span class="number">0</span>, j] = theta[<span class="number">0</span>, j] - ((alpha / len(X)) * np.sum(term))</span><br><span class="line"></span><br><span class="line">        theta = temp</span><br><span class="line">        cost[i] = computeCost(X, y, theta)</span><br><span class="line">    <span class="keyword">return</span> theta, cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># append a ones column to the front of the data set</span></span><br><span class="line">data.insert(<span class="number">0</span>, <span class="string">'Ones'</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set X (training data) and y (target variable)</span></span><br><span class="line">cols = data.shape[<span class="number">1</span>]</span><br><span class="line">X = data.iloc[:, <span class="number">0</span>:cols - <span class="number">1</span>]</span><br><span class="line">y = data.iloc[:, cols - <span class="number">1</span>:cols]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># convert from data frames to numpy matrices</span></span><br><span class="line">X = np.matrix(X.values)</span><br><span class="line">y = np.matrix(y.values)</span><br><span class="line">theta = np.matrix(np.array([<span class="number">0</span>, <span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize variables for learning rate and iterations</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line">iters = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># perform gradient descent to "fit" the model parameters</span></span><br><span class="line">g, cost = gradientDescent(X, y, theta, alpha, iters)</span><br><span class="line"></span><br><span class="line">x = np.linspace(data.Population.min(), data.Population.max(), <span class="number">100</span>)</span><br><span class="line">f = g[<span class="number">0</span>, <span class="number">0</span>] + (g[<span class="number">0</span>, <span class="number">1</span>] * x)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">ax.plot(x, f, <span class="string">'r'</span>, label=<span class="string">'Prediction'</span>)</span><br><span class="line">ax.scatter(data.Population, data.Profit, label=<span class="string">'Traning Data'</span>)</span><br><span class="line">ax.legend(loc=<span class="number">2</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Population'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Profit'</span>)</span><br><span class="line">ax.set_title(<span class="string">'Predicted Profit vs. Population Size'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看损失值的变化</span></span><br><span class="line"><span class="comment"># fig, ax = plt.subplots(figsize=(12,8))</span></span><br><span class="line"><span class="comment"># ax.plot(np.arange(iters), cost, 'r')</span></span><br><span class="line"><span class="comment"># ax.set_xlabel('Iterations')</span></span><br><span class="line"><span class="comment"># ax.set_ylabel('Cost')</span></span><br><span class="line"><span class="comment"># ax.set_title('Error vs. Training Epoch')</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a><strong>逻辑回归</strong></h2><h3 id="简述-1"><a href="#简述-1" class="headerlink" title="简述"></a><strong>简述</strong></h3><p>Logistic回归算法基于$Sigmoid$函数，或者说$Sigmoid$就是逻辑回归函数。$Sigmoid$函数定义如下： $\frac{1}{1+e^{-z}}$。函数值域范围$(0,1)$。<br>因此逻辑回归函数的表达式如下：  </p><p>$$h_\theta = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}}$$<br>$$其中，g(z) = \frac{1}{1+e^{-z}}$$  </p><p>其导数形式为：  </p><p>$$g\prime(z) = \frac{d}{dz}\frac{1}{1+e^{-z}}$$<br>$$=\frac{1}{(1+e^{-z})^2}(e^{-z})$$<br>$$=\frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}})$$<br>$$ = g(z)(1-g(z))$$</p><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a><strong>代价函数</strong></h3><p>逻辑回归方法主要是用最大似然估计来学习的，所以单个样本的后验概率为：  </p><p>$$p(y|x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}$$</p><p>到整个样本的后验概率就是:  </p><p>$$L(\theta) = p(y|X;\theta)$$<br>$$ = \prod_{i=1}^{m}p(y^i|x^i;\theta)$$<br>$$ = \prod_{i=1}^{m}(h_\theta(x^i))^{y^i}(1-h_\theta(x^i))^{1-y^i}$$<br>$$其中，P(y=1|x;\theta)=h_\theta(x),P(y=0|x;\theta)=1-h_\theta(x)$$<br>$$通过对数进一步简化有：l(\theta) = \log L(\theta) = \sum_{i=1}^m(y^i\log h(x^i)+(1-y^i)\log(1-h(x^i)))$$</p><p>而逻辑回归的代价函数就是$−l(\theta)$。也就是如下所示：  </p><p>$$J(\theta) = \frac{1}{m}\left[\sum_{i=1}^{m}y^i\log h_\theta(x^i)+(1-y^i)\log(1-h_\theta(x^i))\right]$$</p><p>同样可以使用梯度下降算法来求解使得代价函数最小的参数。其梯度下降法公式为：  </p><p>$$\frac{\partial}{\partial\theta_j}l(\theta) = \left(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)}\right)\frac{\partial}{\partial\theta_j}g(\theta^Tx)$$<br>$$= \left(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)}\right)\left(1-g(\theta^Tx)\frac{\partial}{\partial\theta_j}(\theta^Tx)\right)g(\theta^Tx)$$<br>$$= (y(1-g(\theta^Tx))-(1-y)g(\theta^Tx))x_j$$<br>$$= (y-h_\theta(x))x_j$$</p><p>$$\theta_j := \theta_j + \alpha(y^i-h_\theta(x^i)x_j^i$$</p><ul><li><p>总结<br>  优点：</p><p>  　　1、实现简单；</p><p>  　　2、分类时计算量非常小，速度很快，存储资源低；</p><p>  缺点：</p><p>  　　1、容易欠拟合，一般准确度不太高</p><p>  　　2、只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；</p><p>  适用数据类型：数值型和标称型数据。<br>  类别：分类算法。<br>  试用场景：解决二分类问题。<br>  如下图：<br>  <img src="/2018/10/17/机器学习——线性回归和逻辑回归/Figure_3.png" alt="图片3"><br>  <img src="/2018/10/17/机器学习——线性回归和逻辑回归/Figure_2.png" alt="图片2"></p></li></ul><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a><strong>代码实现</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">Plot multinomial and One-vs-Rest Logistic Regression</span></span><br><span class="line"><span class="string">'</span><span class="string">''</span></span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># make 3-class dataset for classification</span></span><br><span class="line">centers = [[-5, 0], [0, 1.5], [5, -1]]</span><br><span class="line">X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)</span><br><span class="line">transformation = [[0.4, 0.2], [-0.4, 1.2]]</span><br><span class="line">X = np.dot(X, transformation)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> multi_class <span class="keyword">in</span> (<span class="string">'multinomial'</span>, <span class="string">'ovr'</span>):</span><br><span class="line">    clf = LogisticRegression(solver=<span class="string">'sag'</span>, max_iter=100, random_state=42,</span><br><span class="line">                             multi_class=multi_class).fit(X, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print the training scores</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"training score : %.3f (%s)"</span> % (clf.score(X, y), multi_class))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create a mesh to plot in</span></span><br><span class="line">    h = .02  <span class="comment"># step size in the mesh</span></span><br><span class="line">    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                         np.arange(y_min, y_max, h))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot the decision boundary. For that, we will assign a color to each</span></span><br><span class="line">    <span class="comment"># point in the mesh [x_min, x_max]x[y_min, y_max].</span></span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    <span class="comment"># Put the result into a color plot</span></span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)</span><br><span class="line">    plt.title(<span class="string">"Decision surface of LogisticRegression (%s)"</span> % multi_class)</span><br><span class="line">    plt.axis(<span class="string">'tight'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot also the training points</span></span><br><span class="line">    colors = <span class="string">"bry"</span></span><br><span class="line">    <span class="keyword">for</span> i, color <span class="keyword">in</span> zip(clf.classes_, colors):</span><br><span class="line">        idx = np.where(y == i)</span><br><span class="line">        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired,</span><br><span class="line">                    edgecolor=<span class="string">'black'</span>, s=20)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot the three one-against-all classifiers</span></span><br><span class="line">    xmin, xmax = plt.xlim()</span><br><span class="line">    ymin, ymax = plt.ylim()</span><br><span class="line">    coef = clf.coef_</span><br><span class="line">    intercept = clf.intercept_</span><br><span class="line"></span><br><span class="line">    def plot_hyperplane(c, color):</span><br><span class="line">        def line(x0):</span><br><span class="line">            <span class="built_in">return</span> (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]</span><br><span class="line">        plt.plot([xmin, xmax], [line(xmin), line(xmax)],</span><br><span class="line">                 ls=<span class="string">"--"</span>, color=color)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, color <span class="keyword">in</span> zip(clf.classes_, colors):</span><br><span class="line">        plot_hyperplane(i, color)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">代码实现(加入正则化)</span></span><br><span class="line"><span class="string">'</span><span class="string">''</span></span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import scipy.optimize as opt</span><br><span class="line">import os</span><br><span class="line">path = os.path.dirname(os.getcwd()) + <span class="string">'\data\ex2data1.txt'</span></span><br><span class="line">data2 = pd.read_csv(path, header=None, names=[<span class="string">'Test 1'</span>, <span class="string">'Test 2'</span>, <span class="string">'Accepted'</span>])</span><br><span class="line"></span><br><span class="line">positive = data2[data2[<span class="string">'Accepted'</span>].isin([1])]</span><br><span class="line">negative = data2[data2[<span class="string">'Accepted'</span>].isin([0])]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sigmoid(z):</span><br><span class="line">    <span class="built_in">return</span> 1 / (1 + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def costReg(theta, X, y, learningRate):</span><br><span class="line">    theta = np.matrix(theta)</span><br><span class="line">    X = np.matrix(X)</span><br><span class="line">    y = np.matrix(y)</span><br><span class="line">    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))</span><br><span class="line">    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))</span><br><span class="line">    reg = (learningRate / 2 * len(X)) * np.sum(np.power(theta[:, 1:theta.shape[1]], 2))</span><br><span class="line">    <span class="built_in">return</span> np.sum(first - second) / (len(X)) + reg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def gradientReg(theta, X, y, learningRate):</span><br><span class="line">    theta = np.matrix(theta)</span><br><span class="line">    X = np.matrix(X)</span><br><span class="line">    y = np.matrix(y)</span><br><span class="line"></span><br><span class="line">    parameters = int(theta.ravel().shape[1])</span><br><span class="line">    grad = np.zeros(parameters)</span><br><span class="line"></span><br><span class="line">    error = sigmoid(X * theta.T) - y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(parameters):</span><br><span class="line">        term = np.multiply(error, X[:,i])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i == 0):</span><br><span class="line">            grad[i] = np.sum(term) / len(X)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            grad[i] = (np.sum(term) / len(X)) + ((learningRate / len(X)) * theta[:,i])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> grad</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def predict(theta, X):</span><br><span class="line">    probability = sigmoid(X * theta.T)</span><br><span class="line">    <span class="built_in">return</span> [1 <span class="keyword">if</span> x &gt;= 0.5 <span class="keyword">else</span> 0 <span class="keyword">for</span> x <span class="keyword">in</span> probability]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">degree = 5</span><br><span class="line">x1 = data2[<span class="string">'Test 1'</span>]</span><br><span class="line">x2 = data2[<span class="string">'Test 2'</span>]</span><br><span class="line"></span><br><span class="line">data2.insert(3, <span class="string">'Ones'</span>, 1)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(1, degree):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(0, i):</span><br><span class="line">        data2[<span class="string">'F'</span> + str(i) + str(j)] = np.power(x1, i-j) * np.power(x2, j)</span><br><span class="line"></span><br><span class="line">data2.drop(<span class="string">'Test 1'</span>, axis=1, inplace=True)</span><br><span class="line">data2.drop(<span class="string">'Test 2'</span>, axis=1, inplace=True)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set X and y (remember from above that we moved the label to column 0)</span></span><br><span class="line">cols = data2.shape[1]</span><br><span class="line">X2 = data2.iloc[:,1:cols]</span><br><span class="line">y2 = data2.iloc[:,0:1]</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert to numpy arrays and initalize the parameter array theta</span></span><br><span class="line">X2 = np.array(X2.values)</span><br><span class="line">y2 = np.array(y2.values)</span><br><span class="line">theta2 = np.zeros(11)</span><br><span class="line"></span><br><span class="line">learningRate = 0.1</span><br><span class="line">result2 = opt.fmin_tnc(func=costReg, x0=theta2, fprime=gradientReg, args=(X2, y2, learningRate))</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(costReg(theta2, X2, y2, learningRate))</span></span><br><span class="line">theta_min = np.matrix(result2[0])</span><br><span class="line">predictions = predict(theta_min, X2)</span><br><span class="line">correct = [1 <span class="keyword">if</span> ((a == 1 and b == 1) or (a == 0 and b == 0)) <span class="keyword">else</span> 0 <span class="keyword">for</span> (a, b) <span class="keyword">in</span> zip(predictions, y2)]</span><br><span class="line">accuracy = (sum(map(int, correct)) % len(correct))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'accuracy = &#123;0&#125;%'</span>.format(accuracy))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;线性回归&quot;&gt;&lt;a href=&quot;#线性回归&quot; class=&quot;headerlink&quot; title=&quot;线性回归&quot;&gt;&lt;/a&gt;&lt;strong&gt;线性回归&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;简述&quot;&gt;&lt;a href=&quot;#简述&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
      <category term="机器学习算法" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="线性回归" scheme="http://yoursite.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="逻辑回归" scheme="http://yoursite.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>scikit-learn整理</title>
    <link href="http://yoursite.com/2018/10/17/scikit-learn%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/10/17/scikit-learn整理/</id>
    <published>2018-10-17T07:54:09.000Z</published>
    <updated>2018-10-17T09:51:48.931Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Flask整理</title>
    <link href="http://yoursite.com/2018/10/17/Flask%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/10/17/Flask整理/</id>
    <published>2018-10-17T07:50:11.000Z</published>
    <updated>2018-10-17T09:51:48.921Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用工具"><a href="#使用工具" class="headerlink" title="使用工具"></a><strong>使用工具</strong></h2><h4 id="Flask后端-Postgresql数据库-JS前端（我未使用）"><a href="#Flask后端-Postgresql数据库-JS前端（我未使用）" class="headerlink" title="Flask后端 + Postgresql数据库 + JS前端（我未使用）"></a>Flask后端 + Postgresql数据库 + JS前端（我未使用）</h4><h2 id="Flask搭建"><a href="#Flask搭建" class="headerlink" title="Flask搭建"></a>Flask搭建</h2><ul><li><p><strong>确定确定目录结构</strong></p><ol><li>app/algorithms: 用来存放相关的算法文件</li><li>app/models: 用来存放数据库的操作</li><li>app/web: 用来存放路由和视图函数</li><li>manage: flask的启动文件</li></ol></li><li><p><strong>确定路由注册方式</strong></p><ol><li>使用蓝图形式来注册路由</li></ol></li><li><p><strong>确定数据库操作方式</strong></p><ol><li>使用sqlalchemy及psycopg2来控制Postgresql数据库</li><li>由于主要是用来进行数据读取的，所以采用非ORM方式构建的表结构，这种方式方便进行查询过滤操作</li></ol></li></ul><h2 id="基于sqlalchemy的Postgresql数据库访问操作"><a href="#基于sqlalchemy的Postgresql数据库访问操作" class="headerlink" title="基于sqlalchemy的Postgresql数据库访问操作"></a>基于sqlalchemy的Postgresql数据库访问操作</h2><ul><li><strong>创建表结构</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sqlalchemy.engine <span class="keyword">import</span> create_engine</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.schema <span class="keyword">import</span> MetaData, Table, Column, ForeignKey, Sequence</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.types <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.sql.expression <span class="keyword">import</span> select,and_</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line">engine = create_engine(<span class="string">'postgres://user:password@hosts/builder'</span>, echo=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">metadata = MetaData()</span><br><span class="line">metadata.bind = engine</span><br><span class="line">    <span class="comment"># 创建桥梁索引表</span></span><br><span class="line">bridges_table = Table(<span class="string">'bridges'</span>, metadata,</span><br><span class="line">                      Column(<span class="string">'id'</span>, Integer, primary_key=<span class="keyword">True</span>),</span><br><span class="line">                      Column(<span class="string">'org_id'</span>, Integer, nullable=<span class="keyword">False</span>),</span><br><span class="line">                      Column(<span class="string">'user_id'</span>, Integer, nullable=<span class="keyword">False</span>),</span><br><span class="line">                      Column(<span class="string">'name'</span>, VARCHAR(length=<span class="number">255</span>), nullable=<span class="keyword">False</span>),</span><br><span class="line">                      Column(<span class="string">'created_date'</span>, TIMESTAMP, nullable=<span class="keyword">False</span>),</span><br><span class="line">                      Column(<span class="string">'finished_date'</span>, TIMESTAMP, nullable=<span class="keyword">True</span>),</span><br><span class="line">                    <span class="comment">#   autoload=True,</span></span><br><span class="line">                      )</span><br></pre></td></tr></table></figure><p>这种方式，有助于进行表查询，具体的相关API介绍及使用放那格式可<a href="https://docs.sqlalchemy.org/en/latest/genindex.html" target="_blank" rel="noopener">点此</a>查看。</p><ul><li><strong>相关操作</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">()</span>:</span></span><br><span class="line">    s = book_table.insert().values(title=<span class="string">'测试写入2'</span>,time=datetime.now())</span><br><span class="line">    c = engine.execute(s)</span><br><span class="line">    c.close()</span><br><span class="line">    <span class="keyword">return</span> c.inserted_primary_key</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">query_code</span><span class="params">(id)</span>:</span></span><br><span class="line">    info = &#123;<span class="string">'id'</span>: <span class="string">''</span>, <span class="string">'title'</span>: <span class="string">''</span>&#125;</span><br><span class="line">    s = select([bridge_jobs_table.c.id.label(<span class="string">'name'</span>)]).where(and_(bridge_jobs_table.c.kind==<span class="string">'桩基'</span>,bridge_jobs_table.c.name==<span class="string">'起钻'</span>))</span><br><span class="line">    codename_query = engine.execute(s)</span><br><span class="line">    print(codename_query.keys())</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> codename_query:</span><br><span class="line">        print(row[<span class="number">0</span>])</span><br><span class="line">    codename_query.close()</span><br><span class="line">    <span class="keyword">return</span> info</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updata</span><span class="params">(id, title)</span>:</span></span><br><span class="line">    s = book_table.update().where(book_table.c.id == id).values(title=title, id=id)</span><br><span class="line">    c = engine.execute(s)</span><br><span class="line">    c.close()</span><br></pre></td></tr></table></figure><h2 id="Flask相关知识"><a href="#Flask相关知识" class="headerlink" title="Flask相关知识"></a>Flask相关知识</h2><ul><li><p><strong>路由操作</strong></p><ol><li><p>静态路由</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@web.route('/hello', methods=['POST', 'GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'hello world!'</span></span><br></pre></td></tr></table></figure></li><li><p>参数路由</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@web.route('/hello/&lt;string:name&gt;', methods=['POST', 'GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'hello %d '</span>% name</span><br></pre></td></tr></table></figure></li><li><p>JSON返回</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@web.route('/hello/&lt;string:name&gt;', methods=['POST', 'GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> jsonify(<span class="string">'hello %d '</span>% name)</span><br></pre></td></tr></table></figure></li><li><p>使用蓝图方式注册路由</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_app</span><span class="params">()</span>:</span></span><br><span class="line">    app = Flask(__name__)</span><br><span class="line">    app.config.from_object(<span class="string">'app.setting'</span>)</span><br><span class="line"></span><br><span class="line">    register_blueprint(app)</span><br><span class="line">    <span class="comment"># db.init_app(app)</span></span><br><span class="line">    <span class="comment"># db.create_app(app=app)</span></span><br><span class="line">    <span class="keyword">return</span> app</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">register_blueprint</span><span class="params">(app)</span>:</span></span><br><span class="line">    <span class="keyword">from</span> app.web.view <span class="keyword">import</span> web</span><br><span class="line">    app.register_blueprint(web)</span><br></pre></td></tr></table></figure></li></ol></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;使用工具&quot;&gt;&lt;a href=&quot;#使用工具&quot; class=&quot;headerlink&quot; title=&quot;使用工具&quot;&gt;&lt;/a&gt;&lt;strong&gt;使用工具&lt;/strong&gt;&lt;/h2&gt;&lt;h4 id=&quot;Flask后端-Postgresql数据库-JS前端（我未使用）&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="Python后端" scheme="http://yoursite.com/categories/Python%E5%90%8E%E7%AB%AF/"/>
    
    
      <category term="flask框架" scheme="http://yoursite.com/tags/flask%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>自省</title>
    <link href="http://yoursite.com/2018/10/06/%E8%87%AA%E7%9C%81/"/>
    <id>http://yoursite.com/2018/10/06/自省/</id>
    <published>2018-10-06T03:06:43.000Z</published>
    <updated>2018-10-29T11:44:55.364Z</updated>
    
    <content type="html"><![CDATA[<script src=/js/crypto-js.js></script><script>function doDecrypt (pwd, onError) {console.log('in doDecrypt');const txt = document.getElementById('enc_content').innerHTML;let plantext;try {const bytes = CryptoJS.AES.decrypt(txt, pwd);var plaintext = bytes.toString(CryptoJS.enc.Utf8);} catch(err) {if(onError) {onError(err);}return;}document.getElementById('enc_content').innerHTML = plaintext;document.getElementById('enc_content').style.display = 'block';document.getElementById('enc_passwd').style.display = 'none';if(typeof MathJax !== 'undefined') {MathJax.Hub.Queue(['resetEquationNumbers', MathJax.InputJax.TeX],['PreProcess', MathJax.Hub],['Reprocess', MathJax.Hub]);}}</script><div id="enc_content" style="display:none">U2FsdGVkX1+0/zCbRLXikCMgIy/YXijiDRcQZEwLM8aOcN4dfYxp/6y2k/e6Dzj06FDyGd2L/cLPjNcM/i3lRpPIQpE19UMNyip2F/4nI+Oy4NnmJ8b87PRoGU+OdPiR/uMI4VvIb/0IsNsCCY97TboiDF4tClSFjzih1jRASPj8Iwo3WHICkvi/oGaFLCv0I6ottVF6fe0K/onRrk54XlhPEpxrkFzttrVjixJJWTI4L3jGoNdYmnbRco5mqPPw3++xLCluShyODSHe3SxGhdrVjWpQVa4GreiF3fXQy8toSaa/BBq6LSDMSrPTYcuGcaH3vJ0qVZ5hO9rpeMoTceYSJXm7XKLd+Qvs1lC+6bLqbyGx+rqWi06441LMLPZk5uHspWM1xgWkv4f+T6B3BNEaOlKZ/l2t1APhGDydCfKRO/CDxsIz0jTMbrMX33XXI4LReMJZurCmQSXFhOV9n5yeRhxTdmBk7WYEKqtN4Az6OkLbPp1tB8dLf0nFOlPx</div><div id="enc_passwd"> <input id="enc_pwd_input" type="password" style="border-radius: 5px;border-style: groove;height: 30px;width: 50%;cursor: auto;font-size: 102%;color: currentColor;outline: none;text-overflow: initial;padding-left: 5px;" onkeydown="if (event.keyCode == 13) { decrypt(); return false;}"> <input type="submit" value="解&nbsp;密" onclick="decrypt()" style="width: 58px;height: 34px;border-radius: 5px;background-color: white;border-style: solid;color: currentColor;"><div id="enc_error" style="display: inline-block;color: #d84527;margin-left: 10px"></div><script>var onError = function(error) {document.getElementById("enc_error").innerHTML = "password error!"};function decrypt() {var passwd = document.getElementById("enc_pwd_input").value;console.log(passwd);doDecrypt(passwd, onError);}</script></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=/js/crypto-js.js&gt;&lt;/script&gt;
&lt;script&gt;
function doDecrypt (pwd, onError) {
	console.log(&#39;in doDecrypt&#39;);
	const txt = document.getE
      
    
    </summary>
    
      <category term="感悟" scheme="http://yoursite.com/categories/%E6%84%9F%E6%82%9F/"/>
    
    
      <category term="自我反省" scheme="http://yoursite.com/tags/%E8%87%AA%E6%88%91%E5%8F%8D%E7%9C%81/"/>
    
  </entry>
  
  <entry>
    <title>PostgreSQL学习记录</title>
    <link href="http://yoursite.com/2018/09/26/PostgreSQL%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2018/09/26/PostgreSQL学习记录/</id>
    <published>2018-09-26T06:26:56.000Z</published>
    <updated>2018-10-17T09:54:05.116Z</updated>
    
    <content type="html"><![CDATA[<p>select<br>fetchall()<br>fetchmany()<br>fetchone()</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;select&lt;br&gt;fetchall()&lt;br&gt;fetchmany()&lt;br&gt;fetchone()&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>docker基础</title>
    <link href="http://yoursite.com/2018/09/03/docker%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/09/03/docker基础/</id>
    <published>2018-09-03T02:44:54.000Z</published>
    <updated>2018-10-17T09:51:48.925Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorflow入门整理</title>
    <link href="http://yoursite.com/2018/08/05/tensorflow%E5%85%A5%E9%97%A8%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/08/05/tensorflow入门整理/</id>
    <published>2018-08-05T10:26:06.000Z</published>
    <updated>2018-08-07T10:08:38.972Z</updated>
    
    <content type="html"><![CDATA[<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><blockquote><p>tf.placeholder()</p></blockquote><p>是tensorflow的一种特殊变量，这种变量并非在初始化时定义好内容，而是在训练的时候才将数据填入其中。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;tf.placeholder()&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;是tensorflow
      
    
    </summary>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
</feed>
