<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZOUZHEN_BLOG</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-03-28T03:29:07.274Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ZOUZHEN</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ssh传输文件</title>
    <link href="http://yoursite.com/2019/03/28/ssh%E4%BC%A0%E8%BE%93%E6%96%87%E4%BB%B6/"/>
    <id>http://yoursite.com/2019/03/28/ssh传输文件/</id>
    <published>2019-03-28T03:27:27.000Z</published>
    <updated>2019-03-28T03:29:07.274Z</updated>
    
    <content type="html"><![CDATA[<h2 id="利用ssh传输文件"><a href="#利用ssh传输文件" class="headerlink" title="利用ssh传输文件"></a>利用ssh传输文件</h2><p>在linux下一般用scp这个命令来通过ssh传输文件。</p><h3 id="1、从服务器上下载文件"><a href="#1、从服务器上下载文件" class="headerlink" title="1、从服务器上下载文件"></a>1、从服务器上下载文件</h3><p>scp username@servername:/path/filename /var/www/local_dir（本地目录）</p><p> 例如scp <a href="mailto:root@192.168.0.101" target="_blank" rel="noopener">root@192.168.0.101</a>:/var/www/test.txt  把192.168.0.101上的/var/www/test.txt 的文件下载到/var/www/local_dir（本地目录）</p><h3 id="2、上传本地文件到服务器"><a href="#2、上传本地文件到服务器" class="headerlink" title="2、上传本地文件到服务器"></a>2、上传本地文件到服务器</h3><p>scp /path/filename username@servername:/path   </p><p>例如scp /var/www/test.php  <a href="mailto:root@192.168.0.101" target="_blank" rel="noopener">root@192.168.0.101</a>:/var/www/  把本机/var/www/目录下的test.php文件上传到192.168.0.101这台服务器上的/var/www/目录中</p><h3 id="3、从服务器下载整个目录"><a href="#3、从服务器下载整个目录" class="headerlink" title="3、从服务器下载整个目录"></a>3、从服务器下载整个目录</h3><p>scp -r username@servername:/var/www/remote_dir/（远程目录） /var/www/local_dir（本地目录）</p><p>例如:scp -r <a href="mailto:root@192.168.0.101" target="_blank" rel="noopener">root@192.168.0.101</a>:/var/www/test  /var/www/  </p><h3 id="4、上传目录到服务器"><a href="#4、上传目录到服务器" class="headerlink" title="4、上传目录到服务器"></a>4、上传目录到服务器</h3><p>scp  -r local_dir username@servername:remote_dir<br>例如：scp -r test  <a href="mailto:root@192.168.0.101" target="_blank" rel="noopener">root@192.168.0.101</a>:/var/www/   把当前目录下的test目录上传到服务器的/var/www/ 目录</p><p>注：目标服务器要开启写入权限。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;利用ssh传输文件&quot;&gt;&lt;a href=&quot;#利用ssh传输文件&quot; class=&quot;headerlink&quot; title=&quot;利用ssh传输文件&quot;&gt;&lt;/a&gt;利用ssh传输文件&lt;/h2&gt;&lt;p&gt;在linux下一般用scp这个命令来通过ssh传输文件。&lt;/p&gt;
&lt;h3 id=&quot;1
      
    
    </summary>
    
      <category term="远程操作" scheme="http://yoursite.com/categories/%E8%BF%9C%E7%A8%8B%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="ssh" scheme="http://yoursite.com/tags/ssh/"/>
    
  </entry>
  
  <entry>
    <title>深度学习框架简介</title>
    <link href="http://yoursite.com/2019/03/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E7%AE%80%E4%BB%8B/"/>
    <id>http://yoursite.com/2019/03/18/深度学习框架简介/</id>
    <published>2019-03-18T10:02:24.000Z</published>
    <updated>2019-03-28T03:30:28.991Z</updated>
    
    <content type="html"><![CDATA[<h2 id="theano"><a href="#theano" class="headerlink" title="theano"></a>theano</h2><p>theano最初于2008年开始开发，是第一个有较大影响力的框架。<br>theano是一个Python库，可用于定义、优化和计算数学表达式，特别是多维数组（numpy.ndarray）<br>theano诞生于研究机构，服务于研究人员，其设计具有较浓厚的学术气息，但在工业设计上有较大的缺陷。</p><h2 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h2><p>Tendorflow是Google于2015年11月10日推出的全新的机器学习开源框架。Tensorflow在很大程度上可以看作是Theano的后继者，都是基于计算图实现自动微分系统。Tensoflow支持多种语言，Java、Go、R和Haskell的alpha版本也被支持。<br>但是Tensorflow也存在一些问题：</p><ul><li>过于复杂的系统设计</li><li>频繁变动的接口</li><li>接口设计过于晦涩难懂</li><li>文档混乱脱节</li></ul><p>直接使用Tensorflow的生产力很低下，因而出现了Keras等第三方封装库（PS：我不认为Keras等第三方封装库可以被称之为深度学习框架）<br>虽然不完美，但是最流行的深度学习框架，社区强大，适合生产环境</p><h2 id="Caffe-Caffe2"><a href="#Caffe-Caffe2" class="headerlink" title="Caffe/Caffe2"></a>Caffe/Caffe2</h2><p>caffe是一个清晰、高效的深度学习框架，可信语言是C++，既可以在CPU上运行，也可以在GPU上运行。其优点为简洁快速、缺点是缺少灵活性。Caffe灵活性的缺失主要是因为它的设计。caffe的作者在FAIR担任主管的时候，开发了Caffe2，Caffe2的设计追求轻量级，在保有扩展性和高性能的同时，Caffe2也强调了便携性。<br>文档不够完善，但性能优异，几乎全平台支持Caffe2，适合生产环境。</p><h2 id="MXNet"><a href="#MXNet" class="headerlink" title="MXNet"></a>MXNet</h2><p>MXNet是一个深度学习库，支持C++、Python、JavaScript等语言；支持命令和符号编程；可以运行在CPU、GPU、集群、服务器、台式机或者移动设备上。<br>MXNet以其超强的分布式支持，明显的内存、显存优化为人所称道。<br>文档略混乱、但分布式性能强大、语言支持最多、适合AWS平台开发。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;theano&quot;&gt;&lt;a href=&quot;#theano&quot; class=&quot;headerlink&quot; title=&quot;theano&quot;&gt;&lt;/a&gt;theano&lt;/h2&gt;&lt;p&gt;theano最初于2008年开始开发，是第一个有较大影响力的框架。&lt;br&gt;theano是一个Python库，
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>框架梳理——RetinaNet/keras</title>
    <link href="http://yoursite.com/2019/01/17/%E6%A1%86%E6%9E%B6%E6%A2%B3%E7%90%86%E2%80%94%E2%80%94RetinaNet-keras/"/>
    <id>http://yoursite.com/2019/01/17/框架梳理——RetinaNet-keras/</id>
    <published>2019-01-17T06:28:36.000Z</published>
    <updated>2019-03-28T03:30:28.985Z</updated>
    
    <content type="html"><![CDATA[<h1 id="框架梳理——RetinaNet-keras"><a href="#框架梳理——RetinaNet-keras" class="headerlink" title="框架梳理——RetinaNet/keras"></a>框架梳理——RetinaNet/keras</h1><p>代码地址：<a href="https://travis-ci.org/fizyr/keras-retinanet" target="_blank" rel="noopener">https://travis-ci.org/fizyr/keras-retinanet</a></p><h2 id="文件目录"><a href="#文件目录" class="headerlink" title="文件目录"></a>文件目录</h2><ul><li><p>keras-retinanet  </p><p>  │<br>  ├── examples(例图和测试Notebook)<br>  │<br>  ├── images(测试图片)<br>  │<br>  ├── keras_retinanet(主要的框架代码)<br>  │<br>  ├── snapshots(模型保存地址)<br>  │<br>  └── tests(测试文件夹，包含对模型的各种测试)</p></li><li><p>keras_retinanet(主要的框架代码)<br>  │<br>  ├── backend(tf或者th相关的设置)<br>  │<br>  ├── bin(包含模型转换、调试、评价、训练等文件)<br>  │<br>  ├── callbacks(与召回相关的函数)<br>  │<br>  ├── layers(基础网络层之上的采样层处理，过滤预选框)<br>  │<br>  ├── callbacks(与召回相关的函数)<br>  │<br>  ├── models(基础网络文件夹，包括densenet、mobilenet、resnet、retinanet、vgg等网络)<br>  │<br>  ├── preprocessing(数据集相关，包括各种数据集的获取、解析等)<br>  │<br>  ├── utils(附属工具，包括锚点框、非最大值抑制等)<br>  │<br>  └── losses.py/initializers.py(损失和初始化等)</p></li></ul><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>关于模型的训练文件，在bin文件夹下，其训练文件的流程大致如下</p><p><img src="/2019/01/17/框架梳理——RetinaNet-keras/keras-retinanet解析.jpg" alt="图片"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;框架梳理——RetinaNet-keras&quot;&gt;&lt;a href=&quot;#框架梳理——RetinaNet-keras&quot; class=&quot;headerlink&quot; title=&quot;框架梳理——RetinaNet/keras&quot;&gt;&lt;/a&gt;框架梳理——RetinaNet/keras&lt;/
      
    
    </summary>
    
      <category term="框架实现" scheme="http://yoursite.com/categories/%E6%A1%86%E6%9E%B6%E5%AE%9E%E7%8E%B0/"/>
    
    
      <category term="RetinaNet" scheme="http://yoursite.com/tags/RetinaNet/"/>
    
      <category term="keras" scheme="http://yoursite.com/tags/keras/"/>
    
  </entry>
  
  <entry>
    <title>keras-RetinaNet问题记录</title>
    <link href="http://yoursite.com/2019/01/16/keras-RetinaNet%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2019/01/16/keras-RetinaNet问题记录/</id>
    <published>2019-01-16T07:39:30.000Z</published>
    <updated>2019-03-28T03:30:28.971Z</updated>
    
    <content type="html"><![CDATA[<h2 id="框架搭建"><a href="#框架搭建" class="headerlink" title="框架搭建"></a>框架搭建</h2><h3 id="获取代码"><a href="#获取代码" class="headerlink" title="获取代码"></a>获取代码</h3><ul><li><p>克隆代码库。  </p><p>  git clone <a href="https://github.com/fizyr/keras-retinanet.git" target="_blank" rel="noopener">https://github.com/fizyr/keras-retinanet.git</a></p></li></ul><h3 id="编译支持"><a href="#编译支持" class="headerlink" title="编译支持"></a>编译支持</h3><p>编译Cython代码</p><ul><li>python setup.py build_ext –inplace</li></ul><h2 id="Retinanet训练Pascal-VOC-2007"><a href="#Retinanet训练Pascal-VOC-2007" class="headerlink" title="Retinanet训练Pascal VOC 2007"></a>Retinanet训练Pascal VOC 2007</h2><h3 id="train"><a href="#train" class="headerlink" title="train"></a>train</h3><pre><code># trainpython3 keras_retinanet/bin/train.py pascal /path/to/VOCdevkit/VOC2007# 使用 --backbone=xxx 选择网络结构，默认是resnet50# xxx可以是resnet模型（`resnet50`，`resnet101`，`resnet152`）# 或`mobilenet`模型（`mobilenet128_1.0`，`mobilenet128_0.75`，`mobilenet160_1.0`等）# 也可以使用models目录下的 resnet.py，mobilenet.py等来自定义网络</code></pre><h3 id="test"><a href="#test" class="headerlink" title="test"></a>test</h3><p>1 首先需要进行模型转换，将训练好的模型转换为测试所需模型，<br>keras-retinanet的训练程序与训练模型一起使用。 与测试模型相比，这些是精简版本，仅包含培训所需的层（回归和分类值）。 如果您希望对模型进行测试（对图像执行对象检测），则需要将训练模型转换为测试模型。</p><pre><code># Running directly from the repository:keras_retinanet/bin/convert_model.py /path/to/training/model.h5 /path/to/save/inference/model.h5# Using the installed script:retinanet-convert-model /path/to/training/model.h5 /path/to/save/inference/model.h5</code></pre><p>2 测试代码</p><pre><code># import kerasimport keras# import keras_retinanetfrom keras_retinanet import modelsfrom keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_imagefrom keras_retinanet.utils.visualization import draw_box, draw_captionfrom keras_retinanet.utils.colors import label_color# import miscellaneous modulesimport matplotlib.pyplot as pltimport cv2import osimport numpy as npimport time# set tf backend to allow memory to grow, instead of claiming everythingimport tensorflow as tfdef get_session():    config = tf.ConfigProto()    config.gpu_options.allow_growth = True    return tf.Session(config=config)# use this environment flag to change which GPU to use#os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;1&quot;# set the modified tf session as backend in keraskeras.backend.tensorflow_backend.set_session(get_session())# adjust this to point to your downloaded/trained model# models can be downloaded here: https://github.com/fizyr/keras-retinanet/releasesmodel_path = os.path.join(&apos;..&apos;, &apos;snapshots&apos;, &apos;resnet50_coco_best_v2.1.0.h5&apos;)# load retinanet modelmodel = models.load_model(model_path, backbone_name=&apos;resnet50&apos;)# if the model is not converted to an inference model, use the line below# see: https://github.com/fizyr/keras-retinanet#converting-a-training-model-to-inference-model#model = models.convert_model(model)#print(model.summary())# load label to names mapping for visualization purposeslabels_to_names = {0: &apos;person&apos;, 1: &apos;bicycle&apos;, 2: &apos;car&apos;, 3: &apos;motorcycle&apos;, 4: &apos;airplane&apos;, 5: &apos;bus&apos;, 6: &apos;train&apos;, 7: &apos;truck&apos;, 8: &apos;boat&apos;, 9: &apos;traffic light&apos;, 10: &apos;fire hydrant&apos;, 11: &apos;stop sign&apos;, 12: &apos;parking meter&apos;, 13: &apos;bench&apos;, 14: &apos;bird&apos;, 15: &apos;cat&apos;, 16: &apos;dog&apos;, 17: &apos;horse&apos;, 18: &apos;sheep&apos;, 19: &apos;cow&apos;, 20: &apos;elephant&apos;, 21: &apos;bear&apos;, 22: &apos;zebra&apos;, 23: &apos;giraffe&apos;, 24: &apos;backpack&apos;, 25: &apos;umbrella&apos;, 26: &apos;handbag&apos;, 27: &apos;tie&apos;, 28: &apos;suitcase&apos;, 29: &apos;frisbee&apos;, 30: &apos;skis&apos;, 31: &apos;snowboard&apos;, 32: &apos;sports ball&apos;, 33: &apos;kite&apos;, 34: &apos;baseball bat&apos;, 35: &apos;baseball glove&apos;, 36: &apos;skateboard&apos;, 37: &apos;surfboard&apos;, 38: &apos;tennis racket&apos;, 39: &apos;bottle&apos;, 40: &apos;wine glass&apos;, 41: &apos;cup&apos;, 42: &apos;fork&apos;, 43: &apos;knife&apos;, 44: &apos;spoon&apos;, 45: &apos;bowl&apos;, 46: &apos;banana&apos;, 47: &apos;apple&apos;, 48: &apos;sandwich&apos;, 49: &apos;orange&apos;, 50: &apos;broccoli&apos;, 51: &apos;carrot&apos;, 52: &apos;hot dog&apos;, 53: &apos;pizza&apos;, 54: &apos;donut&apos;, 55: &apos;cake&apos;, 56: &apos;chair&apos;, 57: &apos;couch&apos;, 58: &apos;potted plant&apos;, 59: &apos;bed&apos;, 60: &apos;dining table&apos;, 61: &apos;toilet&apos;, 62: &apos;tv&apos;, 63: &apos;laptop&apos;, 64: &apos;mouse&apos;, 65: &apos;remote&apos;, 66: &apos;keyboard&apos;, 67: &apos;cell phone&apos;, 68: &apos;microwave&apos;, 69: &apos;oven&apos;, 70: &apos;toaster&apos;, 71: &apos;sink&apos;, 72: &apos;refrigerator&apos;, 73: &apos;book&apos;, 74: &apos;clock&apos;, 75: &apos;vase&apos;, 76: &apos;scissors&apos;, 77: &apos;teddy bear&apos;, 78: &apos;hair drier&apos;, 79: &apos;toothbrush&apos;}# load imageimage = read_image_bgr(&apos;000000008021.jpg&apos;)# copy to draw ondraw = image.copy()draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)# preprocess image for networkimage = preprocess_image(image)image, scale = resize_image(image)# process imagestart = time.time()boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))print(&quot;processing time: &quot;, time.time() - start)# correct for image scaleboxes /= scale# visualize detectionsfor box, score, label in zip(boxes[0], scores[0], labels[0]):    # scores are sorted so we can break    if score &lt; 0.5:        break    color = label_color(label)    b = box.astype(int)    draw_box(draw, b, color=color)    caption = &quot;{} {:.3f}&quot;.format(labels_to_names[label], score)    draw_caption(draw, b, caption)plt.figure(figsize=(15, 15))plt.axis(&apos;off&apos;)plt.imshow(draw)plt.show()</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;框架搭建&quot;&gt;&lt;a href=&quot;#框架搭建&quot; class=&quot;headerlink&quot; title=&quot;框架搭建&quot;&gt;&lt;/a&gt;框架搭建&lt;/h2&gt;&lt;h3 id=&quot;获取代码&quot;&gt;&lt;a href=&quot;#获取代码&quot; class=&quot;headerlink&quot; title=&quot;获取代码&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="算法实现" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/"/>
    
    
      <category term="RetinaNet" scheme="http://yoursite.com/tags/RetinaNet/"/>
    
  </entry>
  
  <entry>
    <title>FPN——CNN特征提取</title>
    <link href="http://yoursite.com/2019/01/14/FPN%E2%80%94%E2%80%94CNN%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    <id>http://yoursite.com/2019/01/14/FPN——CNN特征提取/</id>
    <published>2019-01-14T09:22:16.000Z</published>
    <updated>2019-03-28T03:30:28.944Z</updated>
    
    <content type="html"><![CDATA[<p>转载自：<a href="https://www.jianshu.com/p/5a28ae9b365d" target="_blank" rel="noopener">https://www.jianshu.com/p/5a28ae9b365d</a> 如有侵权，请联系删除</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>FPN是一种利用常规CNN模型来高效提取图片中各维度特征的方法。在计算机视觉学科中，多维度的目标检测一直以来都是通过将缩小或扩大后的不同维度图片作为输入来生成出反映不同维度信息的特征组合。这种办法确实也能有效地表达出图片之上的各种维度特征，但却对硬件计算能力及内存大小有较高要求，因此只能在有限的领域内部使用。<br>FPN通过利用常规CNN模型内部从底至上各个层对同一scale图片不同维度的特征表达结构，提出了一种可有效在单一图片视图下生成对其的多维度特征表达的方法。它可以有效地赋能常规CNN模型，从而可以生成出表达能力更强的feature maps以供下一阶段计算机视觉任务像object detection/semantic segmentation等来使用。本质上说它是一种加强主干网络CNN特征表达的方法。</p><h2 id="Featurized-image-pyramid"><a href="#Featurized-image-pyramid" class="headerlink" title="Featurized image pyramid"></a>Featurized image pyramid</h2><p>下图中描述了四种不同的得到一张图片多维度特征组合的方法。  </p><p><img src="/2019/01/14/FPN——CNN特征提取/1.JPG" alt="图片"><br>上图(a)中的方法即为常规的生成一张图片的多维度特征组合的经典方法。即对某一输入图片我们通过压缩或放大从而形成不同维度的图片作为模型输入，使用同一模型对这些不同维度的图片分别处理后，最终再将这些分别得到的特征（feature maps）组合起来就得到了我们想要的可反映多维度信息的特征集。此种方法缺点在于需要对同一图片在更改维度后输入处理多次，因此对计算机的算力及内存大小都有较高要求。<br>图(b)中的方法则只拿单一维度的图片做为输入，然后经CNN模型处理后，拿最终一层的feature maps作为最终的特征集。显然此种方法只能得到单一维度的信息。优点是计算简单，对计算机算力及内存大小都无过高需求。此方法为大多数R-CNN系列目标检测方法所用像R-CNN/Fast-RCNN/Faster-RCNN等。因此最终这些模型对小维度的目标检测性能不是很好。<br>图(c)中的方法同样是拿单一维度的图片做为输入，不过最终选取用于接下来分类或检测任务时的特征组合时，此方法不只选用了最后一层的high level feature maps，同样也会选用稍靠下的反映图片low level 信息的feature maps。然后将这些不同层次（反映不同level的图片信息）的特征简单合并起来（一般为concat处理），用于最终的特征组合输出。此方法可见于SSD当中。不过SSD在选取层特征时都选用了较高层次的网络。比如在它以VGG16作为主干网络的检测模型里面所选用的最低的Convolution的层为Conv4，这样一些具有更低级别信息的层特征像Conv2/Conv3就被它给漏掉了，于是它对更小维度的目标检测效果就不大好。<br>图(d)中的方法同图(c)中的方法有些类似，也是拿单一维度的图片作为输入，然后它会选取所有层的特征来处理然后再联合起来做为最终的特征输出组合。（作者在论文中拿Resnet为实例时并没选用Conv1层，那是为了算力及内存上的考虑，毕竟Conv1层的size还是比较大的，所包含的特征跟直接的图片像素信息也过于接近）。另外还对这些反映不同级别图片信息的各层自上向下进行了再处理以能更好地组合从而形成较好的特征表达（详细过程会在下面章节中进一步介绍）。而此方法正是我们本文中要讲的FPN CNN特征提取方法。</p><h2 id="FPN基本架构"><a href="#FPN基本架构" class="headerlink" title="FPN基本架构"></a>FPN基本架构</h2><p>FPN会使用CNN网络中每一层的信息来生成最后的表达特征组合。下图是它的基本架构。从中我们能看到FPN会模型每个CNN层的特征输出进行处理以生成反映此维度信息的特征。而自上至下处理后所生成出的特征之间也有个关联关系，即上层high level的特征会影响下一层次的low level特征表达。最终所有的特征一起用来作为下一步的目标检测或类别分析等任务的输入。</p><p><img src="/2019/01/14/FPN——CNN特征提取/2.JPG" alt="图片"></p><h2 id="FPN详细介绍"><a href="#FPN详细介绍" class="headerlink" title="FPN详细介绍"></a>FPN详细介绍</h2><p>FPN是传统CNN网络对图片信息进行表达输出的一种增强。它目的是为了改进CNN网络的特征提取方式，从而可以使最终输出的特征更好地表示出输入图片各个维度的信息。它的基本过程有三个分别为：自下至上的通路即自下至上的不同维度特征生成；自上至下的通路即自上至下的特征补充增强；CNN网络层特征与最终输出的各维度特征之间的关联表达。<br>我们在下图中能看出这三个过程的细粒度表示。</p><p><img src="/2019/01/14/FPN——CNN特征提取/3.JPG" alt="图片"></p><ul><li><p>自下至上的通路（Bottom-top pathway）：这个没啥奇怪就是指的普通CNN特征自底至上逐层浓缩表达特征的一个过程。此过程很早即被认识到了即较底的层反映较浅层次的图片信息特征像边缘等；较高的层则反映较深层次的图片特征像物体轮廓、乃至类别等；</p></li><li><p>自上至下的通路（Top-bottome pathway）：上层的特征输出一般其feature map size比较小，但却能表示更大维度（同时也是更加high level）的图片信息。此类high level信息经实验证明能够对后续的目标检测、物体分类等任务发挥关键作用。因此我们在处理每一层信息时会参考上一层的high level信息做为其输入（这里只是在将上层feature map等比例放大后再与本层的feature maps做element wise相加）;</p></li><li><p>CNN层特征与每一级别输出之间的表达关联：在这里作者实验表明使用1x1的Conv即可生成较好的输出特征，它可有效地降低中间层次的channels 数目。最终这些1x1的Convs使得我们输出不同维度的各个feature maps有着相同的channels数目（本文用到的Resnet-101主干网络中，各个层次特征的最终输出channels数目为256）。</p></li></ul><h2 id="FPN在目标检测中的实际应用"><a href="#FPN在目标检测中的实际应用" class="headerlink" title="FPN在目标检测中的实际应用"></a>FPN在目标检测中的实际应用</h2><p>以下为一个FPN特征提取方法在RCNN目标检测框架中应用的例子。从中我们可以更加详细地了解到它的具体实现。</p><p><img src="/2019/01/14/FPN——CNN特征提取/4.JPG" alt="图片"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载自：&lt;a href=&quot;https://www.jianshu.com/p/5a28ae9b365d&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.jianshu.com/p/5a28ae9b365d&lt;/a&gt; 如有侵权，请联系删除
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="图像识别" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/"/>
    
      <category term="FPN" scheme="http://yoursite.com/tags/FPN/"/>
    
  </entry>
  
  <entry>
    <title>目标检测——RetinaNet</title>
    <link href="http://yoursite.com/2019/01/14/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94RetinaNet/"/>
    <id>http://yoursite.com/2019/01/14/目标检测——RetinaNet/</id>
    <published>2019-01-14T08:23:53.000Z</published>
    <updated>2019-03-28T03:30:28.993Z</updated>
    
    <content type="html"><![CDATA[<p>转载自：<a href="https://www.jianshu.com/p/8e501a159b28" target="_blank" rel="noopener">https://www.jianshu.com/p/8e501a159b28</a> 如有侵权，请联系删除</p><h1 id="RetinaNet-Focal-loss在目标检测网络中的应用"><a href="#RetinaNet-Focal-loss在目标检测网络中的应用" class="headerlink" title="RetinaNet: Focal loss在目标检测网络中的应用"></a>RetinaNet: Focal loss在目标检测网络中的应用</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>RetinaNet是2018年Facebook AI团队在目标检测领域新的贡献。它的重要作者名单中Ross Girshick与Kaiming He赫然在列。来自Microsoft的Sun Jian团队与现在Facebook的Ross/Kaiming团队在当前视觉目标分类、检测领域有着北乔峰、南慕容一般的独特地位。这两个实验室的文章多是行业里前进方向的提示牌。<br>RetinaNet只是原来FPN网络与FCN网络的组合应用，因此在目标网络检测框架上它并无特别亮眼创新。文章中最大的创新来自于Focal loss的提出及在单阶段目标检测网络RetinaNet（实质为Resnet + FPN + FCN）的成功应用。Focal loss是一种改进了的交叉熵(cross-entropy, CE)loss，它通过在原有的CE loss上乘了个使易检测目标对模型训练贡献削弱的指数式，从而使得Focal loss成功地解决了在目标检测时，正负样本区域极不平衡而目标检测loss易被大批量负样本所左右的问题。此问题是单阶段目标检测框架（如SSD/Yolo系列）与双阶段目标检测框架（如Faster-RCNN/R-FCN等）accuracy gap的最大原因。在Focal loss提出之前，已有的目标检测网络都是通过像Boot strapping/Hard example mining等方法来解决此问题的。作者通过后续实验成功表明Focal loss可在单阶段目标检测网络中成功使用，并最终能以更快的速率实现与双阶段目标检测网络近似或更优的效果。</p><h2 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h2><p>常规的单阶段目标检测网络像SSD一般在模型训练时会先大密度地在模型终端的系列feature maps上生成出10,000甚至100,0000个目标候选区域。然后再分别对这些候选区域进行分类与位置回归识别。而在这些生成的数万个候选区域中，绝大多数都是不包含待检测目标的图片背景，这样就造成了机器学习中经典的训练样本正负不平衡的问题。它往往会造成最终算出的training loss为占绝对多数但包含信息量却很少的负样本所支配，少样正样本提供的关键信息却不能在一般所用的training loss中发挥正常作用，从而无法得出一个能对模型训练提供正确指导的loss。<br>常用的解决此问题的方法就是负样本挖掘。或其它更复杂的用于过滤负样本从而使正负样本数维持一定比率的样本取样方法。而在此篇文章中作者提出了可通过候选区域包含潜在目标概率进而对最终的training loss进行较正的方法。实验表明这种新提出的focal loss在单阶段目标检测任务上表现突出，有效地解决了此领域里面潜在的类别不平衡问题。</p><h2 id="Focal-loss"><a href="#Focal-loss" class="headerlink" title="Focal loss"></a>Focal loss</h2><ul><li><p>CE(cross-entropy) loss<br>  以下为典型的交叉熵loss，它广泛用于当下的图像分类、检测CNN网络当中。</p><p>  <img src="/2019/01/14/目标检测——RetinaNet/1.JPG" alt="图片"></p></li><li><p>Balanced CE loss<br>  考虑到上节中提到的类别不平衡问题对最终training loss的不利影响，我们自然会想到可通过在loss公式中使用与目标存在概率成反比的系数对其进行较正。如下公式即是此朴素想法的体现。它也是作者最终Focus loss的baseline。</p><p>  <img src="/2019/01/14/目标检测——RetinaNet/2.JPG" alt="图片"></p></li><li><p>Focal loss定义<br>  以下是作者提出的focal loss的想法。</p><p>  <img src="/2019/01/14/目标检测——RetinaNet/3.JPG" alt="图片"><br>  下图为focal loss与常规CE loss的对比。从中，我们易看出focal loss所加的指数式系数可对正负样本对loss的贡献自动调节。当某样本类别比较明确些，它对整体loss的贡献就比较少；而若某样本类别不易区分，则对整体loss的贡献就相对偏大。这样得到的loss最终将集中精力去诱导模型去努力分辨那些难分的目标类别，于是就有效提升了整体的目标检测准度。不过在此focus loss计算当中，我们引入了一个新的hyper parameter即γ。一般来说新参数的引入，往往会伴随着模型使用难度的增加。在本文中，作者有试者对其进行调节，线性搜索后得出将γ设为2时，模型检测效果最好。</p><p>  <img src="/2019/01/14/目标检测——RetinaNet/4.JPG" alt="图片"></p><p>  在最终所用的focal loss上，作者还引入了α系数，它能够使得focal loss对不同类别更加平衡。实验表明它会比原始的focal loss效果更好。</p><p>  <img src="/2019/01/14/目标检测——RetinaNet/5.JPG" alt="图片"></p></li></ul><h2 id="模型的初始化参数选择"><a href="#模型的初始化参数选择" class="headerlink" title="模型的初始化参数选择"></a>模型的初始化参数选择</h2><p>一般我们初始化CNN网络模型时都会使用无偏的参数对其初始化，比如Conv的kernel 参数我们会以bias 为0，variance为0.01的某分布来对其初始化。但是如果我们的模型要去处理类别极度不平衡的情况，那么就会考虑到这样对训练数据分布无任选先验假设的初始化会使得在训练过程中，我们的参数更偏向于拥有更多数量的负样本的情况去进化。作者观察下来发现它在训练时会出现极度的不稳定。于是作者在初始化模型最后一层参数时考虑了数据样本分布的不平衡性，这样使得初始训练时最终得出的loss不会对过多的负样本数量所惊讶到，从而有效地规避了初始训练时模型的震荡与不稳定</p><h2 id="RetinaNet检测框架"><a href="#RetinaNet检测框架" class="headerlink" title="RetinaNet检测框架"></a>RetinaNet检测框架</h2><p>RetinaNet本质上是Resnet + FPN + 两个FCN子网络。<br>以下为RetinaNet目标框架框架图。有了之前blog里面提到的FPN与FCN的知识后，我们很容易理解此框架的设计含义。</p><p><img src="/2019/01/14/目标检测——RetinaNet/6.JPG" alt="图片"></p><p>一般主干网络可选用任一有效的特征提取网络如vgg16或resnet系列，此处作者分别尝试了resnet-50与resnet-101。而FPN则是对resnet-50里面自动形成的多尺度特征进行了强化利用，从而得到了表达力更强、包含多尺度目标区域信息的feature maps集合。最后在FPN所吐出的feature maps集合上，分别使用了两个FCN子网络（它们有着相同的网络结构却各自独立，并不share参数）用来完成目标框类别分类与位置回归任务。</p><h2 id="模型的推理与训练"><a href="#模型的推理与训练" class="headerlink" title="模型的推理与训练"></a>模型的推理与训练</h2><ul><li><p>模型推理 </p><p>  一旦我们有了训练好的模型，在正式部署时，只需对其作一次forward，然后对最终生成的目标区域进行过渡。然后只对每个FPN level上目标存在概率最高的前1000个目标框进一步地decoding处理。接下来再将所有FPN level上得到的目标框汇集起来，统一使用极大值抑制的方法进一步过渡（其中极大值抑制时所用的阈值为0.5）。这样，我们就得到了最终的目标与其位置框架。</p></li><li><p>模型训练</p><p>  模型训练中主要在后端Loss计算时采用了Focal loss，另外也在模型初始化时考虑到了正负样本极度不平衡的情况进而对模型最后一个conv layer的bias参数作了有偏初始化。 </p><p>  训练时用了SGD，mini batch size为16，在8个GPU上一块训练，每个GPU上local batch size为2。最大iterations数目为90,000；模型初始lr为0.01,接下来随着训练进行分step wisely 降低。真正的training loss则为表达目标类别的focus loss与表达目标框位置回归信息的L1 loss的和。  </p><p>  下图为RetinaNet模型的检测准度与性能<br>  <img src="/2019/01/14/目标检测——RetinaNet/7.JPG" alt="图片"></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载自：&lt;a href=&quot;https://www.jianshu.com/p/8e501a159b28&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.jianshu.com/p/8e501a159b28&lt;/a&gt; 如有侵权，请联系删除
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="RetinaNet" scheme="http://yoursite.com/tags/RetinaNet/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>项目整理</title>
    <link href="http://yoursite.com/2019/01/14/%E9%A1%B9%E7%9B%AE%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2019/01/14/项目整理/</id>
    <published>2019-01-14T01:25:39.000Z</published>
    <updated>2019-03-28T03:30:29.015Z</updated>
    
    <content type="html"><![CDATA[<h1 id="渝黔高速——隧道"><a href="#渝黔高速——隧道" class="headerlink" title="渝黔高速——隧道"></a>渝黔高速——隧道</h1><p>开发工具：JavaScript + Git + Docker + Postgresql<br>IDE：Vscode</p><p>表结构设计：</p><ul><li>隧道表</li><li>衬砌表</li><li>工法分区表</li><li>衬砌工程量表</li><li>分区工程量表</li><li>模型表</li><li>进度表</li><li>施工劳务记录表</li><li>劳务合同清单表</li></ul><h2 id="业务分析"><a href="#业务分析" class="headerlink" title="业务分析"></a>业务分析</h2><h3 id="模型的创建"><a href="#模型的创建" class="headerlink" title="模型的创建"></a>模型的创建</h3><ul><li>隧道的创建以及获取信息</li><li>衬砌的创建以及信息获取</li><li>工法分区的创建以及信息获取</li><li>衬砌工程量的录入</li><li>隧道段的创建、查询、修改</li><li>在段下面创建模型（根据衬砌类型和分区批量创建）</li><li>分区工程量参数设置（根据已经创建的模型，对每一个分区进行分区工程量参数设置）、修改</li><li>分区工程量参数</li></ul><h3 id="进度"><a href="#进度" class="headerlink" title="进度"></a>进度</h3><ul><li>根据模型获取相关的工程量，起止桩号</li><li>创建进尺</li><li>修改完成状态</li><li>查看已完成进尺</li></ul><h3 id="工程量"><a href="#工程量" class="headerlink" title="工程量"></a>工程量</h3><ul><li>统计当前施工完成的工程量</li><li>根据时间筛选当前完成的工程量</li></ul><h3 id="产值"><a href="#产值" class="headerlink" title="产值"></a>产值</h3><ul><li>根据工程量计算得到当前施工的总产值以及单个隧道产值</li></ul><h3 id="分包"><a href="#分包" class="headerlink" title="分包"></a>分包</h3><ul><li>合同工程量清单的创建、获取</li><li>劳务成本的计算</li></ul><h3 id="业务逻辑"><a href="#业务逻辑" class="headerlink" title="业务逻辑"></a>业务逻辑</h3><p><img src="/2019/01/14/项目整理/1.png" alt="图片"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;渝黔高速——隧道&quot;&gt;&lt;a href=&quot;#渝黔高速——隧道&quot; class=&quot;headerlink&quot; title=&quot;渝黔高速——隧道&quot;&gt;&lt;/a&gt;渝黔高速——隧道&lt;/h1&gt;&lt;p&gt;开发工具：JavaScript + Git + Docker + Postgresql&lt;br
      
    
    </summary>
    
      <category term="笔记" scheme="http://yoursite.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="项目" scheme="http://yoursite.com/tags/%E9%A1%B9%E7%9B%AE/"/>
    
      <category term="隧道" scheme="http://yoursite.com/tags/%E9%9A%A7%E9%81%93/"/>
    
  </entry>
  
  <entry>
    <title>基于Pytorch的特征图提取</title>
    <link href="http://yoursite.com/2018/11/13/%E5%9F%BA%E4%BA%8EPytorch%E7%9A%84%E7%89%B9%E5%BE%81%E5%9B%BE%E6%8F%90%E5%8F%96/"/>
    <id>http://yoursite.com/2018/11/13/基于Pytorch的特征图提取/</id>
    <published>2018-11-13T02:49:13.000Z</published>
    <updated>2018-11-13T03:19:20.839Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h3><p>为了方便理解卷积神经网络的运行过程，需要对卷积神经网络的运行结果进行可视化的展示。  </p><p>大致可分为如下步骤：</p><ul><li>单个图片的提取</li><li>神经网络的构建</li><li>特征图的提取</li><li>可视化展示</li></ul><h4 id="单个图片的提取"><a href="#单个图片的提取" class="headerlink" title="单个图片的提取"></a>单个图片的提取</h4><p>根据目标要求，需要对单个图片进行卷积运算，但是Pytorch中读取数据主要用到torch.utils.data.DataLoader类，因此我们需要编写单个图片的读取程序</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_picture</span><span class="params">(picture_dir, transform)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    该算法实现了读取图片，并将其类型转化为Tensor</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    tmp = []</span><br><span class="line">    img = skimage.io.imread(picture_dir)</span><br><span class="line">    tmp.append(img)</span><br><span class="line">    img = skimage.io.imread(<span class="string">'./picture/4.jpg'</span>)</span><br><span class="line">    tmp.append(img)</span><br><span class="line">    img256 = [skimage.transform.resize(img, (<span class="number">256</span>, <span class="number">256</span>)) <span class="keyword">for</span> img <span class="keyword">in</span> tmp]</span><br><span class="line">    img256 = np.asarray(img256)</span><br><span class="line">    img256 = img256.astype(np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transform(img256[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p><strong>注意：</strong> 神经网络的输入是四维形式，我们返回的图片是三维形式，需要使用unsqueeze()插入一个维度</p><h4 id="神经网络的构建"><a href="#神经网络的构建" class="headerlink" title="神经网络的构建"></a>神经网络的构建</h4><p>网络的基于LeNet构建，不过为了方便展示，将其中的参数按照256<em>256</em>3进行的参数的修正  </p><p>网络构建如下：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    该类继承了torch.nn.Modul类</span></span><br><span class="line"><span class="string">    构建LeNet神经网络模型</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LeNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第一层神经网络，包括卷积层、线性激活函数、池化层</span></span><br><span class="line">        self.conv1 = nn.Sequential( </span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),   <span class="comment"># input_size=(3*256*256)，padding=2</span></span><br><span class="line">            nn.ReLU(),                  <span class="comment"># input_size=(32*256*256)</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),  <span class="comment"># output_size=(32*128*128)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二层神经网络，包括卷积层、线性激活函数、池化层</span></span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),  <span class="comment"># input_size=(32*128*128)</span></span><br><span class="line">            nn.ReLU(),            <span class="comment"># input_size=(64*128*128)</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)    <span class="comment"># output_size=(64*64*64)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 全连接层(将神经网络的神经元的多维输出转化为一维)</span></span><br><span class="line">        self.fc1 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">64</span> * <span class="number">64</span> * <span class="number">64</span>, <span class="number">128</span>),  <span class="comment"># 进行线性变换</span></span><br><span class="line">            nn.ReLU()                    <span class="comment"># 进行ReLu激活</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出层(将全连接层的一维输出进行处理)</span></span><br><span class="line">        self.fc2 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">84</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将输出层的数据进行分类(输出预测值)</span></span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">62</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向传播过程，输入为x</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="comment"># nn.Linear()的输入输出都是维度为一的值，所以要把多维度的tensor展平成一维</span></span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p><h4 id="特征图的提取"><a href="#特征图的提取" class="headerlink" title="特征图的提取"></a>特征图的提取</h4><p>直接上代码：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureExtractor</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, submodule, extracted_layers)</span>:</span></span><br><span class="line">        super(FeatureExtractor, self).__init__()</span><br><span class="line">        self.submodule = submodule</span><br><span class="line">        self.extracted_layers = extracted_layers</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        outputs = []</span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.submodule._modules.items():</span><br><span class="line">        <span class="comment"># 目前不展示全连接层</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">"fc"</span> <span class="keyword">in</span> name: </span><br><span class="line">                x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">            print(module)</span><br><span class="line">            x = module(x)</span><br><span class="line">            print(name)</span><br><span class="line">            <span class="keyword">if</span> name <span class="keyword">in</span> self.extracted_layers:</span><br><span class="line">                outputs.append(x)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure></p><h4 id="可视化展示"><a href="#可视化展示" class="headerlink" title="可视化展示"></a>可视化展示</h4><p>可视化展示使用matplotlib</p><p>代码如下：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征输出可视化</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">    ax = plt.subplot(<span class="number">6</span>, <span class="number">6</span>, i + <span class="number">1</span>)</span><br><span class="line">    ax.set_title(<span class="string">'Feature &#123;&#125;'</span>.format(i))</span><br><span class="line">    ax.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.imshow(x[<span class="number">0</span>].data.numpy()[<span class="number">0</span>,i,:,:],cmap=<span class="string">'jet'</span>)</span><br><span class="line">plt.plot()</span><br></pre></td></tr></table></figure></p><h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><p>在此贴上完整代码</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="keyword">as</span> tv</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> skimage.data</span><br><span class="line"><span class="keyword">import</span> skimage.io</span><br><span class="line"><span class="keyword">import</span> skimage.transform</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义是否使用GPU</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load training and testing datasets.</span></span><br><span class="line">pic_dir = <span class="string">'./picture/3.jpg'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据预处理方式(将输入的类似numpy中arrary形式的数据转化为pytorch中的张量（tensor）)</span></span><br><span class="line">transform = transforms.ToTensor()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_picture</span><span class="params">(picture_dir, transform)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    该算法实现了读取图片，并将其类型转化为Tensor</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    img = skimage.io.imread(picture_dir)</span><br><span class="line">    img256 = skimage.transform.resize(img, (<span class="number">256</span>, <span class="number">256</span>))</span><br><span class="line">    img256 = np.asarray(img256)</span><br><span class="line">    img256 = img256.astype(np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transform(img256)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_picture_rgb</span><span class="params">(picture_dir)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    该函数实现了显示图片的RGB三通道颜色</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    img = skimage.io.imread(picture_dir)</span><br><span class="line">    img256 = skimage.transform.resize(img, (<span class="number">256</span>, <span class="number">256</span>))</span><br><span class="line">    skimage.io.imsave(<span class="string">'./picture/4.jpg'</span>,img256)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 取单一通道值显示</span></span><br><span class="line">    <span class="comment"># for i in range(3):</span></span><br><span class="line">    <span class="comment">#     img = img256[:,:,i]</span></span><br><span class="line">    <span class="comment">#     ax = plt.subplot(1, 3, i + 1)</span></span><br><span class="line">    <span class="comment">#     ax.set_title('Feature &#123;&#125;'.format(i))</span></span><br><span class="line">    <span class="comment">#     ax.axis('off')</span></span><br><span class="line">    <span class="comment">#     plt.imshow(img)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># r = img256.copy()</span></span><br><span class="line">    <span class="comment"># r[:,:,0:2]=0</span></span><br><span class="line">    <span class="comment"># ax = plt.subplot(1, 4, 1)</span></span><br><span class="line">    <span class="comment"># ax.set_title('B Channel')</span></span><br><span class="line">    <span class="comment"># # ax.axis('off')</span></span><br><span class="line">    <span class="comment"># plt.imshow(r)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># g = img256.copy()</span></span><br><span class="line">    <span class="comment"># g[:,:,0]=0</span></span><br><span class="line">    <span class="comment"># g[:,:,2]=0</span></span><br><span class="line">    <span class="comment"># ax = plt.subplot(1, 4, 2)</span></span><br><span class="line">    <span class="comment"># ax.set_title('G Channel')</span></span><br><span class="line">    <span class="comment"># # ax.axis('off')</span></span><br><span class="line">    <span class="comment"># plt.imshow(g)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># b = img256.copy()</span></span><br><span class="line">    <span class="comment"># b[:,:,1:3]=0</span></span><br><span class="line">    <span class="comment"># ax = plt.subplot(1, 4, 3)</span></span><br><span class="line">    <span class="comment"># ax.set_title('R Channel')</span></span><br><span class="line">    <span class="comment"># # ax.axis('off')</span></span><br><span class="line">    <span class="comment"># plt.imshow(b)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># img = img256.copy()</span></span><br><span class="line">    <span class="comment"># ax = plt.subplot(1, 4, 4)</span></span><br><span class="line">    <span class="comment"># ax.set_title('image')</span></span><br><span class="line">    <span class="comment"># # ax.axis('off')</span></span><br><span class="line">    <span class="comment"># plt.imshow(img)</span></span><br><span class="line"></span><br><span class="line">    img = img256.copy()</span><br><span class="line">    ax = plt.subplot()</span><br><span class="line">    ax.set_title(<span class="string">'image'</span>)</span><br><span class="line">    <span class="comment"># ax.axis('off')</span></span><br><span class="line">    plt.imshow(img)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    该类继承了torch.nn.Modul类</span></span><br><span class="line"><span class="string">    构建LeNet神经网络模型</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LeNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第一层神经网络，包括卷积层、线性激活函数、池化层</span></span><br><span class="line">        self.conv1 = nn.Sequential( </span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),   <span class="comment"># input_size=(3*256*256)，padding=2</span></span><br><span class="line">            nn.ReLU(),                  <span class="comment"># input_size=(32*256*256)</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),  <span class="comment"># output_size=(32*128*128)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二层神经网络，包括卷积层、线性激活函数、池化层</span></span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),  <span class="comment"># input_size=(32*128*128)</span></span><br><span class="line">            nn.ReLU(),            <span class="comment"># input_size=(64*128*128)</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)    <span class="comment"># output_size=(64*64*64)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 全连接层(将神经网络的神经元的多维输出转化为一维)</span></span><br><span class="line">        self.fc1 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">64</span> * <span class="number">64</span> * <span class="number">64</span>, <span class="number">128</span>),  <span class="comment"># 进行线性变换</span></span><br><span class="line">            nn.ReLU()                    <span class="comment"># 进行ReLu激活</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出层(将全连接层的一维输出进行处理)</span></span><br><span class="line">        self.fc2 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">84</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将输出层的数据进行分类(输出预测值)</span></span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">62</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向传播过程，输入为x</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="comment"># nn.Linear()的输入输出都是维度为一的值，所以要把多维度的tensor展平成一维</span></span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中间特征提取</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureExtractor</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, submodule, extracted_layers)</span>:</span></span><br><span class="line">        super(FeatureExtractor, self).__init__()</span><br><span class="line">        self.submodule = submodule</span><br><span class="line">        self.extracted_layers = extracted_layers</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        outputs = []</span><br><span class="line">        print(self.submodule._modules.items())</span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.submodule._modules.items():</span><br><span class="line">            <span class="keyword">if</span> <span class="string">"fc"</span> <span class="keyword">in</span> name: </span><br><span class="line">                print(name)</span><br><span class="line">                x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">            print(module)</span><br><span class="line">            x = module(x)</span><br><span class="line">            print(name)</span><br><span class="line">            <span class="keyword">if</span> name <span class="keyword">in</span> self.extracted_layers:</span><br><span class="line">                outputs.append(x)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_feature</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 输入数据</span></span><br><span class="line">    img = get_picture(pic_dir, transform)</span><br><span class="line">    <span class="comment"># 插入维度</span></span><br><span class="line">    img = img.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    img = img.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 特征输出</span></span><br><span class="line">    net = LeNet().to(device)</span><br><span class="line">    <span class="comment"># net.load_state_dict(torch.load('./model/net_050.pth'))</span></span><br><span class="line">    exact_list = [<span class="string">"conv1"</span>，<span class="string">"conv2"</span>]</span><br><span class="line">    myexactor = FeatureExtractor(net, exact_list)</span><br><span class="line">    x = myexactor(img)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 特征输出可视化</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">        ax = plt.subplot(<span class="number">6</span>, <span class="number">6</span>, i + <span class="number">1</span>)</span><br><span class="line">        ax.set_title(<span class="string">'Feature &#123;&#125;'</span>.format(i))</span><br><span class="line">        ax.axis(<span class="string">'off'</span>)</span><br><span class="line">        plt.imshow(x[<span class="number">0</span>].data.numpy()[<span class="number">0</span>,i,:,:],cmap=<span class="string">'jet'</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    get_picture_rgb(pic_dir)</span><br><span class="line">    <span class="comment"># get_feature()</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;简述&quot;&gt;&lt;a href=&quot;#简述&quot; class=&quot;headerlink&quot; title=&quot;简述&quot;&gt;&lt;/a&gt;简述&lt;/h3&gt;&lt;p&gt;为了方便理解卷积神经网络的运行过程，需要对卷积神经网络的运行结果进行可视化的展示。  &lt;/p&gt;
&lt;p&gt;大致可分为如下步骤：&lt;/p&gt;
&lt;ul&gt;
      
    
    </summary>
    
      <category term="Pytorch框架" scheme="http://yoursite.com/categories/Pytorch%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="feature map" scheme="http://yoursite.com/tags/feature-map/"/>
    
  </entry>
  
  <entry>
    <title>基于pytorch的交通标志识别</title>
    <link href="http://yoursite.com/2018/11/11/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E4%BA%A4%E9%80%9A%E6%A0%87%E5%BF%97%E8%AF%86%E5%88%AB/"/>
    <id>http://yoursite.com/2018/11/11/基于pytorch的交通标志识别/</id>
    <published>2018-11-11T03:57:49.000Z</published>
    <updated>2018-11-11T07:07:34.337Z</updated>
    
    <content type="html"><![CDATA[<p>本文是在<a href="https://www.jianshu.com/p/d8feaddc7bdf文章的基础上用Pytorch实现的" target="_blank" rel="noopener">https://www.jianshu.com/p/d8feaddc7bdf文章的基础上用Pytorch实现的</a></p><p>话不多说，直接上代码，具体的可以看代码中的解释</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="keyword">as</span> tv</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> skimage.data</span><br><span class="line"><span class="keyword">import</span> skimage.transform</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义是否使用GPU</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">使得我们能够手动输入命令行参数，就是让风格变得和Linux命令行差不多</span></span><br><span class="line"><span class="string">argparse是python的一个包，用来解析输入的参数</span></span><br><span class="line"><span class="string">如：</span></span><br><span class="line"><span class="string">    python mnist.py --outf model  </span></span><br><span class="line"><span class="string">    （意思是将训练的模型保存到model文件夹下，当然，你也可以不加参数，那样的话代码最后一行</span></span><br><span class="line"><span class="string">      torch.save()就需要注释掉了）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    python mnist.py --net model/net_005.pth</span></span><br><span class="line"><span class="string">    （意思是加载之前训练好的网络模型，前提是训练使用的网络和测试使用的网络是同一个网络模型，保证权重参数矩阵相等）</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">'--outf'</span>, default=<span class="string">'./model/'</span>, help=<span class="string">'folder to output images and model checkpoints'</span>)  <span class="comment"># 模型保存路径</span></span><br><span class="line">parser.add_argument(<span class="string">'--net'</span>, default=<span class="string">'./model/net.pth'</span>, help=<span class="string">"path to netG (to continue training)"</span>)  <span class="comment"># 模型加载路径</span></span><br><span class="line">opt = parser.parse_args()  <span class="comment"># 解析得到你在路径中输入的参数，比如 --outf 后的"model"或者 --net 后的"model/net_005.pth"，是作为字符串形式保存的</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load training and testing datasets.</span></span><br><span class="line">ROOT_PATH = <span class="string">"./traffic"</span></span><br><span class="line">train_data_dir = os.path.join(ROOT_PATH, <span class="string">"datasets/BelgiumTS/Training"</span>)</span><br><span class="line">test_data_dir = os.path.join(ROOT_PATH, <span class="string">"datasets/BelgiumTS/Testing"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">定义LeNet神经网络，进一步的理解可查看Pytorch入门，里面很详细，代码本质上是一样的，这里做了一些封装</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    该类继承了torch.nn.Modul类</span></span><br><span class="line"><span class="string">    构建LeNet神经网络模型</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LeNet, self).__init__()  <span class="comment"># 这一个是python中的调用父类LeNet的方法，因为LeNet继承了nn.Module，如果不加这一句，无法使用导入的torch.nn中的方法，这涉及到python的类继承问题，你暂时不用深究</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第一层神经网络，包括卷积层、线性激活函数、池化层</span></span><br><span class="line">        self.conv1 = nn.Sequential(     <span class="comment"># input_size=(1*28*28)：输入层图片的输入尺寸，我看了那个文档，发现不需要天，会自动适配维度</span></span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),   <span class="comment"># padding=2保证输入输出尺寸相同：采用的是两个像素点进行填充，用尺寸为5的卷积核，保证了输入和输出尺寸的相同</span></span><br><span class="line">            nn.ReLU(),                  <span class="comment"># input_size=(6*28*28)：同上，其中的6是卷积后得到的通道个数，或者叫特征个数，进行ReLu激活</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), <span class="comment"># output_size=(6*14*14)：经过池化层后的输出</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二层神经网络，包括卷积层、线性激活函数、池化层</span></span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>),  <span class="comment"># input_size=(6*14*14)：  经过上一层池化层后的输出,作为第二层卷积层的输入，不采用填充方式进行卷积</span></span><br><span class="line">            nn.ReLU(),            <span class="comment"># input_size=(16*10*10)： 对卷积神经网络的输出进行ReLu激活</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)    <span class="comment"># output_size=(16*5*5)：  池化层后的输出结果</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 全连接层(将神经网络的神经元的多维输出转化为一维)</span></span><br><span class="line">        self.fc1 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">64</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">128</span>),  <span class="comment"># 进行线性变换</span></span><br><span class="line">            nn.ReLU()                    <span class="comment"># 进行ReLu激活</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出层(将全连接层的一维输出进行处理)</span></span><br><span class="line">        self.fc2 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">84</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将输出层的数据进行分类(输出预测值)</span></span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">62</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向传播过程，输入为x</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="comment"># nn.Linear()的输入输出都是维度为一的值，所以要把多维度的tensor展平成一维</span></span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数设置</span></span><br><span class="line">EPOCH = <span class="number">20</span>   <span class="comment"># 遍历数据集次数(训练模型的轮数)</span></span><br><span class="line">BATCH_SIZE = <span class="number">3</span>     <span class="comment"># 批处理尺寸(batch_size)：关于为何进行批处理，文档中有不错的介绍</span></span><br><span class="line">LR = <span class="number">0.001</span>        <span class="comment"># 学习率：模型训练过程中每次优化的幅度</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据预处理方式(将输入的类似numpy中arrary形式的数据转化为pytorch中的张量（tensor）)</span></span><br><span class="line">transform = transforms.ToTensor()</span><br><span class="line"><span class="comment"># transform = torch.FloatTensor</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练数据集(此处是加载MNIST手写数据集)</span></span><br><span class="line">trainset = tv.datasets.Traffic(</span><br><span class="line">    root=train_data_dir, <span class="comment"># 如果从本地加载数据集，对应的加载路径</span></span><br><span class="line">    train=<span class="keyword">True</span>,     <span class="comment"># 训练模型</span></span><br><span class="line">    download=<span class="keyword">True</span>,  <span class="comment"># 是否从网络下载训练数据集</span></span><br><span class="line">    transform=transform  <span class="comment"># 数据的转换形式</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练批处理数据</span></span><br><span class="line">trainloader = torch.utils.data.DataLoader(</span><br><span class="line">    trainset,                <span class="comment"># 加载测试集</span></span><br><span class="line">    batch_size=BATCH_SIZE,   <span class="comment"># 最小批处理尺寸</span></span><br><span class="line">    shuffle=<span class="keyword">True</span>,            <span class="comment"># 标识进行数据迭代时候将数据打乱</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义测试数据集</span></span><br><span class="line">testset = tv.datasets.Traffic(</span><br><span class="line">    root=test_data_dir, <span class="comment"># 如果从本地加载数据集，对应的加载路径</span></span><br><span class="line">    train=<span class="keyword">True</span>,     <span class="comment"># 训练模型</span></span><br><span class="line">    download=<span class="keyword">True</span>,  <span class="comment"># 是否从网络下载训练数据集</span></span><br><span class="line">    transform=transform  <span class="comment"># 数据的转换形式</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义测试批处理数据</span></span><br><span class="line">testloader = torch.utils.data.DataLoader(</span><br><span class="line">    testset,                 <span class="comment"># 加载测试集</span></span><br><span class="line">    batch_size=BATCH_SIZE,   <span class="comment"># 最小批处理尺寸</span></span><br><span class="line">    shuffle=<span class="keyword">False</span>,           <span class="comment"># 标识进行数据迭代时候不将数据打乱</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_train</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 定义损失函数loss function 和优化方式（采用SGD）</span></span><br><span class="line">    net = LeNet().to(device)</span><br><span class="line">    criterion = nn.CrossEntropyLoss()  <span class="comment"># 交叉熵损失函数，通常用于多分类问题上</span></span><br><span class="line">    optimizer = optim.SGD(net.parameters(), lr=LR, momentum=<span class="number">0.9</span>)  <span class="comment"># 优化函数</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">        sum_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 数据读取（采用python的枚举方法获得标签和数据，这一部分可能和numpy相关）</span></span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader):</span><br><span class="line">            inputs, labels = data</span><br><span class="line">            <span class="comment"># labels = [torch.LongTensor(label) for label in labels]</span></span><br><span class="line">            <span class="comment"># 将输入数据和标签放入构建的图中 注：图的概念可在pytorch入门中查</span></span><br><span class="line">            inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 梯度清零</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># forward + backward  注: 这一部分是训练神经网络的核心</span></span><br><span class="line">            outputs = net(inputs)</span><br><span class="line">            loss = criterion(outputs, labels)</span><br><span class="line">            loss.backward() <span class="comment"># 反向自动求导</span></span><br><span class="line">            optimizer.step() <span class="comment"># 进行优化</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 每训练100个batch打印一次平均loss</span></span><br><span class="line">            sum_loss += loss.item()</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">48</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">'[%d, %d] loss: %.03f'</span></span><br><span class="line">                      % (epoch + <span class="number">1</span>, i + <span class="number">1</span>, sum_loss / <span class="number">100</span>))</span><br><span class="line">                sum_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 每跑完一次epoch测试一下准确率</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            correct = <span class="number">0</span></span><br><span class="line">            total = <span class="number">0</span></span><br><span class="line">            <span class="comment"># for i, data in enumerate(testloader):</span></span><br><span class="line">            <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">                images, labels = data</span><br><span class="line">                images, labels = images.to(device), labels.to(device)</span><br><span class="line">                outputs = net(images)</span><br><span class="line">                <span class="comment"># 取得分最高的那个类</span></span><br><span class="line">                _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">                total += labels.size(<span class="number">0</span>)</span><br><span class="line">                correct += (predicted == labels).sum()</span><br><span class="line">            print(<span class="string">'第%d个epoch的识别准确率为：%d%%'</span> % (epoch + <span class="number">1</span>, (<span class="number">100</span> * correct / total)))</span><br><span class="line">    torch.save(net.state_dict(), <span class="string">'%s/net_%03d.pth'</span> % (opt.outf, epoch + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    model_train()</span><br></pre></td></tr></table></figure><h3 id="主要问题——数据读取"><a href="#主要问题——数据读取" class="headerlink" title="主要问题——数据读取"></a>主要问题——数据读取</h3><p>PyTorch中数据读取的一个重要接口是torch.utils.data.DataLoader，该接口定义在dataloader.py脚本中，只要是用PyTorch来训练模型基本都会用到该接口，为了满足pytorch的数据读取要求，写了一个tv.datasets.Traffic的读取文件，是基于mnist数据集的读取进行编写的：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> data</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"><span class="keyword">import</span> errno</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> skimage.data</span><br><span class="line"><span class="keyword">import</span> skimage.transform</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(data_dir, train=True)</span>:</span></span><br><span class="line">    <span class="string">"""Loads a data set and returns two lists:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    images: a list of Numpy arrays, each representing an image.</span></span><br><span class="line"><span class="string">    labels: a list of numbers that represent the images labels.</span></span><br><span class="line"><span class="string">    仅仅只是加载图片到数组，并没有对图片进行缩放比例</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        <span class="comment"># Get all subdirectories of data_dir. Each represents a label.</span></span><br><span class="line">        directories = [d <span class="keyword">for</span> d <span class="keyword">in</span> os.listdir(data_dir)</span><br><span class="line">                    <span class="keyword">if</span> os.path.isdir(os.path.join(data_dir, d))]</span><br><span class="line">        <span class="comment"># Loop through the label directories and collect the data in</span></span><br><span class="line">        <span class="comment"># two lists, labels and images.</span></span><br><span class="line">        labels = []</span><br><span class="line">        images = []</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> directories:</span><br><span class="line">            label_dir = os.path.join(data_dir, d)</span><br><span class="line">            file_names = [os.path.join(label_dir, f)</span><br><span class="line">                        <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(label_dir) <span class="keyword">if</span> f.endswith(<span class="string">".ppm"</span>)]</span><br><span class="line">            <span class="comment"># For each label, load it's images and add them to the images list.</span></span><br><span class="line">            <span class="comment"># And add the label number (i.e. directory name) to the labels list.</span></span><br><span class="line">            <span class="keyword">for</span> f <span class="keyword">in</span> file_names:</span><br><span class="line">                images.append(skimage.data.imread(f))</span><br><span class="line">                labels.append(int(d))  <span class="comment"># 为每一个图片加上标签</span></span><br><span class="line">    images28 = [skimage.transform.resize(image, (<span class="number">28</span>, <span class="number">28</span>)) <span class="keyword">for</span> image <span class="keyword">in</span> images]</span><br><span class="line">    labels_a = np.asarray(labels)</span><br><span class="line">    images_a = np.asarray(images28)</span><br><span class="line">    <span class="keyword">return</span> images_a, labels_a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Traffic</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Traffic Dataset.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, train=True, transform=None, target_transform=None, download=False)</span>:</span></span><br><span class="line">        self.root = os.path.expanduser(root)</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line">        self.train = train  <span class="comment"># training set or test set</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            self.train_data, self.train_labels = load_data(root,train)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.train_data, self.train_labels = load_data(root,train)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            index (int): Index</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            tuple: (image, target) where target is index of the target class.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            img, target = self.train_data[index], self.train_labels[index]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img, target = self.test_data[index], self.test_labels[index]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># doing this so that it is consistent with all other datasets</span></span><br><span class="line">        img = img.astype(np.float32)</span><br><span class="line">        target = torch.LongTensor([target])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.target_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            target = self.target_transform(target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, target</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            <span class="keyword">return</span> len(self.train_data)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> len(self.test_data)</span><br></pre></td></tr></table></figure><p><strong>注意:</strong> 在返回标签的时候，由于数据格式的问题，需要将标签放入一个list中，之后再转换为LongTensor，并取其第一个数据。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文是在&lt;a href=&quot;https://www.jianshu.com/p/d8feaddc7bdf文章的基础上用Pytorch实现的&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.jianshu.com/p/d8feaddc7b
      
    
    </summary>
    
      <category term="Pytorch框架" scheme="http://yoursite.com/categories/Pytorch%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="交通标志识别" scheme="http://yoursite.com/tags/%E4%BA%A4%E9%80%9A%E6%A0%87%E5%BF%97%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>机器学习的基本概念</title>
    <link href="http://yoursite.com/2018/10/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <id>http://yoursite.com/2018/10/29/机器学习的基本概念/</id>
    <published>2018-10-29T07:17:56.000Z</published>
    <updated>2018-10-30T12:58:21.733Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习和深度学习的关系在此不做赘述，本文主要说明机器学习的基本知识</p><h2 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h2><p>机器学习算法是一种能够从数据中学习的算法，Mitchell（1997）提供了一个简洁的定义：“对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。”</p><h3 id="任务T"><a href="#任务T" class="headerlink" title="任务T"></a><strong>任务T</strong></h3><p>通常机器学习任务定义为机器学习系统应该如何处理样本（example）。样本是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的特征（feature）的集合。  </p><p>一些非常常见的机器学习的任务列举如下：</p><ul><li>分类</li><li>输入缺失分类</li><li>回归</li><li>转录</li><li>机器翻译</li><li>结构化输出</li><li>异常检测</li><li>合成和采样</li><li>确实值填补</li><li>去噪</li><li>密度估计或概率质量函数估计  </li></ul><p>当然，还有很多其他同类型或其他类型的任务，这里我们列举的任务类型只是用来介绍机器学习可以做哪些任务。</p><h3 id="性能度量P"><a href="#性能度量P" class="headerlink" title="性能度量P"></a><strong>性能度量P</strong></h3><p>为了评估机器学习算法的能力，引入了对其性能的定量度量。通常性能度量P是特定与系统执行的任务T而言的。</p><p>这就需要我们针对不同的的任务来寻找相对应的性能度量标准。</p><h3 id="经验E"><a href="#经验E" class="headerlink" title="经验E"></a><strong>经验E</strong></h3><p>根据学习过程中的不同经验，机器学习算法可以大致分类为无监督（unsupervised）算法和监督（supervised）算法。  </p><ul><li>无监督学习算法（unsupervised learning algorithm）：训练包含有很多特征的数据集，然后学习出这个数据集上有用的结构性质。</li><li>监督学习算法（supervised learning algorithm）：训练含有很多特征的数据集，不过数据集中的样本都有一个标签（label）或者目标（target）。</li></ul><p>大部分学习算法可以被理解为在整个 <strong>数据集（dataset）</strong> 上获取经验。数据集可以用很多不同方式来表示，在所有的情况下，数据集都是样本的集合，而样本是特征的集合。</p><ul><li>表示数据集的常用方法是 <strong>设计矩阵（design matrix）</strong></li></ul><h2 id="容量、过拟合和欠拟合"><a href="#容量、过拟合和欠拟合" class="headerlink" title="容量、过拟合和欠拟合"></a>容量、过拟合和欠拟合</h2><p>机器学习的主要挑战是我们的算法必须能够在先前未观测到的新输入上表现良好，而不只是在训练集上表现良好。在先前未观测到的输入上表现良好的能力被称为 <strong>泛化（generalization）</strong>。</p><ul><li>训练误差（training error）：使用训练集，进行性能度量的一个量。来优化模型</li><li>泛化误差（generalization errot）：（也被称为测试误差（test error）），使用测试集，进行性能度量的一个量。</li></ul><p>训练集和测试集通过数据集上被称为数据生成过程（data generating process）的概率分部生成。在生成数据集时，不会提前固定参数，然后采样得到两个数据集，一般是先采样得到训练集，然后挑选参数去降低训练集误差，然后采样得到测试集。在这个过程中，测试误差会大于或等于训练误差期望。以下是决定机器学习算法效果是否好的因素：  </p><ul><li>降低训练误差</li><li>缩小训练误差和测试误差的差距  </li></ul><p>这两个因素对应机器学习的两个主要挑战：<strong>欠拟合（underfitting）和过拟合（overfitting）</strong>。</p><p>通过调整模型的容量（capacity）可以控制模型是否偏向于过拟合或者欠拟合。通俗来讲，模型的容量是指其拟合各种函数的能力。容量低的模型可能很难拟合训练集，容量高的模型可能会过拟合，因为记住了不适用于测试集的训练集性质。一种控制训练算法容量的方法是选择假设空间（hypothesis space），即学习算法可以选择为解决方法的函数集。容量不仅取决与模型的选择，模型规定了调整参数降低训练目标时，学习算法可以从哪些函数中选择函数。这被称为模型的表示容量（representational capacity）。在很多情况下，由于额外的限制因素，比如优化算法的不完美，意味着学习算法的有效容量（effective capacity）可能小于模型的表示容量。</p><h3 id="没有免费午餐定理"><a href="#没有免费午餐定理" class="headerlink" title="没有免费午餐定理"></a><strong>没有免费午餐定理</strong></h3><p>机器学习的没有免费午餐定理（no free lunch theorem）表明（Wolpert，1996），在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率。换言之，在某种意义上，没有一个机器学习算法总是比其他的要好。</p><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a><strong>正则化</strong></h3><p>正则化是指修改学习算法，使其降低泛化误差而非训练误差。正则化是机器学习领域领域的中心问题之一，只有优化能够与其重要性想提并论。</p><h2 id="超参数和验证集"><a href="#超参数和验证集" class="headerlink" title="超参数和验证集"></a>超参数和验证集</h2><p>大多数机器学习算法都有超参数，可以设置来控制算法行为。超参数的值不是通过学习算法本身学习出来的（尽管我们可以设置一个嵌套的学习过程，一个学习算法为另一个学习算法学习出最有超参数）<br>用于挑选超参数的数据子集被称为验证集</p><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a><strong>交叉验证</strong></h3><p>当数据集太小时，也有替代方法允许我们使用所有的样本估计平均测试误差，代价是增加了计算量。最常用的方法是K-折交叉验证过程。</p><ul><li>K-折交叉验证：将数据集分成k个不重合的子集，测试误差可以估计为k次计算后的平均测试误差。在第i次测试时，数据的第i个自己用于测试集，其他的数据用于训练集，但是带来的一个问题是不存在平均误差方差的无偏估计（Bengio and Grandvalet，2004）</li></ul><h2 id="估计、偏差和方差"><a href="#估计、偏差和方差" class="headerlink" title="估计、偏差和方差"></a>估计、偏差和方差</h2><p>统计领域为我们提供了很多工具来实现机器学习目标，不仅可以解决训练集上的任务，还可以泛化。基本的概念，例如参数估计、偏差和方差，对于正式的刻画泛化、欠拟合和过拟合都非常有帮助。</p><h3 id="点估计"><a href="#点估计" class="headerlink" title="点估计"></a><strong>点估计</strong></h3><p>点估计试图为一些感兴趣的量提供单个“最优”预测。一般地，感兴趣的量可以是单个参数，或是某些参数模型中的一个向量参数。</p><ul><li>函数估计：有时我们会关注函数估计（或函数近似）。</li></ul><h3 id="偏差"><a href="#偏差" class="headerlink" title="偏差"></a><strong>偏差</strong></h3><p>估计的偏差有 <strong>无偏（unbiased）和渐近无偏（asymptotically unbiased）</strong>。</p><ul><li>伯努利分布</li><li>均值的高斯分布估计</li><li><p>高斯分布方差估计</p><ul><li>样本方差</li></ul></li><li><p>无偏样本方差</p></li></ul><h3 id="方差和标准差"><a href="#方差和标准差" class="headerlink" title="方差和标准差"></a><strong>方差和标准差</strong></h3><p>我们有时会考虑估计量的另一个性质是它作为数据样本的函数，期望的变化程度是多少。正如我们可以计算估计量的期望来决定它的偏差，我们也可以计算它的方差。估计量的方差就是一个方差，另外，方差的平方根被称为标准差（standard error）。</p><h3 id="权衡偏差和方差以最小化均方误差"><a href="#权衡偏差和方差以最小化均方误差" class="headerlink" title="权衡偏差和方差以最小化均方误差"></a><strong>权衡偏差和方差以最小化均方误差</strong></h3><p>偏差和方差度量着估计量的两个不同误差来源。偏差度量着偏离真实函数或参数的误差期望，而方差度量这数据上任意特定采样可能导致的估计期望的偏差。<br>当面对一个偏差更大的估计和一个方差更大的估计时，我们如何进行选择。判断这种权衡最常用的方法是<strong>交叉验证</strong>，我们还可以比较这些估计的<strong>均方误差（mean squared error， MSE）</strong>。MSE度量着估计和真实值之间平方误差的总体期望偏差。</p><h3 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a><strong>一致性</strong></h3><p>一致性保证了估计量的偏差会随着样本数目的增多而减少。然而，反过来是不正确的——渐进无偏并不意味着一致性。</p><h2 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h2><p>之前，我们已经看到过常用估计的定义，并分析了它们的性质。但是这些估计是从哪里来的呢？我们希望有些准则可以让我们从不同模型中得到特定函数作为好的估计，而不是猜测某些函数可能是好的估计，然后分析其偏差和方差。<br>最常用的准则是最大似然估计。</p><h3 id="条件对数似然和均方误差"><a href="#条件对数似然和均方误差" class="headerlink" title="条件对数似然和均方误差"></a><strong>条件对数似然和均方误差</strong></h3><ul><li>线性回归作为最大似然</li></ul><h3 id="最大似然的性质"><a href="#最大似然的性质" class="headerlink" title="最大似然的性质"></a><strong>最大似然的性质</strong></h3><p>最大似然估计最吸引人的地方在于，它被证明当样本数目趋向于无穷大时，就收敛率而言是最好的渐进估计。<br>在合适的条件下，最大似然估计具有一致性，意味着训练样本数目趋向于无穷大时，参数的最大似然估计会收敛到参数的真实值。</p><h2 id="贝叶斯统计"><a href="#贝叶斯统计" class="headerlink" title="贝叶斯统计"></a>贝叶斯统计</h2><p>前面讨论的属于<strong>频率派统计（frequentist statistics）</strong> 方法和基于估计单一值的方法，然后基于该估计作所有的预测。另一种方法是在做预测时会考虑所有可能的值。后者属于<strong>贝叶斯统计（Bayesian statistics）</strong>的范畴。  </p><ul><li>贝叶斯线性回归</li></ul><h3 id="最大后验（MAP）估计"><a href="#最大后验（MAP）估计" class="headerlink" title="最大后验（MAP）估计"></a><strong>最大后验（MAP）估计</strong></h3><p> MAP贝叶斯推断提供了一个直观的方法来设计复杂但可解释的正则化项。例如，更复杂的惩罚项可以通过混合高斯分部作为先验得到，而不是一个单独的高斯分布（Nowlan and Hinton，1992）</p><h2 id="监督学习算法"><a href="#监督学习算法" class="headerlink" title="监督学习算法"></a>监督学习算法</h2><p>粗略的说，监督学习算法是给定一组输入x和输出y的训练集，学习如何关联输入和输出。在许多情况下，输出y很难自动收集，必须由人来提供“监督”，不过该术语仍然适用于训练集目标可以被自动收集的情况。  </p><h3 id="概率监督学习"><a href="#概率监督学习" class="headerlink" title="概率监督学习"></a><strong>概率监督学习</strong></h3><h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a><strong>支持向量机</strong></h3><p>支持向量机（support vector machine，SVM）是监督学习中最有影响力的方法之一。类似于逻辑回归，但支持向量机不输出概率，只输出类别。<br>支持向量机的一个重要创新是核技巧（kernal trick）。核技巧观察到许多机器学习算法都可以写成样本间点积的形式。<br>最常用的核函数是高斯核（Gaussian kernel），这个核也被称为径向基函数（radial basis function，RBF）核。<br>支持向量机不是唯一可以使用核技巧来增强的算法，许多其他的线性模型也可以通过这种方式来增强。使用核技巧的算法类别被称为和机器（kernel machine）或核方法（kernel method）。核机器的一个主要缺点是计算决策函数的成本关于训练样本的数目是线性的。而训练的样本则被称做支持向量（support vector）。</p><h3 id="其他简单的机器学习算法"><a href="#其他简单的机器学习算法" class="headerlink" title="其他简单的机器学习算法"></a><strong>其他简单的机器学习算法</strong></h3><ul><li>决策树</li></ul><h2 id="无监督学习算法"><a href="#无监督学习算法" class="headerlink" title="无监督学习算法"></a>无监督学习算法</h2><p>无监督算法只处理“特征”，不操作监督信号。监督和无监督算法之间的区别没有规范严格的定义，因为没有客观的判断来区分监督者提供的值是特征还是目标。通俗的说，无监督学习的大多数尝试是指从不需要人为注释的样本的分布中抽取信息。该术语通常与密度估计相关，学习从分布中采样、学习从分布中去噪、寻找数据分布的流形或是将数据中相关的样本聚类。</p><h3 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a><strong>主成分分析</strong></h3><h3 id="k-均值聚类"><a href="#k-均值聚类" class="headerlink" title="k-均值聚类"></a><strong>k-均值聚类</strong></h3><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>几乎所有的深度学习算法都用到了一个非常重要的算法：梯度下降算法（stochastic gradient,SGD）。<br>机器学习中反复出现的一个问题是好的泛化需要大的训练集，但大的计算集的计算代价也更大。机器学习算法中的代价函数通常可以分解成每个样本的代价函数的总和。<br>随机梯度下降的核心是，梯度是期望。期望可使用小规模的样本近似估计。</p><h2 id="构建机器学习算法"><a href="#构建机器学习算法" class="headerlink" title="构建机器学习算法"></a>构建机器学习算法</h2><h2 id="构建机器学习算法-1"><a href="#构建机器学习算法-1" class="headerlink" title="构建机器学习算法"></a>构建机器学习算法</h2><h2 id="促使深度学习发展的挑战"><a href="#促使深度学习发展的挑战" class="headerlink" title="促使深度学习发展的挑战"></a>促使深度学习发展的挑战</h2><ul><li>维度灾难</li><li>局部不变性和平滑正则化</li><li>流形学习</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;机器学习和深度学习的关系在此不做赘述，本文主要说明机器学习的基本知识&lt;/p&gt;
&lt;h2 id=&quot;学习算法&quot;&gt;&lt;a href=&quot;#学习算法&quot; class=&quot;headerlink&quot; title=&quot;学习算法&quot;&gt;&lt;/a&gt;学习算法&lt;/h2&gt;&lt;p&gt;机器学习算法是一种能够从数据中学习的算法
      
    
    </summary>
    
      <category term="机器学习算法" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="概念" scheme="http://yoursite.com/tags/%E6%A6%82%E5%BF%B5/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch--线性回归和逻辑回归</title>
    <link href="http://yoursite.com/2018/10/20/Pytorch-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2018/10/20/Pytorch-线性回归和逻辑回归/</id>
    <published>2018-10-20T05:53:49.000Z</published>
    <updated>2018-11-05T13:34:59.932Z</updated>
    
    <content type="html"><![CDATA[<h3 id="代码如下"><a href="#代码如下" class="headerlink" title="代码如下"></a><strong>代码如下</strong></h3><p>利用torch中的线性回归和逻辑回归模块实现</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">torch 一维线性回归算法</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成训练数据</span></span><br><span class="line">np.random.seed(<span class="number">10</span>)</span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">30</span>, <span class="number">20</span>)</span><br><span class="line">y = x * <span class="number">3</span> + np.random.normal(<span class="number">0</span>, <span class="number">5</span>, <span class="number">20</span>)</span><br><span class="line">x = np.array(x, dtype=np.float32).reshape([<span class="number">20</span>, <span class="number">1</span>])</span><br><span class="line">y = np.array(y, dtype=np.float32).reshape([<span class="number">20</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为torch中的张量形式</span></span><br><span class="line">x_train = torch.from_numpy(x)</span><br><span class="line">y_train = torch.from_numpy(y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    线性回归模型：一维线性回归</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LinearRegression, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model  = LinearRegression().cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model = LinearRegression()</span><br><span class="line"></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        inputs = Variable(x_train).cuda()</span><br><span class="line">        target = Variable(y_train).cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        inputs = Variable(x_train)</span><br><span class="line">        target = Variable(y_train)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    out = model(inputs)</span><br><span class="line">    loss = criterion(out, target)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Epoch[&#123;&#125;/&#123;&#125;], loss:&#123;:.6f&#125;'</span>.format(epoch+<span class="number">1</span>, num_epochs, loss.data[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">model.eval()</span><br><span class="line">predict = model(Variable(x_train))</span><br><span class="line">predict = predict.data.numpy()</span><br><span class="line"></span><br><span class="line">plt.plot(x_train.numpy(), y_train.numpy(), <span class="string">'ro'</span>, label=<span class="string">'Original data'</span>)</span><br><span class="line">plt.plot(x_train.numpy(), predict, label=<span class="string">'predict data'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">torch 一维线性回归算法(多项式回归)</span></span><br><span class="line"><span class="string">'</span><span class="string">''</span></span><br><span class="line"><span class="comment"># 生成训练数据</span></span><br><span class="line"></span><br><span class="line">def make_features(x):</span><br><span class="line">    <span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">    建立多项式特征</span></span><br><span class="line"><span class="string">    '</span><span class="string">''</span></span><br><span class="line">    x = x.unsqueeze(1)</span><br><span class="line">    <span class="built_in">return</span> torch.cat([x ** i <span class="keyword">for</span> i <span class="keyword">in</span> range(1, 4)], 1)</span><br><span class="line"></span><br><span class="line">W_target = torch.FloatTensor([0.5, 3, 2.4]).unsqueeze(1)</span><br><span class="line">b_target = torch.FloatTensor([0.9])</span><br><span class="line"></span><br><span class="line">def f(x):</span><br><span class="line">    <span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">    实际函数</span></span><br><span class="line"><span class="string">    '</span><span class="string">''</span></span><br><span class="line">    <span class="built_in">return</span> x.mm(W_target) + b_target[0]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_batch(batch_size=32):</span><br><span class="line">    <span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">    生成训练数据</span></span><br><span class="line"><span class="string">    '</span><span class="string">''</span></span><br><span class="line">    random = torch.randn(batch_size)</span><br><span class="line">    x = make_features(random)</span><br><span class="line">    y = f(x)</span><br><span class="line">    <span class="comment"># print(random,x)</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        <span class="built_in">return</span> Variable(x).cuda(), Variable(y).cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">return</span> Variable(random), Variable(x), Variable(y)</span><br><span class="line"></span><br><span class="line">class poly_model(nn.Module):</span><br><span class="line">    <span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">    多项式线性回归（三维）</span></span><br><span class="line"><span class="string">    '</span><span class="string">''</span></span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(poly_model, self).__init__()</span><br><span class="line">        self.poly = nn.Linear(3, 1)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        out = self.poly(x)</span><br><span class="line">        <span class="built_in">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model = poly_model().cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model = poly_model()</span><br><span class="line"></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=0.001)</span><br><span class="line"></span><br><span class="line">epoch = 0</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> True:</span><br><span class="line">    _, batch_x, batch_y = get_batch()</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    out = model(batch_x)</span><br><span class="line">    loss = criterion(out, batch_y)</span><br><span class="line">    print_loss = loss.data[0]</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(print_loss)</span><br><span class="line">    epoch += 1 </span><br><span class="line">    <span class="keyword">if</span> print_loss &lt; 0.001:</span><br><span class="line">        <span class="built_in">break</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">逻辑回归</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LogisticRegression, self).__init__()</span><br><span class="line">        self.lr = nn.Linear(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">        self.sm = nn.Sigmoid()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.lr(x)</span><br><span class="line">        x = self.sm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">logistic_model = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    logistic_model.cuda()</span><br><span class="line"></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line">optimezer = torch.optim.SGD(logistic_model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">50000</span>):</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        x = Variable(x_data).cuda()</span><br><span class="line">        y = Variable(y_data).cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = Variable(x_data)</span><br><span class="line">        y = Variable(y_data)</span><br><span class="line">    </span><br><span class="line">    out = logistic_model(x)</span><br><span class="line">    loss = criterion(out, y)</span><br><span class="line">    print_loss = loss.data[<span class="number">0</span>]</span><br><span class="line">    mask = out.ge(<span class="number">0.5</span>).float()</span><br><span class="line">    correct = (mask == y).sum()</span><br><span class="line">    acc = correct.data[<span class="number">0</span>] / x.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'*'</span>*<span class="number">10</span>)</span><br><span class="line">        print(<span class="string">'epoch &#123;&#125;'</span>.format(epoch+<span class="number">1</span>))</span><br><span class="line">        print(<span class="string">'loss is &#123;:.4f&#125;'</span>.format(print_loss))</span><br><span class="line">        print(<span class="string">'acc is &#123;:.4f&#125;'</span>.format(acc))</span><br><span class="line">        <span class="number">0</span></span><br><span class="line"></span><br><span class="line">w0, w1 = logistic_model.lr.weight[<span class="number">0</span>]</span><br><span class="line">w0 = w0.data[<span class="number">0</span>]</span><br><span class="line">w1 = w1.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">b = logistic_model.lr.bias.data[<span class="number">0</span>]</span><br><span class="line">plot_x = np.arrange(<span class="number">30</span>,<span class="number">100</span>,<span class="number">0.1</span>)</span><br><span class="line">plot_y = (-w0 * plot_x -b)/ w1</span><br><span class="line">plot.plot(plot_x, plot_y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;代码如下&quot;&gt;&lt;a href=&quot;#代码如下&quot; class=&quot;headerlink&quot; title=&quot;代码如下&quot;&gt;&lt;/a&gt;&lt;strong&gt;代码如下&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;利用torch中的线性回归和逻辑回归模块实现&lt;/p&gt;
&lt;figure class=&quot;hig
      
    
    </summary>
    
      <category term="Pytorch框架" scheme="http://yoursite.com/categories/Pytorch%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="线性回归" scheme="http://yoursite.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="逻辑回归" scheme="http://yoursite.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>机器学习——线性回归和逻辑回归</title>
    <link href="http://yoursite.com/2018/10/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2018/10/17/机器学习——线性回归和逻辑回归/</id>
    <published>2018-10-17T10:01:24.000Z</published>
    <updated>2018-10-20T05:52:40.969Z</updated>
    
    <content type="html"><![CDATA[<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a><strong>线性回归</strong></h2><h3 id="简述"><a href="#简述" class="headerlink" title="简述"></a><strong>简述</strong></h3><p>在统计学中，线性回归（Linear Regression）是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合（自变量都是一次方）。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。简单来说，就是找到一条直线去拟合数据点。如下图：<br><img src="/2018/10/17/机器学习——线性回归和逻辑回归/Figure_1.png" alt="图片"></p><p>优点：结果易于理解，计算上不复杂。<br>缺点：对非线性数据拟合不好。<br>适用数据类型：数值型和标称型数据。<br>算法类型：回归算法</p><p>线性回归的模型函数如下：  </p><p>$$h_\theta = \theta^Tx$$  </p><p>它的损失函数如下：  </p><p>$$J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2$$  </p><p>通过训练数据集寻找参数的最优解，即求解可以得到$minJ(θ)$的参数向量$θ$,其中这里的参数向量也可以分为参数和$w$和$b$,分别表示权重和偏置值。<br>求解最优解的方法有最小二乘法和梯度下降法。</p><ul><li><p><strong>梯度下降法</strong><br>  梯度下降算法的思想如下(这里以一元线性回归为例)：</p><p>  首先，我们有一个代价函数，假设是$J(θ_0,θ_1)$，我们的目标是$minθ_0,θ_1 J(θ_0,θ_1)$。<br>  接下来的做法是：</p><ul><li>首先是随机选择一个参数的组合$(θ_0,θ_1)$,一般是设$θ_0=0,θ_1=0$;</li><li><p>然后是不断改变$(θ_0,θ_1)$，并计算代价函数，直到一个局部最小值。之所以是局部最小值，是因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值，选择不同的初始参数组合，可能会找到不同的局部最小值。<br>下面给出梯度下降算法的公式：<br>repeat until convergence{</p><p>  $$\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(θ_0,θ_1)(for\ j =0\ and\ j=1)$$<br>}<br>也就是在梯度下降中，不断重复上述公式直到收敛，也就是找到局部最小值局部最小值。其中符号$:=$是赋值符号的意思。</p></li><li><p>而应用梯度下降法到线性回归，则公式如下：  </p><p>$$\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{i})-y^i)$$<br>$$\theta_1 := \theta_1 - \alpha\frac{1}{m}\sum_{i=1}^m((h_\theta(x^i)-y^i)\cdot x^i)$$  </p><p>公式中的$\alpha$称为学习率(learning rate)，它决定了我们沿着能让代价函数下降程度最大的<br>方向向下迈进的步子有多大。<br>在梯度下降中，还涉及都一个参数更新的问题，即更新$(\theta_0,\theta_1)$,一般我们的做法是同步更新.<br>最后，上述梯度下降算法公式实际上是一个叫<strong>批量梯度下降(batch gradient descent)</strong>，即它在每次梯度下降中都是使用整个训练集的数据，<br>所以公式中是带有$ \sum_{i=1}^m $.</p></li></ul></li><li><p><strong>岭回归（ridge regression）</strong><br>  岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价，获得回归系数更为符合实际、更可靠的回归方法，对病态数据的耐受性远远强于最小二乘法。</p><p>  岭回归分析法是从根本上消除复共线性影响的统计方法。岭回归模型通过在相关矩阵中引入一个很小的岭参数$K（1&gt;K&gt;0）$，并将它加到主对角线元素上，从而降低参数的最小二乘估计中复共线特征向量的影响，减小复共线变量系数最小二乘估计的方法，以保证参数估计更接近真实情况。岭回归分析将所有的变量引入模型中，比逐步回归分析提供更多的信息。</p></li></ul><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a><strong>代码实现</strong></h3><p>线性回归的相关数据及代码<a href="https://github.com/zouzhen/machine-learning-algorithms-in-python" target="_blank" rel="noopener">点此</a></p><ul><li><p>使用sklearn包中的线性回归算法</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets, linear_model</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error, r2_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the diabetes dataset</span></span><br><span class="line">diabetes = datasets.load_diabetes()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use only one feature</span></span><br><span class="line">diabetes_X = diabetes.data[:, np.newaxis, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the data into training/testing sets</span></span><br><span class="line">diabetes_X_train = diabetes_X[:<span class="number">-20</span>]</span><br><span class="line">diabetes_X_test = diabetes_X[<span class="number">-20</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the targets into training/testing sets</span></span><br><span class="line">diabetes_y_train = diabetes.target[:<span class="number">-20</span>]</span><br><span class="line">diabetes_y_test = diabetes.target[<span class="number">-20</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create linear regression object</span></span><br><span class="line">regr = linear_model.LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model using the training sets</span></span><br><span class="line">regr.fit(diabetes_X_train, diabetes_y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions using the testing set</span></span><br><span class="line">diabetes_y_pred = regr.predict(diabetes_X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The coefficients</span></span><br><span class="line">print(<span class="string">'Coefficients: \n'</span>, regr.coef_)</span><br><span class="line"><span class="comment"># The mean squared error</span></span><br><span class="line">print(<span class="string">"Mean squared error: %.2f"</span></span><br><span class="line">      % mean_squared_error(diabetes_y_test, diabetes_y_pred))</span><br><span class="line"><span class="comment"># Explained variance score: 1 is perfect prediction</span></span><br><span class="line">print(<span class="string">'Variance score: %.2f'</span> % r2_score(diabetes_y_test, diabetes_y_pred))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot outputs</span></span><br><span class="line">plt.scatter(diabetes_X_test, diabetes_y_test,  color=<span class="string">'black'</span>)</span><br><span class="line">plt.plot(diabetes_X_test, diabetes_y_pred, color=<span class="string">'blue'</span>, linewidth=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">plt.xticks(())</span><br><span class="line">plt.yticks(())</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li><li><p>使用代码实现算法</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">path = os.path.dirname(os.getcwd()) + <span class="string">'\data\ex1data1.txt'</span></span><br><span class="line">data = pd.read_csv(path, header=<span class="keyword">None</span>, names=[<span class="string">'Population'</span>, <span class="string">'Profit'</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(X, y, theta)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    损失函数</span></span><br><span class="line"><span class="string">    X: 自变量</span></span><br><span class="line"><span class="string">    y: 因变量</span></span><br><span class="line"><span class="string">    theta: 参数向量</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    inner = np.power(((X * theta.T) - y), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.sum(inner) / (<span class="number">2</span> * len(X))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X, y, theta, alpha, iters)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    梯度下降算法</span></span><br><span class="line"><span class="string">    X: 自变量</span></span><br><span class="line"><span class="string">    y: 因变量</span></span><br><span class="line"><span class="string">    theta: 参数向量</span></span><br><span class="line"><span class="string">    alpha: 学习率</span></span><br><span class="line"><span class="string">    iters: 计算次数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 暂存参数向量</span></span><br><span class="line">    temp = np.matrix(np.zeros(theta.shape))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将参数向量降为一维，返回视图，可以修改原始的参数向量</span></span><br><span class="line">    parameters = int(theta.ravel().shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 损失值消耗记录</span></span><br><span class="line">    cost = np.zeros(iters)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度下降的计算</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">        error = (X * theta.T) - y</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(parameters):</span><br><span class="line">            term = np.multiply(error, X[:, j])</span><br><span class="line">            temp[<span class="number">0</span>, j] = theta[<span class="number">0</span>, j] - ((alpha / len(X)) * np.sum(term))</span><br><span class="line"></span><br><span class="line">        theta = temp</span><br><span class="line">        cost[i] = computeCost(X, y, theta)</span><br><span class="line">    <span class="keyword">return</span> theta, cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># append a ones column to the front of the data set</span></span><br><span class="line">data.insert(<span class="number">0</span>, <span class="string">'Ones'</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set X (training data) and y (target variable)</span></span><br><span class="line">cols = data.shape[<span class="number">1</span>]</span><br><span class="line">X = data.iloc[:, <span class="number">0</span>:cols - <span class="number">1</span>]</span><br><span class="line">y = data.iloc[:, cols - <span class="number">1</span>:cols]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># convert from data frames to numpy matrices</span></span><br><span class="line">X = np.matrix(X.values)</span><br><span class="line">y = np.matrix(y.values)</span><br><span class="line">theta = np.matrix(np.array([<span class="number">0</span>, <span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize variables for learning rate and iterations</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line">iters = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># perform gradient descent to "fit" the model parameters</span></span><br><span class="line">g, cost = gradientDescent(X, y, theta, alpha, iters)</span><br><span class="line"></span><br><span class="line">x = np.linspace(data.Population.min(), data.Population.max(), <span class="number">100</span>)</span><br><span class="line">f = g[<span class="number">0</span>, <span class="number">0</span>] + (g[<span class="number">0</span>, <span class="number">1</span>] * x)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">ax.plot(x, f, <span class="string">'r'</span>, label=<span class="string">'Prediction'</span>)</span><br><span class="line">ax.scatter(data.Population, data.Profit, label=<span class="string">'Traning Data'</span>)</span><br><span class="line">ax.legend(loc=<span class="number">2</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Population'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Profit'</span>)</span><br><span class="line">ax.set_title(<span class="string">'Predicted Profit vs. Population Size'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看损失值的变化</span></span><br><span class="line"><span class="comment"># fig, ax = plt.subplots(figsize=(12,8))</span></span><br><span class="line"><span class="comment"># ax.plot(np.arange(iters), cost, 'r')</span></span><br><span class="line"><span class="comment"># ax.set_xlabel('Iterations')</span></span><br><span class="line"><span class="comment"># ax.set_ylabel('Cost')</span></span><br><span class="line"><span class="comment"># ax.set_title('Error vs. Training Epoch')</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a><strong>逻辑回归</strong></h2><h3 id="简述-1"><a href="#简述-1" class="headerlink" title="简述"></a><strong>简述</strong></h3><p>Logistic回归算法基于$Sigmoid$函数，或者说$Sigmoid$就是逻辑回归函数。$Sigmoid$函数定义如下： $\frac{1}{1+e^{-z}}$。函数值域范围$(0,1)$。<br>因此逻辑回归函数的表达式如下：  </p><p>$$h_\theta = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}}$$<br>$$其中，g(z) = \frac{1}{1+e^{-z}}$$  </p><p>其导数形式为：  </p><p>$$g\prime(z) = \frac{d}{dz}\frac{1}{1+e^{-z}}$$<br>$$=\frac{1}{(1+e^{-z})^2}(e^{-z})$$<br>$$=\frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}})$$<br>$$ = g(z)(1-g(z))$$</p><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a><strong>代价函数</strong></h3><p>逻辑回归方法主要是用最大似然估计来学习的，所以单个样本的后验概率为：  </p><p>$$p(y|x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}$$</p><p>到整个样本的后验概率就是:  </p><p>$$L(\theta) = p(y|X;\theta)$$<br>$$ = \prod_{i=1}^{m}p(y^i|x^i;\theta)$$<br>$$ = \prod_{i=1}^{m}(h_\theta(x^i))^{y^i}(1-h_\theta(x^i))^{1-y^i}$$<br>$$其中，P(y=1|x;\theta)=h_\theta(x),P(y=0|x;\theta)=1-h_\theta(x)$$<br>$$通过对数进一步简化有：l(\theta) = \log L(\theta) = \sum_{i=1}^m(y^i\log h(x^i)+(1-y^i)\log(1-h(x^i)))$$</p><p>而逻辑回归的代价函数就是$−l(\theta)$。也就是如下所示：  </p><p>$$J(\theta) = \frac{1}{m}\left[\sum_{i=1}^{m}y^i\log h_\theta(x^i)+(1-y^i)\log(1-h_\theta(x^i))\right]$$</p><p>同样可以使用梯度下降算法来求解使得代价函数最小的参数。其梯度下降法公式为：  </p><p>$$\frac{\partial}{\partial\theta_j}l(\theta) = \left(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)}\right)\frac{\partial}{\partial\theta_j}g(\theta^Tx)$$<br>$$= \left(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)}\right)\left(1-g(\theta^Tx)\frac{\partial}{\partial\theta_j}(\theta^Tx)\right)g(\theta^Tx)$$<br>$$= (y(1-g(\theta^Tx))-(1-y)g(\theta^Tx))x_j$$<br>$$= (y-h_\theta(x))x_j$$</p><p>$$\theta_j := \theta_j + \alpha(y^i-h_\theta(x^i)x_j^i$$</p><ul><li><p>总结<br>  优点：</p><p>  　　1、实现简单；</p><p>  　　2、分类时计算量非常小，速度很快，存储资源低；</p><p>  缺点：</p><p>  　　1、容易欠拟合，一般准确度不太高</p><p>  　　2、只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；</p><p>  适用数据类型：数值型和标称型数据。<br>  类别：分类算法。<br>  试用场景：解决二分类问题。<br>  如下图：<br>  <img src="/2018/10/17/机器学习——线性回归和逻辑回归/Figure_3.png" alt="图片3"><br>  <img src="/2018/10/17/机器学习——线性回归和逻辑回归/Figure_2.png" alt="图片2"></p></li></ul><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a><strong>代码实现</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">Plot multinomial and One-vs-Rest Logistic Regression</span></span><br><span class="line"><span class="string">'</span><span class="string">''</span></span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># make 3-class dataset for classification</span></span><br><span class="line">centers = [[-5, 0], [0, 1.5], [5, -1]]</span><br><span class="line">X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)</span><br><span class="line">transformation = [[0.4, 0.2], [-0.4, 1.2]]</span><br><span class="line">X = np.dot(X, transformation)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> multi_class <span class="keyword">in</span> (<span class="string">'multinomial'</span>, <span class="string">'ovr'</span>):</span><br><span class="line">    clf = LogisticRegression(solver=<span class="string">'sag'</span>, max_iter=100, random_state=42,</span><br><span class="line">                             multi_class=multi_class).fit(X, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print the training scores</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"training score : %.3f (%s)"</span> % (clf.score(X, y), multi_class))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create a mesh to plot in</span></span><br><span class="line">    h = .02  <span class="comment"># step size in the mesh</span></span><br><span class="line">    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                         np.arange(y_min, y_max, h))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot the decision boundary. For that, we will assign a color to each</span></span><br><span class="line">    <span class="comment"># point in the mesh [x_min, x_max]x[y_min, y_max].</span></span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    <span class="comment"># Put the result into a color plot</span></span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)</span><br><span class="line">    plt.title(<span class="string">"Decision surface of LogisticRegression (%s)"</span> % multi_class)</span><br><span class="line">    plt.axis(<span class="string">'tight'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot also the training points</span></span><br><span class="line">    colors = <span class="string">"bry"</span></span><br><span class="line">    <span class="keyword">for</span> i, color <span class="keyword">in</span> zip(clf.classes_, colors):</span><br><span class="line">        idx = np.where(y == i)</span><br><span class="line">        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired,</span><br><span class="line">                    edgecolor=<span class="string">'black'</span>, s=20)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot the three one-against-all classifiers</span></span><br><span class="line">    xmin, xmax = plt.xlim()</span><br><span class="line">    ymin, ymax = plt.ylim()</span><br><span class="line">    coef = clf.coef_</span><br><span class="line">    intercept = clf.intercept_</span><br><span class="line"></span><br><span class="line">    def plot_hyperplane(c, color):</span><br><span class="line">        def line(x0):</span><br><span class="line">            <span class="built_in">return</span> (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]</span><br><span class="line">        plt.plot([xmin, xmax], [line(xmin), line(xmax)],</span><br><span class="line">                 ls=<span class="string">"--"</span>, color=color)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, color <span class="keyword">in</span> zip(clf.classes_, colors):</span><br><span class="line">        plot_hyperplane(i, color)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">代码实现(加入正则化)</span></span><br><span class="line"><span class="string">'</span><span class="string">''</span></span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import scipy.optimize as opt</span><br><span class="line">import os</span><br><span class="line">path = os.path.dirname(os.getcwd()) + <span class="string">'\data\ex2data1.txt'</span></span><br><span class="line">data2 = pd.read_csv(path, header=None, names=[<span class="string">'Test 1'</span>, <span class="string">'Test 2'</span>, <span class="string">'Accepted'</span>])</span><br><span class="line"></span><br><span class="line">positive = data2[data2[<span class="string">'Accepted'</span>].isin([1])]</span><br><span class="line">negative = data2[data2[<span class="string">'Accepted'</span>].isin([0])]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sigmoid(z):</span><br><span class="line">    <span class="built_in">return</span> 1 / (1 + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def costReg(theta, X, y, learningRate):</span><br><span class="line">    theta = np.matrix(theta)</span><br><span class="line">    X = np.matrix(X)</span><br><span class="line">    y = np.matrix(y)</span><br><span class="line">    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))</span><br><span class="line">    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))</span><br><span class="line">    reg = (learningRate / 2 * len(X)) * np.sum(np.power(theta[:, 1:theta.shape[1]], 2))</span><br><span class="line">    <span class="built_in">return</span> np.sum(first - second) / (len(X)) + reg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def gradientReg(theta, X, y, learningRate):</span><br><span class="line">    theta = np.matrix(theta)</span><br><span class="line">    X = np.matrix(X)</span><br><span class="line">    y = np.matrix(y)</span><br><span class="line"></span><br><span class="line">    parameters = int(theta.ravel().shape[1])</span><br><span class="line">    grad = np.zeros(parameters)</span><br><span class="line"></span><br><span class="line">    error = sigmoid(X * theta.T) - y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(parameters):</span><br><span class="line">        term = np.multiply(error, X[:,i])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i == 0):</span><br><span class="line">            grad[i] = np.sum(term) / len(X)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            grad[i] = (np.sum(term) / len(X)) + ((learningRate / len(X)) * theta[:,i])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> grad</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def predict(theta, X):</span><br><span class="line">    probability = sigmoid(X * theta.T)</span><br><span class="line">    <span class="built_in">return</span> [1 <span class="keyword">if</span> x &gt;= 0.5 <span class="keyword">else</span> 0 <span class="keyword">for</span> x <span class="keyword">in</span> probability]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">degree = 5</span><br><span class="line">x1 = data2[<span class="string">'Test 1'</span>]</span><br><span class="line">x2 = data2[<span class="string">'Test 2'</span>]</span><br><span class="line"></span><br><span class="line">data2.insert(3, <span class="string">'Ones'</span>, 1)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(1, degree):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(0, i):</span><br><span class="line">        data2[<span class="string">'F'</span> + str(i) + str(j)] = np.power(x1, i-j) * np.power(x2, j)</span><br><span class="line"></span><br><span class="line">data2.drop(<span class="string">'Test 1'</span>, axis=1, inplace=True)</span><br><span class="line">data2.drop(<span class="string">'Test 2'</span>, axis=1, inplace=True)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set X and y (remember from above that we moved the label to column 0)</span></span><br><span class="line">cols = data2.shape[1]</span><br><span class="line">X2 = data2.iloc[:,1:cols]</span><br><span class="line">y2 = data2.iloc[:,0:1]</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert to numpy arrays and initalize the parameter array theta</span></span><br><span class="line">X2 = np.array(X2.values)</span><br><span class="line">y2 = np.array(y2.values)</span><br><span class="line">theta2 = np.zeros(11)</span><br><span class="line"></span><br><span class="line">learningRate = 0.1</span><br><span class="line">result2 = opt.fmin_tnc(func=costReg, x0=theta2, fprime=gradientReg, args=(X2, y2, learningRate))</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(costReg(theta2, X2, y2, learningRate))</span></span><br><span class="line">theta_min = np.matrix(result2[0])</span><br><span class="line">predictions = predict(theta_min, X2)</span><br><span class="line">correct = [1 <span class="keyword">if</span> ((a == 1 and b == 1) or (a == 0 and b == 0)) <span class="keyword">else</span> 0 <span class="keyword">for</span> (a, b) <span class="keyword">in</span> zip(predictions, y2)]</span><br><span class="line">accuracy = (sum(map(int, correct)) % len(correct))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'accuracy = &#123;0&#125;%'</span>.format(accuracy))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;线性回归&quot;&gt;&lt;a href=&quot;#线性回归&quot; class=&quot;headerlink&quot; title=&quot;线性回归&quot;&gt;&lt;/a&gt;&lt;strong&gt;线性回归&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;简述&quot;&gt;&lt;a href=&quot;#简述&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
      <category term="机器学习算法" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="线性回归" scheme="http://yoursite.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="逻辑回归" scheme="http://yoursite.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>scikit-learn整理</title>
    <link href="http://yoursite.com/2018/10/17/scikit-learn%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/10/17/scikit-learn整理/</id>
    <published>2018-10-17T07:54:09.000Z</published>
    <updated>2018-10-17T09:51:48.931Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Flask整理</title>
    <link href="http://yoursite.com/2018/10/17/Flask%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/10/17/Flask整理/</id>
    <published>2018-10-17T07:50:11.000Z</published>
    <updated>2018-10-17T09:51:48.921Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用工具"><a href="#使用工具" class="headerlink" title="使用工具"></a><strong>使用工具</strong></h2><h4 id="Flask后端-Postgresql数据库-JS前端（我未使用）"><a href="#Flask后端-Postgresql数据库-JS前端（我未使用）" class="headerlink" title="Flask后端 + Postgresql数据库 + JS前端（我未使用）"></a>Flask后端 + Postgresql数据库 + JS前端（我未使用）</h4><h2 id="Flask搭建"><a href="#Flask搭建" class="headerlink" title="Flask搭建"></a>Flask搭建</h2><ul><li><p><strong>确定确定目录结构</strong></p><ol><li>app/algorithms: 用来存放相关的算法文件</li><li>app/models: 用来存放数据库的操作</li><li>app/web: 用来存放路由和视图函数</li><li>manage: flask的启动文件</li></ol></li><li><p><strong>确定路由注册方式</strong></p><ol><li>使用蓝图形式来注册路由</li></ol></li><li><p><strong>确定数据库操作方式</strong></p><ol><li>使用sqlalchemy及psycopg2来控制Postgresql数据库</li><li>由于主要是用来进行数据读取的，所以采用非ORM方式构建的表结构，这种方式方便进行查询过滤操作</li></ol></li></ul><h2 id="基于sqlalchemy的Postgresql数据库访问操作"><a href="#基于sqlalchemy的Postgresql数据库访问操作" class="headerlink" title="基于sqlalchemy的Postgresql数据库访问操作"></a>基于sqlalchemy的Postgresql数据库访问操作</h2><ul><li><strong>创建表结构</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sqlalchemy.engine <span class="keyword">import</span> create_engine</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.schema <span class="keyword">import</span> MetaData, Table, Column, ForeignKey, Sequence</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.types <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.sql.expression <span class="keyword">import</span> select,and_</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line">engine = create_engine(<span class="string">'postgres://user:password@hosts/builder'</span>, echo=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">metadata = MetaData()</span><br><span class="line">metadata.bind = engine</span><br><span class="line">    <span class="comment"># 创建桥梁索引表</span></span><br><span class="line">bridges_table = Table(<span class="string">'bridges'</span>, metadata,</span><br><span class="line">                      Column(<span class="string">'id'</span>, Integer, primary_key=<span class="keyword">True</span>),</span><br><span class="line">                      Column(<span class="string">'org_id'</span>, Integer, nullable=<span class="keyword">False</span>),</span><br><span class="line">                      Column(<span class="string">'user_id'</span>, Integer, nullable=<span class="keyword">False</span>),</span><br><span class="line">                      Column(<span class="string">'name'</span>, VARCHAR(length=<span class="number">255</span>), nullable=<span class="keyword">False</span>),</span><br><span class="line">                      Column(<span class="string">'created_date'</span>, TIMESTAMP, nullable=<span class="keyword">False</span>),</span><br><span class="line">                      Column(<span class="string">'finished_date'</span>, TIMESTAMP, nullable=<span class="keyword">True</span>),</span><br><span class="line">                    <span class="comment">#   autoload=True,</span></span><br><span class="line">                      )</span><br></pre></td></tr></table></figure><p>这种方式，有助于进行表查询，具体的相关API介绍及使用放那格式可<a href="https://docs.sqlalchemy.org/en/latest/genindex.html" target="_blank" rel="noopener">点此</a>查看。</p><ul><li><strong>相关操作</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">()</span>:</span></span><br><span class="line">    s = book_table.insert().values(title=<span class="string">'测试写入2'</span>,time=datetime.now())</span><br><span class="line">    c = engine.execute(s)</span><br><span class="line">    c.close()</span><br><span class="line">    <span class="keyword">return</span> c.inserted_primary_key</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">query_code</span><span class="params">(id)</span>:</span></span><br><span class="line">    info = &#123;<span class="string">'id'</span>: <span class="string">''</span>, <span class="string">'title'</span>: <span class="string">''</span>&#125;</span><br><span class="line">    s = select([bridge_jobs_table.c.id.label(<span class="string">'name'</span>)]).where(and_(bridge_jobs_table.c.kind==<span class="string">'桩基'</span>,bridge_jobs_table.c.name==<span class="string">'起钻'</span>))</span><br><span class="line">    codename_query = engine.execute(s)</span><br><span class="line">    print(codename_query.keys())</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> codename_query:</span><br><span class="line">        print(row[<span class="number">0</span>])</span><br><span class="line">    codename_query.close()</span><br><span class="line">    <span class="keyword">return</span> info</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updata</span><span class="params">(id, title)</span>:</span></span><br><span class="line">    s = book_table.update().where(book_table.c.id == id).values(title=title, id=id)</span><br><span class="line">    c = engine.execute(s)</span><br><span class="line">    c.close()</span><br></pre></td></tr></table></figure><h2 id="Flask相关知识"><a href="#Flask相关知识" class="headerlink" title="Flask相关知识"></a>Flask相关知识</h2><ul><li><p><strong>路由操作</strong></p><ol><li><p>静态路由</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@web.route('/hello', methods=['POST', 'GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'hello world!'</span></span><br></pre></td></tr></table></figure></li><li><p>参数路由</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@web.route('/hello/&lt;string:name&gt;', methods=['POST', 'GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'hello %d '</span>% name</span><br></pre></td></tr></table></figure></li><li><p>JSON返回</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@web.route('/hello/&lt;string:name&gt;', methods=['POST', 'GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> jsonify(<span class="string">'hello %d '</span>% name)</span><br></pre></td></tr></table></figure></li><li><p>使用蓝图方式注册路由</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_app</span><span class="params">()</span>:</span></span><br><span class="line">    app = Flask(__name__)</span><br><span class="line">    app.config.from_object(<span class="string">'app.setting'</span>)</span><br><span class="line"></span><br><span class="line">    register_blueprint(app)</span><br><span class="line">    <span class="comment"># db.init_app(app)</span></span><br><span class="line">    <span class="comment"># db.create_app(app=app)</span></span><br><span class="line">    <span class="keyword">return</span> app</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">register_blueprint</span><span class="params">(app)</span>:</span></span><br><span class="line">    <span class="keyword">from</span> app.web.view <span class="keyword">import</span> web</span><br><span class="line">    app.register_blueprint(web)</span><br></pre></td></tr></table></figure></li></ol></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;使用工具&quot;&gt;&lt;a href=&quot;#使用工具&quot; class=&quot;headerlink&quot; title=&quot;使用工具&quot;&gt;&lt;/a&gt;&lt;strong&gt;使用工具&lt;/strong&gt;&lt;/h2&gt;&lt;h4 id=&quot;Flask后端-Postgresql数据库-JS前端（我未使用）&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="Python后端" scheme="http://yoursite.com/categories/Python%E5%90%8E%E7%AB%AF/"/>
    
    
      <category term="flask框架" scheme="http://yoursite.com/tags/flask%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>自省</title>
    <link href="http://yoursite.com/2018/10/06/%E8%87%AA%E7%9C%81/"/>
    <id>http://yoursite.com/2018/10/06/自省/</id>
    <published>2018-10-06T03:06:43.000Z</published>
    <updated>2018-10-29T11:44:55.364Z</updated>
    
    <content type="html"><![CDATA[<script src=/js/crypto-js.js></script><script>function doDecrypt (pwd, onError) {console.log('in doDecrypt');const txt = document.getElementById('enc_content').innerHTML;let plantext;try {const bytes = CryptoJS.AES.decrypt(txt, pwd);var plaintext = bytes.toString(CryptoJS.enc.Utf8);} catch(err) {if(onError) {onError(err);}return;}document.getElementById('enc_content').innerHTML = plaintext;document.getElementById('enc_content').style.display = 'block';document.getElementById('enc_passwd').style.display = 'none';if(typeof MathJax !== 'undefined') {MathJax.Hub.Queue(['resetEquationNumbers', MathJax.InputJax.TeX],['PreProcess', MathJax.Hub],['Reprocess', MathJax.Hub]);}}</script><div id="enc_content" style="display:none">U2FsdGVkX188xf+vlXICG22c3xImFW4BK+Wfjdk2tjnoD/dRSVBAuF7KBb7FDojG9aTQOsrHkcD/Fx1sIS1rY6QTCQi5+HcV3NyfZHRQ/H6cCYsHCUKZVmOgYuHGM/Nh4OEj7Lsn/aLZ4niPGJSOAJTx+M/l6nkTMFDjWEBVtOSpQiZgWlJ7K2Ao5FCRO0gHEErUHeYdqn1eUx80ztf4a0pGi3tWsFbQr/YKic8jslEawBWwee21J2LK0jMuigeK/tz5e8m+1r7eiWH4gRHVMeRHROkeqqlmRWFh4lrHLsSfl/4V/2A06kOjRRcDN05UzKoxEsPH2Hqmnd2v+47NQKHVYt3IbQIO/tpf/BC7WaVqcMmtGClVJyvWsr96S+2xjgBgFnXS+LoiSKTOokb1TmnZdS9vJsjku21JbCm97ZQzpoFZ0Evo7bCz4vTm6o2TkC0Iy5Cej0oyHoAIaebHomc0WyFNUN5hR9Vf5JFQAT2SuFtaYTi4IT+E7FPKjwM0</div><div id="enc_passwd"> <input id="enc_pwd_input" type="password" style="border-radius: 5px;border-style: groove;height: 30px;width: 50%;cursor: auto;font-size: 102%;color: currentColor;outline: none;text-overflow: initial;padding-left: 5px;" onkeydown="if (event.keyCode == 13) { decrypt(); return false;}"> <input type="submit" value="解&nbsp;密" onclick="decrypt()" style="width: 58px;height: 34px;border-radius: 5px;background-color: white;border-style: solid;color: currentColor;"><div id="enc_error" style="display: inline-block;color: #d84527;margin-left: 10px"></div><script>var onError = function(error) {document.getElementById("enc_error").innerHTML = "password error!"};function decrypt() {var passwd = document.getElementById("enc_pwd_input").value;console.log(passwd);doDecrypt(passwd, onError);}</script></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=/js/crypto-js.js&gt;&lt;/script&gt;
&lt;script&gt;
function doDecrypt (pwd, onError) {
	console.log(&#39;in doDecrypt&#39;);
	const txt = document.getE
      
    
    </summary>
    
      <category term="感悟" scheme="http://yoursite.com/categories/%E6%84%9F%E6%82%9F/"/>
    
    
      <category term="自我反省" scheme="http://yoursite.com/tags/%E8%87%AA%E6%88%91%E5%8F%8D%E7%9C%81/"/>
    
  </entry>
  
  <entry>
    <title>PostgreSQL学习记录</title>
    <link href="http://yoursite.com/2018/09/26/PostgreSQL%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2018/09/26/PostgreSQL学习记录/</id>
    <published>2018-09-26T06:26:56.000Z</published>
    <updated>2018-10-17T09:54:05.116Z</updated>
    
    <content type="html"><![CDATA[<p>select<br>fetchall()<br>fetchmany()<br>fetchone()</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;select&lt;br&gt;fetchall()&lt;br&gt;fetchmany()&lt;br&gt;fetchone()&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>docker基础</title>
    <link href="http://yoursite.com/2018/09/03/docker%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/09/03/docker基础/</id>
    <published>2018-09-03T02:44:54.000Z</published>
    <updated>2018-10-17T09:51:48.925Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorflow入门整理</title>
    <link href="http://yoursite.com/2018/08/05/tensorflow%E5%85%A5%E9%97%A8%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/08/05/tensorflow入门整理/</id>
    <published>2018-08-05T10:26:06.000Z</published>
    <updated>2018-08-07T10:08:38.972Z</updated>
    
    <content type="html"><![CDATA[<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><blockquote><p>tf.placeholder()</p></blockquote><p>是tensorflow的一种特殊变量，这种变量并非在初始化时定义好内容，而是在训练的时候才将数据填入其中。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;tf.placeholder()&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;是tensorflow
      
    
    </summary>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>vim基本使用方法</title>
    <link href="http://yoursite.com/2018/08/01/vim%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2018/08/01/vim基本使用方法/</id>
    <published>2018-08-01T07:39:35.000Z</published>
    <updated>2018-08-05T09:10:09.401Z</updated>
    
    <content type="html"><![CDATA[<h2 id="vi的基本概念"><a href="#vi的基本概念" class="headerlink" title="vi的基本概念"></a>vi的基本概念</h2><p>基本上vi可以分为三种状态，分别是命令模式（command mode）、插入模式（Insert mode）和底行模式（last line mode），各模式的功能区分如下：</p><ol><li>命令行模式command mode）<br>控制屏幕光标的移动，字符、字或行的删除，移动复制某区段及进入Insert mode下，或者到 last line mode。</li><li>插入模式（Insert mode）<br>只有在Insert mode下，才可以做文字输入，按「ESC」键可回到命令行模式。</li><li>底行模式（last line mode）<br>将文件保存或退出vi，也可以设置编辑环境，如寻找字符串、列出行号……等。</li></ol><p>不过一般我们在使用时把vi简化成两个模式，就是将底行模式（last line mode）也算入命令行模式command mode）。</p><h2 id="vi的基本操作"><a href="#vi的基本操作" class="headerlink" title="vi的基本操作"></a>vi的基本操作</h2><ol><li><p>进入vi<br>在系统提示符号输入vi及文件名称后，就进入vi全屏幕编辑画面：$ vi myfile。不过有一点要特别注意，就是您进入vi之后，是处于「命令行模式（command mode）」，您要切换到「插入模式（Insert mode）」才能够输入文字。初次使用vi的人都会想先用上下左右键移动光标，结果电脑一直哔哔叫，把自己气个半死，所以进入vi后，先不要乱动，转换到「插入模式（Insert mode）」再说吧！</p></li><li><p>切换至插入模式（Insert mode）编辑文件<br>在「命令行模式（command mode）」下按一下字母「i」就可以进入「插入模式（Insert mode）」，这时候你就可以开始输入文字了。</p></li><li><p>Insert 的切换<br>您目前处于「插入模式（Insert mode）」，您就只能一直输入文字，如果您发现输错了字！想用光标键往回移动，将该字删除，就要先按一下「ESC」键转到「命令行模式（command mode）」再删除文字。</p></li><li><p>退出vi及保存文件<br>在「命令行模式（command mode）」下，按一下「：」冒号键进入「Last line mode」，例如：<br>: w filename （输入 「w filename」将文章以指定的文件名filename保存）<br>: wq (输入「wq」，存盘并退出vi)<br>: q! (输入q!， 不存盘强制退出vi)</p></li></ol><h2 id="命令行模式（command-mode）功能键"><a href="#命令行模式（command-mode）功能键" class="headerlink" title="命令行模式（command mode）功能键"></a>命令行模式（command mode）功能键</h2><ol><li><p>插入模式<br>按「i」切换进入插入模式「insert mode」，按“i”进入插入模式后是从光标当前位置开始输入文件；<br>按「a」进入插入模式后，是从目前光标所在位置的下一个位置开始输入文字；<br>按「o」进入插入模式后，是插入新的一行，从行首开始输入文字。</p></li><li><p>从插入模式切换为命令行模式<br>按「ESC」键。</p></li><li><p>移动光标<br>vi可以直接用键盘上的光标来上下左右移动，但正规的vi是用小写英文字母「h」、「j」、「k」、「l」，分别控制光标左、下、上、右移一格。<br>按「ctrl」+「b」：屏幕往“后”移动一页。<br>按「ctrl」+「f」：屏幕往“前”移动一页。<br>按「ctrl」+「u」：屏幕往“后”移动半页。<br>按「ctrl」+「d」：屏幕往“前”移动半页。<br>按数字「0」：移到文章的开头。<br>按「G」：移动到文章的最后。<br>按「$」：移动到光标所在行的“行尾”。<br>按「^」：移动到光标所在行的“行首”<br>按「w」：光标跳到下个字的开头<br>按「e」：光标跳到下个字的字尾<br>按「b」：光标回到上个字的开头<br>按「#l」：光标移到该行的第#个位置，如：5l,56l。</p></li><li><p>删除文字<br>「x」：每按一次，删除光标所在位置的“后面”一个字符。<br>「#x」：例如，「6x」表示删除光标所在位置的“后面”6个字符。<br>「X」：大写的X，每按一次，删除光标所在位置的“前面”一个字符。<br>「#X」：例如，「20X」表示删除光标所在位置的“前面”20个字符。<br>「dd」：删除光标所在行。<br>「#dd」：从光标所在行开始删除#行</p></li><li><p>复制<br>「yw」：将光标所在之处到字尾的字符复制到缓冲区中。<br>「#yw」：复制#个字到缓冲区<br>「yy」：复制光标所在行到缓冲区。<br>「#yy」：例如，「6yy」表示拷贝从光标所在的该行“往下数”6行文字。<br>「p」：将缓冲区内的字符贴到光标所在位置。注意：所有与“y”有关的复制命令都必须与“p”配合才能完成复制与粘贴功能。</p></li><li><p>替换<br>「r」：替换光标所在处的字符。<br>「R」：替换光标所到之处的字符，直到按下「ESC」键为止。</p></li><li><p>回复上一次操作<br>「u」：如果您误执行一个命令，可以马上按下「u」，回到上一个操作。按多次“u”可以执行多次回复。</p></li><li><p>更改<br>「cw」：更改光标所在处的字到字尾处<br>「c#w」：例如，「c3w」表示更改3个字</p></li><li><p>跳至指定的行<br>「ctrl」+「g」列出光标所在行的行号。<br>「#G」：例如，「15G」，表示移动光标至文章的第15行行首。</p></li></ol><h2 id="Last-line-mode下命令简介"><a href="#Last-line-mode下命令简介" class="headerlink" title="Last line mode下命令简介"></a>Last line mode下命令简介</h2><p>　　在使用「last line mode」之前，请记住先按「ESC」键确定您已经处于「command mode」下后，再按「：」冒号即可进入「last line mode」。</p><ol><li><p>列出行号<br>「set nu」：输入「set nu」后，会在文件中的每一行前面列出行号。</p></li><li><p>跳到文件中的某一行<br>「#」：「#」号表示一个数字，在冒号后输入一个数字，再按回车键就会跳到该行了，如输入数字15，再回车，就会跳到文章的第15行。</p></li><li><p>查找字符<br>「/关键字」：先按「/」键，再输入您想寻找的字符，如果第一次找的关键字不是您想要的，可以一直按「n」会往后寻找到您要的关键字为止。<br>「?关键字」：先按「?」键，再输入您想寻找的字符，如果第一次找的关键字不是您想要的，可以一直按「n」会往前寻找到您要的关键字为止。</p></li><li><p>保存文件<br>「w」：在冒号输入字母「w」就可以将文件保存起来。</p></li><li><p>离开vi<br>「q」：按「q」就是退出，如果无法离开vi，可以在「q」后跟一个「!」强制离开vi。<br>「qw」：一般建议离开时，搭配「w」一起使用，这样在退出的时候还可以保存文件。</p></li></ol><h2 id="vi命令列表"><a href="#vi命令列表" class="headerlink" title="vi命令列表"></a>vi命令列表</h2><ol><li><p>下表列出命令模式下的一些键的功能：</p><p> h左移光标一个字符<br> l右移光标一个字符<br> k光标上移一行<br> j光标下移一行<br> ^光标移动至行首<br> 0数字“0”，光标移至文章的开头<br> G光标移至文章的最后<br> $光标移动至行尾<br> Ctrl+f向前翻屏<br> Ctrl+b向后翻屏<br> Ctrl+d向前翻半屏<br> Ctrl+u向后翻半屏<br> i在光标位置前插入字符<br> a在光标所在位置的后一个字符开始增加<br> o插入新的一行，从行首开始输入<br> ESC从输入状态退至命令状态<br> x删除光标后面的字符<br> #x删除光标后的＃个字符<br> X(大写X)，删除光标前面的字符<br> #X删除光标前面的#个字符<br> dd删除光标所在的行<br> #dd删除从光标所在行数的#行<br> yw复制光标所在位置的一个字<br> #yw复制光标所在位置的#个字<br> yy复制光标所在位置的一行<br> #yy复制从光标所在行数的#行<br> p粘贴<br> u取消操作<br> cw更改光标所在位置的一个字<br> #cw更改光标所在位置的#个字</p></li></ol><ol start="2"><li><p>下表列出行命令模式下的一些指令  </p><p> w filename储存正在编辑的文件为filename<br> wq filename储存正在编辑的文件为filename，并退出vi<br> q!放弃所有修改，退出vi<br> set nu显示行号<br> /或?查找，在/后输入要查找的内容<br> n与/或?一起使用，如果查找的内容不是想要找的关键字，按n或向后（与/联用）或向前（与?联用）继续查找，直到找到为止。</p></li></ol><p>高手总结的图：<img src="/2018/08/01/vim基本使用方法/vim.jpg" alt="vim"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;vi的基本概念&quot;&gt;&lt;a href=&quot;#vi的基本概念&quot; class=&quot;headerlink&quot; title=&quot;vi的基本概念&quot;&gt;&lt;/a&gt;vi的基本概念&lt;/h2&gt;&lt;p&gt;基本上vi可以分为三种状态，分别是命令模式（command mode）、插入模式（Insert mo
      
    
    </summary>
    
      <category term="工具" scheme="http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="vim" scheme="http://yoursite.com/tags/vim/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch入门</title>
    <link href="http://yoursite.com/2018/08/01/PyTorch%E5%85%A5%E9%97%A8/"/>
    <id>http://yoursite.com/2018/08/01/PyTorch入门/</id>
    <published>2018-08-01T06:59:07.000Z</published>
    <updated>2018-10-20T05:56:23.094Z</updated>
    
    <content type="html"><![CDATA[<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><hr><ul><li><font color="#00dddd" size="4">张量</font><br>  </li></ul><hr><p>张量类似于NumPy的ndarray，另外还有Tensors也可用于GPU以加速计算。  </p><pre><code>from __future__ import print_functionimport torch  </code></pre><p>构造一个未初始化的5x3矩阵：</p><pre><code>x = torch.empty(5, 3)print(x)    tensor([[ 3.2401e+18,  0.0000e+00,  1.3474e-08],    [ 4.5586e-41,  1.3476e-08,  4.5586e-41],    [ 1.3476e-08,  4.5586e-41,  1.3474e-08],    [ 4.5586e-41,  1.3475e-08,  4.5586e-41],    [ 1.3476e-08,  4.5586e-41,  1.3476e-08]])  </code></pre><p>构造一个矩阵填充的零和dtype long：</p><pre><code>x = torch.zeros(5, 3, dtype=torch.long)print(x)  tensor([[ 0,  0,  0],        [ 0,  0,  0],        [ 0,  0,  0],        [ 0,  0,  0],        [ 0,  0,  0]])  </code></pre><p>直接从数据构造张量：</p><pre><code>x = torch.tensor([5.5, 3])print(x)tensor([ 5.5000,  3.0000])</code></pre><p>或者根据现有的张量创建张量。除非用户提供新值，否则这些方法将重用输入张量的属性，例如dtype</p><pre><code>x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizesprint(x)x = torch.randn_like(x, dtype=torch.float)    # override dtype!print(x)                                      # result has the same sizetensor([[ 1.,  1.,  1.],        [ 1.,  1.,  1.],        [ 1.,  1.,  1.],        [ 1.,  1.,  1.],        [ 1.,  1.,  1.]], dtype=torch.float64)tensor([[ 0.2641,  0.0149,  0.7355],        [ 0.6106, -1.2480,  1.0592],        [ 2.6305,  0.5582,  0.3042],        [-1.4410,  2.4951, -0.0818],        [ 0.8605,  0.0001, -0.7220]])</code></pre><p>得到它的大小：  </p><pre><code>print(x.size())torch.Size([5, 3])  </code></pre><p><strong>注意</strong>  </p><p><strong>torch.Size</strong> 实际上是一个元组，因此它支持所有元组操作。  </p><hr><ul><li><font color="#00dddd" size="4">操作</font><br>   </li></ul><hr><p>操作有多种语法。在下面的示例中，我们将查看添加操作。</p><p>增加：语法1  </p><pre><code>y = torch.rand(5, 3)print(x + y)tensor([[ 0.7355,  0.2798,  0.9392],        [ 1.0300, -0.6085,  1.7991],        [ 2.8120,  1.2438,  1.2999],        [-1.0534,  2.8053,  0.0163],        [ 1.4088,  0.9000, -0.1172]])</code></pre><p>增加：语法2</p><pre><code>print(torch.add(x, y))tensor([[ 0.7355,  0.2798,  0.9392],        [ 1.0300, -0.6085,  1.7991],        [ 2.8120,  1.2438,  1.2999],        [-1.0534,  2.8053,  0.0163],        [ 1.4088,  0.9000, -0.1172]])</code></pre><p>增加：提供输出张量作为参数</p><pre><code>result = torch.empty(5, 3)torch.add(x, y, out=result)print(result)tensor([[ 0.7355,  0.2798,  0.9392],        [ 1.0300, -0.6085,  1.7991],        [ 2.8120,  1.2438,  1.2999],        [-1.0534,  2.8053,  0.0163],        [ 1.4088,  0.9000, -0.1172]])</code></pre><p>增加：就地</p><pre><code># adds x to yy.add_(x)print(y)tensor([[ 0.7355,  0.2798,  0.9392],        [ 1.0300, -0.6085,  1.7991],        [ 2.8120,  1.2438,  1.2999],        [-1.0534,  2.8053,  0.0163],        [ 1.4088,  0.9000, -0.1172]])</code></pre><p><strong>注意</strong></p><p>任何使原位张量变形的操作都是用<em>。后固定的。例如：x.copy</em>(y)，x.t_()，将改变x。</p><p>你可以使用标准的NumPy索引与所有的铃声和​​口哨！</p><pre><code>print(x[:, 1])tensor([ 0.0149, -1.2480,  0.5582,  2.4951,  0.0001])</code></pre><p>调整大小：如果要调整大小/重塑张量，可以使用torch.view：</p><pre><code>x = torch.randn(4, 4)y = x.view(16)z = x.view(-1, 8)  # the size -1 is inferred from other dimensionsprint(x.size(), y.size(), z.size())torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</code></pre><p>如果你有一个元素张量，用于.item()获取值作为Python数字</p><pre><code>x = torch.randn(1)print(x)print(x.item())tensor([ 1.3159])1.3159412145614624</code></pre><h3 id="NumPy-Bridge"><a href="#NumPy-Bridge" class="headerlink" title="NumPy Bridge"></a>NumPy Bridge</h3><p>将Torch Tensor转换为NumPy阵列（反之亦然）是一件轻而易举的事。<br>Torch Tensor和NumPy阵列将共享其底层内存位置，更改一个将改变另一个。</p><hr><ul><li><font color="#00dddd" size="4">将Torch Tensor转换为NumPy数组</font><br>    </li></ul><hr><pre><code>a = torch.ones(5)print(a)tensor([ 1.,  1.,  1.,  1.,  1.])b = a.numpy()print(b)[1. 1. 1. 1. 1.]</code></pre><p>了解numpy数组的值如何变化。</p><pre><code>a.add_(1)print(a)print(b)tensor([ 2.,  2.,  2.,  2.,  2.])[2. 2. 2. 2. 2.]</code></pre><hr><ul><li><font color="#00dddd" size="4">将NumPy数组转换为Torch Tensor</font><br></li></ul><hr><p>了解更改np阵列如何自动更改Torch Tensor</p><pre><code>import numpy as npa = np.ones(5)b = torch.from_numpy(a)np.add(a, 1, out=a)print(a)print(b)[2. 2. 2. 2. 2.]tensor([ 2.,  2.,  2.,  2.,  2.], dtype=torch.float64)</code></pre><p>除了CharTensor之外，CPU上的所有Tensors都支持转换为NumPy并返回。</p><hr><ul><li><font color="#00dddd" size="4">CUDA Tensors</font><br></li></ul><hr><p>可以使用该.to方法将张量移动到任何设备上。</p><pre><code># let us run this cell only if CUDA is available# We will use ``torch.device`` objects to move tensors in and out of GPUif torch.cuda.is_available():    device = torch.device(&quot;cuda&quot;)          # a CUDA device object    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU    x = x.to(device)                       # or just use strings ``.to(&quot;cuda&quot;)``    z = x + y    print(z)    print(z.to(&quot;cpu&quot;, torch.double))       # ``.to`` can also change dtype together!tensor([ 2.3159], device=&apos;cuda:0&apos;)tensor([ 2.3159], dtype=torch.float64)</code></pre><hr><ul><li><font color="#00dddd" size="6">torch</font><br></li></ul><hr><blockquote><p>torch.eye</p></blockquote><p>torch.eye(n, m=None, out=None)<br>返回一个2维张量，对角线位置全1，其它位置全0</p><p>参数:</p><ul><li>n (int ) – 行数</li><li>m (int, optional) – 列数.如果为None,则默认为n</li><li>out (Tensor, optinal) - Output tensor</li></ul><p>返回值: 对角线位置全1，其它位置全0的2维张量</p><p>返回值类型: <font color="#00099ff" size="4">Tensor</font><br></p><p>例子:</p><pre><code>&gt;&gt;&gt; torch.eye(3)1  0  00  1  00  0  1[torch.FloatTensor of size 3x3]</code></pre><blockquote><p>from_numpy</p></blockquote><p>torch.from_numpy(ndarray) → Tensor<br>Numpy桥，将numpy.ndarray 转换为pytorch的 Tensor。 返回的张量tensor和numpy的ndarray共享同一内存空间。修改一个会导致另外一个也被修改。返回的张量不能改变大小。</p><p>例子:</p><pre><code>&gt;&gt;&gt; a = numpy.array([1, 2, 3])&gt;&gt;&gt; t = torch.from_numpy(a)&gt;&gt;&gt; ttorch.LongTensor([1, 2, 3])&gt;&gt;&gt; t[0] = -1&gt;&gt;&gt; aarray([-1,  2,  3])</code></pre><blockquote><p>torch.linspace</p></blockquote><p>torch.linspace(start, end, steps=100, out=None) → Tensor<br>返回一个1维张量，包含在区间start 和 end 上均匀间隔的steps个点。 输出1维张量的长度为steps。</p><p>参数:</p><ul><li>start (float) – 序列的起始点</li><li>end (float) – 序列的最终值</li><li>steps (int) – 在start 和 end间生成的样本数  </li><li>out (Tensor, optional) – 结果张量  </li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; torch.linspace(3, 10, steps=5)3.00004.75006.50008.250010.0000[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.linspace(-10, 10, steps=5)-10-50510[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5)-10-50510[torch.FloatTensor of size 5]</code></pre><blockquote><p>torch.logspace</p></blockquote><p>torch.logspace(start, end, steps=100, out=None) → Tensor<br>返回一个1维张量，包含在区间 10start 和 10end上以对数刻度均匀间隔的steps个点。 输出1维张量的长度为steps。</p><p>参数:</p><ul><li>start (float) – 序列的起始点</li><li>end (float) – 序列的最终值</li><li>steps (int) – 在start 和 end间生成的样本数  </li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5)1.0000e-101.0000e-051.0000e+001.0000e+051.0000e+10[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5)1.25892.11353.54815.956610.0000[torch.FloatTensor of size 5]</code></pre><blockquote><p>torch.ones</p></blockquote><p>torch.ones(*sizes, out=None) → Tensor<br>返回一个全为1 的张量，形状由可变参数sizes定义。</p><p>参数:</p><ul><li>sizes (int…) – 整数序列，定义了输出形状  </li><li>out (Tensor, optional) – 结果张量  </li></ul><p>例子:  </p><pre><code>&gt;&gt;&gt; torch.ones(2, 3)1  1  11  1  1[torch.FloatTensor of size 2x3]&gt;&gt;&gt; torch.ones(5)11111[torch.FloatTensor of size 5]</code></pre><blockquote><p>torch.rand</p></blockquote><p>torch.rand(*sizes, out=None) → Tensor<br>返回一个张量，包含了从区间[0,1)的均匀分布中抽取的一组随机数，形状由可变参数sizes 定义。</p><p>参数:</p><ul><li>sizes (int…) – 整数序列，定义了输出形状  </li><li>out (Tensor, optinal) - 结果张量  </li></ul><p>例子：  </p><pre><code>&gt;&gt;&gt; torch.rand(4)0.91930.33470.32320.7715[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.rand(2, 3)0.5010  0.5140  0.07190.1435  0.5636  0.0538[torch.FloatTensor of size 2x3]</code></pre><blockquote><p>torch.randn</p></blockquote><p>torch.randn(*sizes, out=None) → Tensor<br>返回一个张量，包含了从标准正态分布(均值为0，方差为 1，即高斯白噪声)中抽取一组随机数，形状由可变参数sizes定义。  </p><p>参数:</p><ul><li>sizes (int…) – 整数序列，定义了输出形状</li><li>out (Tensor, optinal) - 结果张量  </li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; torch.randn(4)-0.11450.0094-1.17170.9846[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.randn(2, 3)1.4339  0.3351 -1.09991.5458 -0.9643 -0.3558[torch.FloatTensor of size 2x3]</code></pre><blockquote><p>torch.randperm</p></blockquote><p>torch.randperm(n, out=None) → LongTensor<br>给定参数n，返回一个从0 到n -1 的随机整数排列。</p><p>参数:</p><ul><li>n (int) – 上边界(不包含)  </li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; torch.randperm(4)2130[torch.LongTensor of size 4]</code></pre><blockquote><p>torch.arange</p></blockquote><p>torch.arange(start, end, step=1, out=None) → Tensor<br>返回一个1维张量，长度为 floor((end−start)/step)。包含从start到end，以step为步长的一组序列值(默认步长为1)。</p><p>参数:</p><ul><li>start (float) – 序列的起始点</li><li>end (float) – 序列的终止点</li><li>step (float) – 相邻点的间隔大小</li></ul><p>out (Tensor, optional) – 结果张量<br>例子：</p><pre><code>&gt;&gt;&gt; torch.arange(1, 4)123[torch.FloatTensor of size 3]&gt;&gt;&gt; torch.arange(1, 2.5, 0.5)1.00001.50002.0000[torch.FloatTensor of size 3]</code></pre><blockquote><p>torch.range</p></blockquote><p>torch.range(start, end, step=1, out=None) → Tensor<br>返回一个1维张量，有 floor((end−start)/step)+1 个元素。包含在半开区间[start, end）从start开始，以step为步长的一组值。 step 是两个值之间的间隔，即 xi+1=xi+step</p><font color="#ff0000" face="黑体">警告：建议使用函数 torch.arange()<br></font><br>参数:<br><br><em> start (float) – 序列的起始点</em> end (float) – 序列的最终值<br><em> step (int) – 相邻点的间隔大小<br><br>out (Tensor, optional) – 结果张量<br>例子：<br><br>    &gt;&gt;&gt; torch.range(1, 4)<br><br>    1<br>    2<br>    3<br>    4<br>    [torch.FloatTensor of size 4]<br><br>    &gt;&gt;&gt; torch.range(1, 4, 0.5)<br><br>    1.0000<br>    1.5000<br>    2.0000<br>    2.5000<br>    3.0000<br>    3.5000<br>    4.0000<br>    [torch.FloatTensor of size 7]<br><br>&gt; torch.zeros<br><br>torch.zeros(</em>sizes, out=None) → Tensor<br>返回一个全为标量 0 的张量，形状由可变参数sizes 定义。<br><br>参数:<br><br><em> sizes (int…) – 整数序列，定义了输出形状<br><br>( out (Tensor, optional) – 结果张量<br>例子：<br><br>    &gt;&gt;&gt; torch.zeros(2, 3)<br><br>    0  0  0<br>    0  0  0<br>    [torch.FloatTensor of size 2x3]<br><br>    &gt;&gt;&gt; torch.zeros(5)<br><br>    0<br>    0<br>    0<br>    0<br>    0<br>    [torch.FloatTensor of size 5]<br><br>#### 索引,切片,连接,换位Indexing, Slicing, Joining, Mutating Ops<br>—<br>&gt; torch.cat<br><br>torch.cat(inputs, dimension=0) → Tensor<br>在给定维度上对输入的张量序列seq 进行连接操作。<br><br>torch.cat()可以看做 torch.split() 和 torch.chunk()的反操作。 cat() 函数可以通过下面例子更好的理解。<br><br>参数:</em> inputs (sequence of Tensors) – 可以是任意相同Tensor 类型的python 序列<br><em> dimension (int, optional) – 沿着此维连接张量序列。<br><br>例子：<br><br>    &gt;&gt;&gt; x = torch.randn(2, 3)<br>    &gt;&gt;&gt; x<br><br>    0.5983 -0.0341  2.4918<br>    1.5981 -0.5265 -0.8735<br>    [torch.FloatTensor of size 2x3]<br><br>    &gt;&gt;&gt; torch.cat((x, x, x), 0)<br><br>    0.5983 -0.0341  2.4918<br>    1.5981 -0.5265 -0.8735<br>    0.5983 -0.0341  2.4918<br>    1.5981 -0.5265 -0.8735<br>    0.5983 -0.0341  2.4918<br>    1.5981 -0.5265 -0.8735<br>    [torch.FloatTensor of size 6x3]<br><br>    &gt;&gt;&gt; torch.cat((x, x, x), 1)<br><br>    0.5983 -0.0341  2.4918  0.5983 -0.0341  2.4918  0.5983 -0.0341  2.4918<br>    1.5981 -0.5265 -0.8735  1.5981 -0.5265 -0.8735  1.5981 -0.5265 -0.8735<br>    [torch.FloatTensor of size 2x9]<br><br>&gt;torch.chunk<br><br>torch.chunk(tensor, chunks, dim=0)<br>在给定维度(轴)上将输入张量进行分块儿。<br><br>参数:</em> tensor (Tensor) – 待分块的输入张量<br><em> chunks (int) – 分块的个数</em> dim (int) – 沿着此维度进行分块<br><br>&gt; torch.gather<br><br>torch.gather(input, dim, index, out=None) → Tensor<br>沿给定轴dim，将输入索引张量index指定位置的值进行聚合。<br><br>对一个3维张量，输出可以定义为：<br><br>    out[i][j][k] = tensor[index[i][j][k]][j][k]  # dim=0<br>    out[i][j][k] = tensor[i][index[i][j][k]][k]  # dim=1<br>    out[i][j][k] = tensor[i][j][index[i][j][k]]  # dim=3<br>例子：<br><br>    &gt;&gt;&gt; t = torch.Tensor([[1,2],[3,4]])<br>    &gt;&gt;&gt; torch.gather(t, 1, torch.LongTensor([[0,0],[1,0]]))<br>    1  1<br>    4  3<br>    [torch.FloatTensor of size 2x2]<br>参数:<br><br><em> input (Tensor) – 源张量</em> dim (int) – 索引的轴<br><em> index (LongTensor) – 聚合元素的下标</em> out (Tensor, optional) – 目标张量<br><br>&gt; torch.index_select<br><br>torch.index_select(input, dim, index, out=None) → Tensor<br>沿着指定维度对输入进行切片，取index中指定的相应项(index为一个LongTensor)，然后返回到一个新的张量， 返回的张量与原始张量<em>Tensor</em>有相同的维度(在指定轴上)。<br><br><font color="#ff0000" face="黑体">注意： 返回的张量不与原始张量共享内存空间。<br></font>  <p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 索引的轴</li><li>index (LongTensor) – 包含索引下标的一维张量</li><li>out (Tensor, optional) – 目标张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)&gt;&gt;&gt; x1.2045  2.4084  0.4001  1.13720.5596  1.5677  0.6219 -0.79541.3635 -1.2313 -0.5414 -1.8478[torch.FloatTensor of size 3x4]&gt;&gt;&gt; indices = torch.LongTensor([0, 2])&gt;&gt;&gt; torch.index_select(x, 0, indices)1.2045  2.4084  0.4001  1.13721.3635 -1.2313 -0.5414 -1.8478[torch.FloatTensor of size 2x4]&gt;&gt;&gt; torch.index_select(x, 1, indices)1.2045  0.40010.5596  0.62191.3635 -0.5414[torch.FloatTensor of size 3x2]</code></pre><blockquote><p>torch.masked_select</p></blockquote><p>torch.masked_select(input, mask, out=None) → Tensor<br>根据掩码张量mask中的二元值，取输入张量中的指定项( mask为一个 ByteTensor)，将取值返回到一个新的1D张量，</p><p>张量 mask须跟input张量有相同数量的元素数目，但形状或维度不需要相同。 注意： 返回的张量不与原始张量共享内存空间。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>mask (ByteTensor) – 掩码张量，包含了二元索引值</li><li>out (Tensor, optional) – 目标张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)&gt;&gt;&gt; x1.2045  2.4084  0.4001  1.13720.5596  1.5677  0.6219 -0.79541.3635 -1.2313 -0.5414 -1.8478[torch.FloatTensor of size 3x4]&gt;&gt;&gt; indices = torch.LongTensor([0, 2])&gt;&gt;&gt; torch.index_select(x, 0, indices)1.2045  2.4084  0.4001  1.13721.3635 -1.2313 -0.5414 -1.8478[torch.FloatTensor of size 2x4]&gt;&gt;&gt; torch.index_select(x, 1, indices)1.2045  0.40010.5596  0.62191.3635 -0.5414[torch.FloatTensor of size 3x2]</code></pre><blockquote><p>torch.nonzero</p></blockquote><p>torch.nonzero(input, out=None) → LongTensor<br>返回一个包含输入input中非零元素索引的张量。输出张量中的每行包含输入中非零元素的索引。</p><p>如果输入input有n维，则输出的索引张量output的形状为 z x n, 这里 z 是输入张量input中所有非零元素的个数。</p><p>参数:</p><ul><li>input (Tensor) – 源张量</li><li>out (LongTensor, optional) – 包含索引值的结果张量  </li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; torch.nonzero(torch.Tensor([1, 1, 1, 0, 1]))0124[torch.LongTensor of size 4x1]&gt;&gt;&gt; torch.nonzero(torch.Tensor([[0.6, 0.0, 0.0, 0.0],...                             [0.0, 0.4, 0.0, 0.0],...                             [0.0, 0.0, 1.2, 0.0],...                             [0.0, 0.0, 0.0,-0.4]]))0  01  12  23  3[torch.LongTensor of size 4x2]</code></pre><blockquote><p>torch.split</p></blockquote><p>torch.split(tensor, split_size, dim=0)<br>将输入张量分割成相等形状的chunks（如果可分）。 如果沿指定维的张量形状大小不能被split_size 整分， 则最后一个分块会小于其它分块。</p><p>参数:</p><ul><li>tensor (Tensor) – 待分割张量</li><li>split_size (int) – 单个分块的形状大小</li><li>dim (int) – 沿着此维进行分割</li></ul><blockquote><p>torch.squeeze</p></blockquote><p>torch.squeeze(input, dim=None, out=None)<br>将输入张量形状中的1 去除并返回。 如果输入是形如(A×1×B×1×C×1×D)，那么输出形状就为： (A×B×C×D)<br>当给定dim时，那么挤压操作只在给定维度上。例如，输入形状为: (A×1×B), squeeze(input, 0) 将会保持张量不变，只有用 squeeze(input, 1)，形状会变成 (A×B)。</p><font color="#ff0000" face="黑体">注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。<br></font>  <p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int, optional) – 如果给定，则input只会在给定维度挤压</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.zeros(2,1,2,1,2)&gt;&gt;&gt; x.size()(2L, 1L, 2L, 1L, 2L)&gt;&gt;&gt; y = torch.squeeze(x)&gt;&gt;&gt; y.size()(2L, 2L, 2L)&gt;&gt;&gt; y = torch.squeeze(x, 0)&gt;&gt;&gt; y.size()(2L, 1L, 2L, 1L, 2L)&gt;&gt;&gt; y = torch.squeeze(x, 1)&gt;&gt;&gt; y.size()(2L, 2L, 1L, 2L)</code></pre><blockquote><p>torch.stack[source]</p></blockquote><p>torch.stack(sequence, dim=0)<br>沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。</p><p>参数:</p><ul><li>sqequence (Sequence) – 待连接的张量序列</li><li>dim (int) – 插入的维度。必须介于 0 与 待连接的张量序列数之间。</li></ul><blockquote><p>torch.t</p></blockquote><p>torch.t(input, out=None) → Tensor<br>输入一个矩阵（2维张量），并转置0, 1维。 可以被视为函数transpose(input, 0, 1)的简写函数。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)&gt;&gt;&gt; x0.4834  0.6907  1.3417-0.1300  0.5295  0.2321[torch.FloatTensor of size 2x3]&gt;&gt;&gt; torch.t(x)0.4834 -0.13000.6907  0.52951.3417  0.2321[torch.FloatTensor of size 3x2]</code></pre><blockquote><p>torch.transpose</p></blockquote><p>torch.transpose(input, dim0, dim1, out=None) → Tensor<br>返回输入矩阵input的转置。交换维度dim0和dim1。 输出张量与输入张量共享内存，所以改变其中一个会导致另外一个也被修改。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>dim0 (int) – 转置的第一维</li><li><p>dim1 (int) – 转置的第二维</p><pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)&gt;&gt;&gt; x0.5983 -0.0341  2.49181.5981 -0.5265 -0.8735[torch.FloatTensor of size 2x3]&gt;&gt;&gt; torch.transpose(x, 0, 1)0.5983  1.5981-0.0341 -0.52652.4918 -0.8735[torch.FloatTensor of size 3x2]</code></pre></li></ul><blockquote><p>torch.unbind</p></blockquote><p>torch.unbind(tensor, dim=0)[source]<br>移除指定维后，返回一个元组，包含了沿着指定维切片后的各个切片</p><p>参数:</p><ul><li>tensor (Tensor) – 输入张量</li><li>dim (int) – 删除的维度</li></ul><blockquote><p>torch.unsqueeze</p></blockquote><p>torch.unsqueeze(input, dim, out=None)<br>返回一个新的张量，对输入的制定位置插入维度 1</p><font color="#ff0000" face="黑体">注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。<br></font>  <p>如果dim为负，则将会被转化dim+input.dim()+1</p><p>参数:  </p><ul><li>tensor (Tensor) – 输入张量</li><li>dim (int) – 插入维度的索引</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.Tensor([1, 2, 3, 4])&gt;&gt;&gt; torch.unsqueeze(x, 0)1  2  3  4[torch.FloatTensor of size 1x4]&gt;&gt;&gt; torch.unsqueeze(x, 1)1234[torch.FloatTensor of size 4x1]</code></pre><h4 id="随机抽样-Random-sampling"><a href="#随机抽样-Random-sampling" class="headerlink" title="随机抽样 Random sampling"></a>随机抽样 Random sampling</h4><hr><blockquote><p>torch.manual_seed</p></blockquote><p>torch.manual_seed(seed)<br>设定生成随机数的种子，并返回一个 torch._C.Generator 对象.</p><p>参数: </p><ul><li>seed (int or long) – 种子.</li></ul><blockquote><p>torch.initial_seed</p></blockquote><p>torch.initial_seed()<br>返回生成随机数的原始种子值（python long）。</p><blockquote><p>torch.get_rng_state</p></blockquote><p>torch.get_rng_state()[source]<br>返回随机生成器状态(ByteTensor)</p><blockquote><p>torch.set_rng_state</p></blockquote><p>torch.set_rng_state(new_state)[source]<br>设定随机生成器状态 参数: new_state (torch.ByteTensor) – 期望的状态</p><blockquote><p>torch.default_generator</p></blockquote><p>torch.default_generator = &lt;torch._C.Generator object&gt;</p><blockquote><p>torch.bernoulli</p></blockquote><p>torch.bernoulli(input, out=None) → Tensor<br>从伯努利分布中抽取二元随机数(0 或者 1)。</p><p>输入张量须包含用于抽取上述二元随机值的概率。 因此，输入中的所有值都必须在［0,1］区间，即 0&lt;=inputi&lt;=1<br>输出张量的第i个元素值， 将会以输入张量的第i个概率值等于1。</p><p>返回值将会是与输入相同大小的张量，每个值为0或者1 </p><p>参数:</p><ul><li>input (Tensor) – 输入为伯努利分布的概率值</li><li>out (Tensor, optional) – 输出张量(可选)  </li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.Tensor(3, 3).uniform_(0, 1) # generate a uniform random matrix with range [0, 1]&gt;&gt;&gt; a0.7544  0.8140  0.98420.5282  0.0595  0.64450.1925  0.9553  0.9732[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.bernoulli(a)1  1  10  0  10  1  1[torch.FloatTensor of size 3x3]&gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing &quot;1&quot; is 1&gt;&gt;&gt; torch.bernoulli(a)1  1  11  1  11  1  1[torch.FloatTensor of size 3x3]&gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing &quot;1&quot; is 0&gt;&gt;&gt; torch.bernoulli(a)0  0  00  0  00  0  0[torch.FloatTensor of size 3x3]</code></pre><blockquote><p>torch.multinomial</p></blockquote><p>torch.multinomial(input, num_samples,replacement=False, out=None) → LongTensor<br>返回一个张量，每行包含从input相应行中定义的多项分布中抽取的num_samples个样本。</p><font color="#ff0000" face="黑体"> [注意]:输入input每行的值不需要总和为1 (这里我们用来做权重)，但是必须非负且总和不能为0。<br></font><p>当抽取样本时，依次从左到右排列(第一个样本对应第一列)。</p><p>如果输入input是一个向量，输出out也是一个相同长度num_samples的向量。如果输入input是有 m行的矩阵，输出out是形如m×n的矩阵。</p><p>如果参数replacement 为 True, 则样本抽取可以重复。否则，一个样本在每行不能被重复抽取。</p><p>参数num_samples必须小于input长度(即，input的列数，如果是input是一个矩阵)。</p><p>参数:</p><ul><li>input (Tensor) – 包含概率值的张量</li><li>num_samples (int) – 抽取的样本数</li><li>replacement (bool, optional) – 布尔值，决定是否能重复抽取</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; weights = torch.Tensor([0, 10, 3, 0]) # create a Tensor of weights&gt;&gt;&gt; torch.multinomial(weights, 4)1200[torch.LongTensor of size 4]&gt;&gt;&gt; torch.multinomial(weights, 4, replacement=True)1212[torch.LongTensor of size 4]</code></pre><blockquote><p>torch.normal()</p></blockquote><p>torch.normal(means, std, out=None)<br>返回一个张量，包含从给定参数means,std的离散正态分布中抽取随机数。 均值means是一个张量，包含每个输出元素相关的正态分布的均值。 std是一个张量，包含每个输出元素相关的正态分布的标准差。 均值和标准差的形状不须匹配，但每个张量的元素个数须相同。</p><p>参数:</p><ul><li>means (Tensor) – 均值</li><li>std (Tensor) – 标准差</li><li>out (Tensor) – 可选的输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; torch.normal(means=torch.arange(1, 11), std=torch.arange(1, 0, -0.1))1.51041.69552.48954.91854.98956.91557.36838.18368.71649.8916[torch.FloatTensor of size 10]torch.normal(mean=0.0, std, out=None)</code></pre><p>与上面函数类似，所有抽取的样本共享均值。</p><p>参数:</p><ul><li>means (Tensor,optional) – 所有分布均值</li><li>std (Tensor) – 每个元素的标准差</li><li>out (Tensor) – 可选的输出张量</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; torch.normal(mean=0.5, std=torch.arange(1, 6))0.57230.0871-0.3783-2.568910.7893[torch.FloatTensor of size 5]</code></pre><p>torch.normal(means, std=1.0, out=None)<br>与上面函数类似，所有抽取的样本共享标准差。</p><p>参数:</p><ul><li>means (Tensor) – 每个元素的均值</li><li>std (float, optional) – 所有分布的标准差</li><li>out (Tensor) – 可选的输出张量</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; torch.normal(means=torch.arange(1, 6))1.16812.88843.77182.56164.2500[torch.FloatTensor of size 5]</code></pre><h4 id="序列化-Serialization"><a href="#序列化-Serialization" class="headerlink" title="序列化 Serialization"></a>序列化 Serialization</h4><hr><blockquote><p>torch.saves[source]</p></blockquote><p>torch.save(obj, f, pickle_module=<module 'pickle'="" from="" '="" home="" jenkins="" miniconda="" lib="" python3.5="" pickle.py'="">, pickle_protocol=2)<br>保存一个对象到一个硬盘文件上 参考: Recommended approach for saving a model </module></p><p>参数：</p><ul><li>obj – 保存对象</li><li>f － 类文件对象 (返回文件描述符)或一个保存文件名的字符串</li><li>pickle_module – 用于pickling元数据和对象的模块</li><li>pickle_protocol – 指定pickle protocal 可以覆盖默认参数</li></ul><blockquote><p>torch.load[source]</p></blockquote><p>torch.load(f, map_location=None, pickle_module=<module 'pickle'="" from="" '="" home="" jenkins="" miniconda="" lib="" python3.5="" pickle.py'="">)<br>从磁盘文件中读取一个通过torch.save()保存的对象。<br>torch.load() 可通过参数map_location 动态地进行内存重映射，使其能从不动设备中读取文件。一般调用时，需两个参数: storage 和 location tag. 返回不同地址中的storage，或着返回None (此时地址可以通过默认方法进行解析). 如果这个参数是字典的话，意味着其是从文件的地址标记到当前系统的地址标记的映射。 默认情况下， location tags中 “cpu”对应host tensors，‘cuda:device_id’ (e.g. ‘cuda:2’) 对应cuda tensors。 用户可以通过register_package进行扩展，使用自己定义的标记和反序列化方法。</module></p><p>参数:</p><ul><li>f – 类文件对象 (返回文件描述符)或一个保存文件名的字符串</li><li>map_location – 一个函数或字典规定如何remap存储位置</li><li>pickle_module – 用于unpickling元数据和对象的模块 (必须匹配序列化文件时的pickle_module )</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;)# Load all tensors onto the CPU&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location=lambda storage, loc: storage)# Map tensors from GPU 1 to GPU 0&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location={&apos;cuda:1&apos;:&apos;cuda:0&apos;})</code></pre><h4 id="并行化-Parallelism"><a href="#并行化-Parallelism" class="headerlink" title="并行化 Parallelism"></a>并行化 Parallelism</h4><hr><blockquote><p>torch.get_num_threads</p></blockquote><p>torch.get_num_threads() → int<br>获得用于并行化CPU操作的OpenMP线程数</p><blockquote><p>torch.set_num_threads</p></blockquote><p>torch.set_num_threads(int)<br>设定用于并行化CPU操作的OpenMP线程数</p><h4 id="数学操作Math-operations"><a href="#数学操作Math-operations" class="headerlink" title="数学操作Math operations"></a>数学操作Math operations</h4><hr><h5 id="Pointwise-Ops"><a href="#Pointwise-Ops" class="headerlink" title="Pointwise Ops"></a>Pointwise Ops</h5><blockquote><p>torch.abs</p></blockquote><p>torch.abs(input, out=None) → Tensor<br>计算输入张量的每个元素绝对值</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.abs(torch.FloatTensor([-1, -2, 3]))FloatTensor([1, 2, 3])</code></pre><p>torch.acos(input, out=None) → Tensor<br>torch.acos(input, out=None) → Tensor</p><p>返回一个新张量，包含输入张量每个元素的反余弦。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.acos(a)2.26081.29561.1075    nan[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.add()</p></blockquote><p>torch.add(input, value, out=None)<br>对输入张量input逐元素加上标量值value，并返回结果到一个新的张量out，即 out=tensor+value。</p><p>如果输入input是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】</p><ul><li>input (Tensor) – 输入张量</li><li>value (Number) – 添加到输入每个元素的数</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a0.4050-1.22271.8688-0.4185[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.add(a, 20)20.405018.777321.868819.5815[torch.FloatTensor of size 4]</code></pre><blockquote><blockquote><p>torch.add(input, value=1, other, out=None)</p></blockquote></blockquote><p>other 张量的每个元素乘以一个标量值value，并加到iput 张量上。返回结果到输出张量out。即，out=input+(other∗value)</p><p>两个张量 input and other的尺寸不需要匹配，但元素总数必须一样。</p><p>注意 :当两个张量形状不匹配时，输入张量的形状会作为输出张量的尺寸。</p><p>如果other是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】</p><p>参数:</p><ul><li>input (Tensor) – 第一个输入张量</li><li>value (Number) – 用于第二个张量的尺寸因子</li><li>other (Tensor) – 第二个输入张量</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; import torch&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.93102.03300.0852-0.2941[torch.FloatTensor of size 4]&gt;&gt;&gt; b = torch.randn(2, 2)&gt;&gt;&gt; b1.0663  0.2544-0.1513  0.0749[torch.FloatTensor of size 2x2]&gt;&gt;&gt; torch.add(a, 10, b)9.73224.5770-1.42790.4552[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.addcdiv</p></blockquote><p>torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None) → Tensor<br>用tensor2对tensor1逐元素相除，然后乘以标量值value 并加到tensor。</p><p>张量的形状不需要匹配，但元素数量必须一致。</p><p>如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。</p><p>参数：</p><ul><li>tensor (Tensor) – 张量，对 tensor1 ./ tensor 进行相加</li><li>value (Number, optional) – 标量，对 tensor1 ./ tensor2 进行相乘</li><li>tensor1 (Tensor) – 张量，作为被除数(分子)</li><li>tensor2 (Tensor) –张量，作为除数(分母)</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; t = torch.randn(2, 3)&gt;&gt;&gt; t1 = torch.randn(1, 6)&gt;&gt;&gt; t2 = torch.randn(6, 1)&gt;&gt;&gt; torch.addcdiv(t, 0.1, t1, t2)0.0122 -0.0188 -0.23540.7396 -1.5721  1.2878[torch.FloatTensor of size 2x3]</code></pre><blockquote><p>torch.addcmul</p></blockquote><p>torch.addcmul(tensor, value=1, tensor1, tensor2, out=None) → Tensor<br>用tensor2对tensor1逐元素相乘，并对结果乘以标量值value然后加到tensor。 张量的形状不需要匹配，但元素数量必须一致。 如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。</p><p>参数：</p><ul><li>tensor (Tensor) – 张量，对tensor1 ./ tensor 进行相加</li><li>value (Number, optional) – 标量，对 tensor1 . tensor2 进行相乘</li><li>tensor1 (Tensor) – 张量，作为乘子1</li><li>tensor2 (Tensor) –张量，作为乘子2</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; t = torch.randn(2, 3)&gt;&gt;&gt; t1 = torch.randn(1, 6)&gt;&gt;&gt; t2 = torch.randn(6, 1)&gt;&gt;&gt; torch.addcmul(t, 0.1, t1, t2)0.0122 -0.0188 -0.23540.7396 -1.5721  1.2878[torch.FloatTensor of size 2x3]</code></pre><blockquote><p>torch.asin</p></blockquote><p>torch.asin(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的反正弦函数</p><p>参数：</p><ul><li>tensor (Tensor) – 输入张量</li><li>nout (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.asin(a)-0.69000.27520.4633    nan[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.atan</p></blockquote><p>torch.atan(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的反正切函数</p><p>参数：</p><ul><li>tensor (Tensor) – 输入张量</li></ul><p>out (Tensor, optional) – 输出张量</p><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.atan(a)-0.56690.26530.42030.9196[torch.FloatTensor of size 4]</code></pre><ul><li>torch.atan2</li></ul><p>torch.atan2(input1, input2, out=None) → Tensor<br>返回一个新张量，包含两个输入张量input1和input2的反正切函数</p><p>参数：</p><ul><li>input1 (Tensor) – 第一个输入张量</li><li>input2 (Tensor) – 第二个输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.atan2(a, torch.randn(4))-2.41672.97550.93631.6613[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.ceil</p></blockquote><p>torch.ceil(input, out=None) → Tensor<br>天井函数，对输入input张量每个元素向上取整, 即取不小于每个元素的最小整数，并返回结果到输出。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.38690.3912-0.8634-0.5468[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.ceil(a)21-0-0[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.clamp</p></blockquote><p>torch.clamp(input, min, max, out=None) → Tensor<br>将输入input张量每个元素的夹紧到区间 $[min,max]$，并返回结果到一个新张量。</p><p>操作定义如下：</p><pre><code>      | min, if x_i &lt; miny_i = | x_i, if min &lt;= x_i &lt;= max      | max, if x_i &gt; max</code></pre><p>如果输入是FloatTensor or DoubleTensor类型，则参数min max 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，min， max取整数、实数皆可。】</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>min (Number) – 限制范围下限</li><li>max (Number) – 限制范围上限</li><li>Nout (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.38690.3912-0.8634-0.5468[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.clamp(a, min=-0.5, max=0.5)0.50000.3912-0.5000-0.5000[torch.FloatTensor of size 4]</code></pre><blockquote><blockquote><p>torch.clamp(input, *, min, out=None) → Tensor</p></blockquote></blockquote><p>将输入input张量每个元素的限制到不小于min ，并返回结果到一个新张量。</p><p>如果输入是FloatTensor or DoubleTensor类型，则参数 min 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，min取整数、实数皆可。】</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>value (Number) – 限制范围下限</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.38690.3912-0.8634-0.5468[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.clamp(a, min=0.5)1.38690.50000.50000.5000[torch.FloatTensor of size 4]</code></pre><blockquote><blockquote><p>torch.clamp(input, *, max, out=None) → Tensor</p></blockquote></blockquote><p>将输入input张量每个元素的限制到不大于max ，并返回结果到一个新张量。</p><p>如果输入是FloatTensor or DoubleTensor类型，则参数 max 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，max取整数、实数皆可。】</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>value (Number) – 限制范围上限</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.38690.3912-0.8634-0.5468[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.clamp(a, max=0.5)0.50000.3912-0.8634-0.5468[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.cos</p></blockquote><p>torch.cos(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的余弦。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.cos(a)0.80410.96330.90180.2557[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.cosh</p></blockquote><p>torch.cosh(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的双曲余弦。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.cosh(a)1.20951.03721.10151.9917[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.div()</p></blockquote><p>torch.div(input, value, out=None)<br>将input逐元素除以标量值value，并返回结果到输出张量out。 即 out=tensor/value<br>如果输入是FloatTensor or DoubleTensor类型，则参数 value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>value (Number) – 除数</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(5)&gt;&gt;&gt; a-0.6147-1.1237-0.1604-0.68530.1063[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.div(a, 0.5)-1.2294-2.2474-0.3208-1.37060.2126[torch.FloatTensor of size 5]</code></pre><blockquote><blockquote><p>torch.div(input, other, out=None)</p></blockquote></blockquote><p>两张量input和other逐元素相除，并将结果返回到输出。即， outi=inputi/otheri<br>两张量形状不须匹配，但元素数须一致。</p><font color="#ff0000" face="黑体">注意：当形状不匹配时，input的形状作为输出张量的形状。<br></font><p>参数：</p><ul><li>input (Tensor) – 张量(分子)</li><li>other (Tensor) – 张量(分母)</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4,4)&gt;&gt;&gt; a-0.1810  0.4017  0.2863 -0.10130.6183  2.0696  0.9012 -1.59330.5679  0.4743 -0.0117 -0.1266-0.1213  0.9629  0.2682  1.5968[torch.FloatTensor of size 4x4]&gt;&gt;&gt; b = torch.randn(8, 2)&gt;&gt;&gt; b0.8774  0.76500.8866  1.4805-0.6490  1.11721.4259 -0.81461.4633 -0.12280.4643 -0.60290.3492  1.52701.6103 -0.6291[torch.FloatTensor of size 8x2]&gt;&gt;&gt; torch.div(a, b)-0.2062  0.5251  0.3229 -0.0684-0.9528  1.8525  0.6320  1.95590.3881 -3.8625 -0.0253  0.2099-0.3473  0.6306  0.1666 -2.5381[torch.FloatTensor of size 4x4]</code></pre><blockquote><p>torch.exp</p></blockquote><p>torch.exp(tensor, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的指数。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li></ul><p>out (Tensor, optional) – 输出张量</p><pre><code>&gt;&gt;&gt; torch.exp(torch.Tensor([0, math.log(2)]))torch.FloatTensor([1, 2])</code></pre><blockquote><p>torch.floor</p></blockquote><p>torch.floor(input, out=None) → Tensor<br>床函数: 返回一个新张量，包含输入input张量每个元素的floor，即不小于元素的最大整数。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.38690.3912-0.8634-0.5468[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.floor(a)10-1-1[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.fmod</p></blockquote><p>torch.fmod(input, divisor, out=None) → Tensor<br>计算除法余数。 除数与被除数可能同时含有整数和浮点数。此时，余数的正负与被除数相同。</p><p>参数：</p><ul><li>input (Tensor) – 被除数</li><li>divisor (Tensor or float) – 除数，一个数或与被除数相同类型的张量  </li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; torch.fmod(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2)torch.FloatTensor([-1, -0, -1, 1, 0, 1])&gt;&gt;&gt; torch.fmod(torch.Tensor([1, 2, 3, 4, 5]), 1.5)torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5])</code></pre><p>参考: torch.remainder(), 计算逐元素余数， 相当于python 中的 % 操作符。</p><blockquote><p>torch.frac</p></blockquote><p>torch.frac(tensor, out=None) → Tensor<br>返回每个元素的分数部分。</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.frac(torch.Tensor([1, 2.5, -3.2])torch.FloatTensor([0, 0.5, -0.2])</code></pre><blockquote><p>torch.lerp</p></blockquote><p>torch.lerp(start, end, weight, out=None)<br>对两个张量以start，end做线性插值， 将结果返回到输出张量。<br>即，outi=starti+weight∗(endi−starti)</p><p>参数：</p><ul><li>start (Tensor) – 起始点张量</li><li>end (Tensor) – 终止点张量</li><li>weight (float) – 插值公式的weight</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; start = torch.arange(1, 5)&gt;&gt;&gt; end = torch.Tensor(4).fill_(10)&gt;&gt;&gt; start1234[torch.FloatTensor of size 4]&gt;&gt;&gt; end10101010[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.lerp(start, end, 0.5)5.50006.00006.50007.0000[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.log</p></blockquote><p>torch.log(input, out=None) → Tensor<br>计算input 的自然对数</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(5)&gt;&gt;&gt; a-0.41830.3722-0.30910.41490.5857[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.log(a)    nan-0.9883    nan-0.8797-0.5349[torch.FloatTensor of size 5]</code></pre><blockquote><p>torch.log1p</p></blockquote><p>torch.log1p(input, out=None) → Tensor<br>计算 input+1的自然对数 yi=log(xi+1)  </p><font color="#ff0000" face="黑体">注意：对值比较小的输入，此函数比torch.log()更准确。<br></font><p>如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(5)&gt;&gt;&gt; a-0.41830.3722-0.30910.41490.5857[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.log1p(a)-0.54180.3164-0.36970.34710.4611[torch.FloatTensor of size 5]</code></pre><blockquote><p>torch.mul</p></blockquote><p>torch.mul(input, value, out=None)<br>用标量值value乘以输入input的每个元素，并返回一个新的结果张量。 out=tensor∗value<br>如果输入是FloatTensor or DoubleTensor类型，则value 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，value取整数、实数皆可。】</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>value (Number) – 乘到每个元素的数</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(3)&gt;&gt;&gt; a-0.9374-0.5254-0.6069[torch.FloatTensor of size 3]&gt;&gt;&gt; torch.mul(a, 100)-93.7411-52.5374-60.6908[torch.FloatTensor of size 3]</code></pre><blockquote><blockquote><p>torch.mul(input, other, out=None)</p></blockquote></blockquote><p>两个张量input,other按元素进行相乘，并返回到输出张量。即计算outi=inputi∗otheri<br>两计算张量形状不须匹配，但总元素数须一致。 注意：当形状不匹配时，input的形状作为输入张量的形状。</p><p>参数：</p><ul><li>input (Tensor) – 第一个相乘张量</li><li>other (Tensor) – 第二个相乘张量</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4,4)&gt;&gt;&gt; a-0.7280  0.0598 -1.4327 -0.5825-0.1427 -0.0690  0.0821 -0.3270-0.9241  0.5110  0.4070 -1.1188-0.8308  0.7426 -0.6240 -1.1582[torch.FloatTensor of size 4x4]&gt;&gt;&gt; b = torch.randn(2, 8)&gt;&gt;&gt; b0.0430 -1.0775  0.6015  1.1647 -0.6549  0.0308 -0.1670  1.0742-1.2593  0.0292 -0.0849  0.4530  1.2404 -0.4659 -0.1840  0.5974[torch.FloatTensor of size 2x8]&gt;&gt;&gt; torch.mul(a, b)-0.0313 -0.0645 -0.8618 -0.67840.0934 -0.0021 -0.0137 -0.35131.1638  0.0149 -0.0346 -0.5068-1.0304 -0.3460  0.1148 -0.6919[torch.FloatTensor of size 4x4]</code></pre><blockquote><p>torch.neg</p></blockquote><p>torch.neg(input, out=None) → Tensor<br>返回一个新张量，包含输入input 张量按元素取负。 即， out=−1∗input</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(5)&gt;&gt;&gt; a-0.44301.1690-0.8836-0.45650.2968[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.neg(a)0.4430-1.16900.88360.4565-0.2968[torch.FloatTensor of size 5]</code></pre><blockquote><p>torch.pow</p></blockquote><p>torch.pow(input, exponent, out=None)<br>对输入input的按元素求exponent次幂值，并返回结果张量。 幂值exponent 可以为单一 float 数或者与input相同元素数的张量。</p><p>当幂值为标量时，执行操作：<br>outi=xexponent  </p><p>当幂值为张量时，执行操作：<br>outi=xexponenti</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>exponent (float or Tensor) – 幂值</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.5274-0.8232-2.11281.7558[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.pow(a, 2)0.27810.67764.46403.0829[torch.FloatTensor of size 4]&gt;&gt;&gt; exp = torch.arange(1, 5)&gt;&gt;&gt; a = torch.arange(1, 5)&gt;&gt;&gt; a1234[torch.FloatTensor of size 4]&gt;&gt;&gt; exp1234[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.pow(a, exp)1427256[torch.FloatTensor of size 4]</code></pre><blockquote><blockquote><p>torch.pow(base, input, out=None)</p></blockquote></blockquote><p>base 为标量浮点值,input为张量， 返回的输出张量 out 与输入张量相同形状。</p><p>执行操作为:<br>outi=baseinputi</p><p>参数：</p><ul><li>base (float) – 标量值，指数的底</li><li>input ( Tensor) – 幂值</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; exp = torch.arange(1, 5)&gt;&gt;&gt; base = 2&gt;&gt;&gt; torch.pow(base, exp)24816[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.reciprocal</p></blockquote><p>torch.reciprocal(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的倒数，即 1.0/x。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.38690.3912-0.8634-0.5468[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.reciprocal(a)0.72102.5565-1.1583-1.8289[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.remainder</p></blockquote><p>torch.remainder(input, divisor, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的除法余数。 除数与被除数可能同时包含整数或浮点数。余数与除数有相同的符号。</p><p>参数：</p><ul><li>input (Tensor) – 被除数</li><li>divisor (Tensor or float) – 除数，一个数或者与除数相同大小的张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; torch.remainder(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2)torch.FloatTensor([1, 0, 1, 1, 0, 1])&gt;&gt;&gt; torch.remainder(torch.Tensor([1, 2, 3, 4, 5]), 1.5)torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5])</code></pre><p>参考: 函数torch.fmod() 同样可以计算除法余数，相当于 C 的 库函数fmod()</p><ul><li>torch.round</li></ul><p>torch.round(input, out=None) → Tensor<br>返回一个新张量，将输入input张量每个元素舍入到最近的整数。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.22901.3409-0.5662-0.0899[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.round(a)11-1-0[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.rsqrt</p></blockquote><p>torch.rsqrt(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的平方根倒数。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.22901.3409-0.5662-0.0899[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.rsqrt(a)0.90200.8636    nan    nan[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.sigmoid</p></blockquote><p>torch.sigmoid(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的sigmoid值。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.49721.35120.1056-0.2650[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.sigmoid(a)0.37820.79430.52640.4341[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.sign</p></blockquote><p>torch.sign(input, out=None) → Tensor<br>符号函数：返回一个新张量，包含输入input张量每个元素的正负。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.sign(a)-1111[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.sin</p></blockquote><p>torch.sin(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的正弦。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.sin(a)-0.59440.26840.43220.9667[torch.FloatTensor of size 4]</code></pre><ul><li>torch.sinh</li></ul><p>torch.sinh(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的双曲正弦。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.sinh(a)-0.68040.27510.46191.7225[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.sqrt</p></blockquote><p>torch.sqrt(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的平方根。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.22901.3409-0.5662-0.0899[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.sqrt(a)1.10861.1580    nan    nan[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.tan</p></blockquote><p>torch.tan(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的正切。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.tan(a)-0.73920.27860.47923.7801[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.tanh</p></blockquote><p>torch.tanh(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的双曲正切。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.63660.27180.44691.3122[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.tanh(a)-0.56250.26530.41930.8648[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.trunc</p></blockquote><p>torch.trunc(input, out=None) → Tensor<br>返回一个新张量，包含输入input张量每个元素的截断值(标量x的截断值是最接近其的整数，其比x更接近零。简而言之，有符号数的小数部分被舍弃)。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a-0.49721.35120.1056-0.2650[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.trunc(a)-010-0[torch.FloatTensor of size 4]</code></pre><h5 id="Reduction-Ops"><a href="#Reduction-Ops" class="headerlink" title="Reduction Ops"></a>Reduction Ops</h5><blockquote><p>torch.cumprod</p></blockquote><p>torch.cumprod(input, dim, out=None) → Tensor<br>返回输入沿指定维度的累积积。例如，如果输入是一个N 元向量，则结果也是一个N 元向量，第i 个输出元素值为$ yi=x1∗x2∗x3∗…∗xi $</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 累积积操作的维度</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(10)&gt;&gt;&gt; a1.11481.84231.4143-0.44031.2859-1.2514-0.47481.1735-1.6332-0.4272[torch.FloatTensor of size 10]&gt;&gt;&gt; torch.cumprod(a, dim=0)1.11482.05372.9045-1.2788-1.64442.0578-0.9770-1.14661.8726-0.8000[torch.FloatTensor of size 10]&gt;&gt;&gt; a[5] = 0.0&gt;&gt;&gt; torch.cumprod(a, dim=0)1.11482.05372.9045-1.2788-1.6444-0.00000.00000.0000-0.00000.0000[torch.FloatTensor of size 10]</code></pre><blockquote><p>torch.cumsum</p></blockquote><p>torch.cumsum(input, dim, out=None) → Tensor<br>返回输入沿指定维度的累积和。例如，如果输入是一个N元向量，则结果也是一个N元向量，第i 个输出元素值为 yi=x1+x2+x3+…+xi</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 累积和操作的维度</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(10)&gt;&gt;&gt; a-0.6039-0.2214-0.3705-0.01691.3415-0.12300.97190.6081-0.12861.0947[torch.FloatTensor of size 10]&gt;&gt;&gt; torch.cumsum(a, dim=0)-0.6039-0.8253-1.1958-1.21270.12880.00580.97771.58581.45722.5519[torch.FloatTensor of size 10]</code></pre><blockquote><p>torch.dist</p></blockquote><p>torch.dist(input, other, p=2, out=None) → Tensor<br>返回 (input - other) 的 p范数 。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>other (Tensor) – 右侧输入张量</li><li>p (float, optional) – 所计算的范数</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.randn(4)&gt;&gt;&gt; x0.2505-0.4571-0.37330.7807[torch.FloatTensor of size 4]&gt;&gt;&gt; y = torch.randn(4)&gt;&gt;&gt; y0.7782-0.51851.4106-2.4063[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.dist(x, y, 3.5)3.302832063224223&gt;&gt;&gt; torch.dist(x, y, 3)3.3677282206393286&gt;&gt;&gt; torch.dist(x, y, 0)inf&gt;&gt;&gt; torch.dist(x, y, 1)5.560028076171875</code></pre><blockquote><p>torch.mean</p></blockquote><p>torch.mean(input) → float<br>返回输入张量所有元素的均值。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; a-0.2946 -0.9143  2.1809[torch.FloatTensor of size 1x3]&gt;&gt;&gt; torch.mean(a)0.32398951053619385</code></pre><blockquote><blockquote><p>torch.mean(input, dim, out=None) → Tensor</p></blockquote></blockquote><p>返回输入张量给定维度dim上每行的均值。</p><p>输出形状与输入相同，除了给定维度上为1.</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – the dimension to reduce</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; a-1.2738 -0.3058  0.1230 -1.96150.8771 -0.5430 -0.9233  0.98791.4107  0.0317 -0.6823  0.2255-1.3854  0.4953 -0.2160  0.2435[torch.FloatTensor of size 4x4]&gt;&gt;&gt; torch.mean(a, 1)-0.85450.09970.2464-0.2157[torch.FloatTensor of size 4x1]</code></pre><blockquote><p>torch.median</p></blockquote><p>torch.median(input, dim=-1, values=None, indices=None) -&gt; (Tensor, LongTensor)<br>返回输入张量给定维度每行的中位数，同时返回一个包含中位数的索引的LongTensor。</p><p>dim值默认为输入张量的最后一维。 输出形状与输入相同，除了给定维度上为1.</p><font color="#ff0000" face="黑体">注意: 这个函数还没有在torch.cuda.Tensor中定义<br></font><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 缩减的维度</li><li>values (Tensor, optional) – 结果张量</li><li>indices (Tensor, optional) – 返回的索引结果张量  </li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a-0.6891 -0.66620.2697  0.74120.5254 -0.74020.5528 -0.2399[torch.FloatTensor of size 4x2]&gt;&gt;&gt; a = torch.randn(4, 5)&gt;&gt;&gt; a0.4056 -0.3372  1.0973 -2.4884  0.43342.1336  0.3841  0.1404 -0.1821 -0.7646-0.2403  1.3975 -2.0068  0.1298  0.0212-1.5371 -0.7257 -0.4871 -0.2359 -1.1724[torch.FloatTensor of size 4x5]&gt;&gt;&gt; torch.median(a, 1)(0.40560.14040.0212-0.7257[torch.FloatTensor of size 4x1],0241[torch.LongTensor of size 4x1])</code></pre><blockquote><p>torch.mode</p></blockquote><p>torch.mode(input, dim=-1, values=None, indices=None) -&gt; (Tensor, LongTensor)<br>返回给定维dim上，每行的众数值。 同时返回一个LongTensor，包含众数职的索引。dim值默认为输入张量的最后一维。</p><p>输出形状与输入相同，除了给定维度上为1.</p><font color="#ff0000" face="黑体">注意: 这个函数还没有在torch.cuda.Tensor中定义<br></font><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 缩减的维度</li><li>values (Tensor, optional) – 结果张量</li><li>indices (Tensor, optional) – 返回的索引张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a-0.6891 -0.66620.2697  0.74120.5254 -0.74020.5528 -0.2399[torch.FloatTensor of size 4x2]&gt;&gt;&gt; a = torch.randn(4, 5)&gt;&gt;&gt; a0.4056 -0.3372  1.0973 -2.4884  0.43342.1336  0.3841  0.1404 -0.1821 -0.7646-0.2403  1.3975 -2.0068  0.1298  0.0212-1.5371 -0.7257 -0.4871 -0.2359 -1.1724[torch.FloatTensor of size 4x5]&gt;&gt;&gt; torch.mode(a, 1)(-2.4884-0.7646-2.0068-1.5371[torch.FloatTensor of size 4x1],3420[torch.LongTensor of size 4x1])</code></pre><blockquote><p>torch.norm</p></blockquote><p>torch.norm(input, p=2) → float<br>返回输入张量input 的p 范数。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>p (float,optional) – 范数计算中的幂指数值</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; a-0.4376 -0.5328  0.9547[torch.FloatTensor of size 1x3]&gt;&gt;&gt; torch.norm(a, 3)1.0338925067372466</code></pre><blockquote><blockquote><p>torch.norm(input, p, dim, out=None) → Tensor</p></blockquote></blockquote><p>返回输入张量给定维dim 上每行的p 范数。 输出形状与输入相同，除了给定维度上为1.</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>p (float) – 范数计算中的幂指数值</li><li>dim (int) – 缩减的维度</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4, 2)&gt;&gt;&gt; a-0.6891 -0.66620.2697  0.74120.5254 -0.74020.5528 -0.2399[torch.FloatTensor of size 4x2]&gt;&gt;&gt; torch.norm(a, 2, 1)0.95850.78880.90770.6026[torch.FloatTensor of size 4x1]&gt;&gt;&gt; torch.norm(a, 0, 1)2222[torch.FloatTensor of size 4x1]</code></pre><blockquote><p>torch.prod</p></blockquote><p>torch.prod(input) → float<br>返回输入张量input 所有元素的积。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; a0.6170  0.3546  0.0253[torch.FloatTensor of size 1x3]&gt;&gt;&gt; torch.prod(a)0.005537458061418483</code></pre><blockquote><blockquote><p>orch.prod(input, dim, out=None) → Tensor</p></blockquote></blockquote><p>返回输入张量给定维度上每行的积。 输出形状与输入相同，除了给定维度上为1.</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 缩减的维度</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4, 2)&gt;&gt;&gt; a0.1598 -0.6884-0.1831 -0.4412-0.9925 -0.6244-0.2416 -0.8080[torch.FloatTensor of size 4x2]&gt;&gt;&gt; torch.prod(a, 1)-0.11000.08080.61970.1952[torch.FloatTensor of size 4x1]</code></pre><blockquote><p>torch.std</p></blockquote><p>torch.std(input) → float<br>返回输入张量input 所有元素的标准差。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; a-1.3063  1.4182 -0.3061[torch.FloatTensor of size 1x3]&gt;&gt;&gt; torch.std(a)1.3782334731508061</code></pre><blockquote><blockquote><p>torch.std(input, dim, out=None) → Tensor</p></blockquote></blockquote><p>返回输入张量给定维度上每行的标准差。 输出形状与输入相同，除了给定维度上为1.</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 缩减的维度</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; a0.1889 -2.4856  0.0043  1.8169-0.7701 -0.4682 -2.2410  0.40980.1919 -1.1856 -1.0361  0.90850.0173  1.0662  0.2143 -0.5576[torch.FloatTensor of size 4x4]&gt;&gt;&gt; torch.std(a, dim=1)1.77561.10251.00450.6725[torch.FloatTensor of size 4x1]</code></pre><blockquote><p>torch.sum</p></blockquote><p>torch.sum(input) → float<br>返回输入张量input 所有元素的和。</p><p>输出形状与输入相同，除了给定维度上为1.</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; a0.6170  0.3546  0.0253[torch.FloatTensor of size 1x3]&gt;&gt;&gt; torch.sum(a)0.9969287421554327</code></pre><blockquote><blockquote><p>torch.sum(input, dim, out=None) → Tensor</p></blockquote></blockquote><p>返回输入张量给定维度上每行的和。 输出形状与输入相同，除了给定维度上为1.</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 缩减的维度</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; a-0.4640  0.0609  0.1122  0.4784-1.3063  1.6443  0.4714 -0.7396-1.3561 -0.1959  1.0609 -1.98552.6833  0.5746 -0.5709 -0.4430[torch.FloatTensor of size 4x4]&gt;&gt;&gt; torch.sum(a, 1)0.18740.0698-2.47672.2440[torch.FloatTensor of size 4x1]</code></pre><blockquote><p>torch.var</p></blockquote><p>torch.var(input) → float<br>返回输入张量所有元素的方差</p><p>输出形状与输入相同，除了给定维度上为1.</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; a-1.3063  1.4182 -0.3061[torch.FloatTensor of size 1x3]&gt;&gt;&gt; torch.var(a)1.899527506513334</code></pre><blockquote><blockquote><p>torch.var(input, dim, out=None) → Tensor</p></blockquote></blockquote><p>返回输入张量给定维度上每行的方差。 输出形状与输入相同，除了给定维度上为1.</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – the dimension to reduce</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; a-1.2738 -0.3058  0.1230 -1.96150.8771 -0.5430 -0.9233  0.98791.4107  0.0317 -0.6823  0.2255-1.3854  0.4953 -0.2160  0.2435[torch.FloatTensor of size 4x4]&gt;&gt;&gt; torch.var(a, 1)0.88590.95090.75480.6949[torch.FloatTensor of size 4x1]</code></pre><h4 id="比较操作-Comparison-Ops"><a href="#比较操作-Comparison-Ops" class="headerlink" title="比较操作 Comparison Ops"></a>比较操作 Comparison Ops</h4><hr><blockquote><p>torch.eq</p></blockquote><p>torch.eq(input, other, out=None) → Tensor<br>比较元素相等性。第二个参数可为一个数或与第一个参数同类型形状的张量。</p><p>参数：</p><ul><li>input (Tensor) – 待比较张量</li><li>other (Tensor or float) – 比较张量或数</li><li>out (Tensor, optional) – 输出张量，须为 ByteTensor类型 or 与input同类型</li></ul><p>返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(相等为1，不等为0 )</p><p>返回类型： Tensor</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.eq(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))1  00  1[torch.ByteTensor of size 2x2]</code></pre><blockquote><p>torch.equal</p></blockquote><p>torch.equal(tensor1, tensor2) → bool<br>如果两个张量有相同的形状和元素值，则返回True ，否则 False。</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.equal(torch.Tensor([1, 2]), torch.Tensor([1, 2]))True</code></pre><blockquote><p>torch.ge</p></blockquote><p>torch.ge(input, other, out=None) → Tensor<br>逐元素比较input和other，即是否 input&gt;=other。</p><p>如果两个张量有相同的形状和元素值，则返回True ，否则 False。 第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p><p>参数:</p><ul><li>input (Tensor) – 待对比的张量</li><li>other (Tensor or float) – 对比的张量或float值</li><li>out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。<br>返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;= other )。  </li></ul><p>返回类型： Tensor</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.ge(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))1  10  1[torch.ByteTensor of size 2x2]</code></pre><blockquote><p>torch.gt</p></blockquote><p>torch.gt(input, other, out=None) → Tensor<br>逐元素比较input和other ， 即是否input&gt;other 如果两个张量有相同的形状和元素值，则返回True ，否则 False。 第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p><p>参数:</p><ul><li>input (Tensor) – 要对比的张量</li><li>other (Tensor or float) – 要对比的张量或float值</li><li>out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。<br>返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;= other )。 返回类型： Tensor</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; torch.gt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))0  10  0[torch.ByteTensor of size 2x2]</code></pre><blockquote><p>torch.kthvalue</p></blockquote><p>torch.kthvalue(input, k, dim=None, out=None) -&gt; (Tensor, LongTensor)<br>取输入张量input指定维上第k 个最小值。如果不指定dim，则默认为input的最后一维。</p><p>返回一个元组 (values,indices)，其中indices是原始输入张量input中沿dim维的第 k 个最小值下标。</p><p>参数:</p><ul><li>input (Tensor) – 要对比的张量</li><li>k (int) – 第 k 个最小值</li><li>dim (int, optional) – 沿着此维进行排序</li><li>out (tuple, optional) – 输出元组 (Tensor, LongTensor) 可选地给定作为 输出 buffers</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.arange(1, 6)&gt;&gt;&gt; x12345[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.kthvalue(x, 4)(4[torch.FloatTensor of size 1],3[torch.LongTensor of size 1])</code></pre><blockquote><p>torch.le</p></blockquote><p>torch.le(input, other, out=None) → Tensor<br>逐元素比较input和other ， 即是否input&lt;=other 第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p><p>参数:</p><ul><li>input (Tensor) – 要对比的张量</li><li>other (Tensor or float ) – 对比的张量或float值</li><li>out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。<br>返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;= other )。</li></ul><p>返回类型： Tensor</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.le(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))1  01  1[torch.ByteTensor of size 2x2]</code></pre><blockquote><p>torch.lt</p></blockquote><p>torch.lt(input, other, out=None) → Tensor<br>逐元素比较input和other ， 即是否 input&lt;other</p><p>第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p><p>参数:</p><ul><li>input (Tensor) – 要对比的张量</li><li>other (Tensor or float ) – 对比的张量或float值</li><li>out (Tensor, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。</li></ul><p>input： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 tensor &gt;= other )。 </p><p>返回类型： Tensor</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.lt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))0  01  0[torch.ByteTensor of size 2x2]</code></pre><blockquote><p>torch.max</p></blockquote><p>torch.max()<br>返回输入张量所有元素的最大值。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; a0.4729 -0.2266 -0.2085[torch.FloatTensor of size 1x3]&gt;&gt;&gt; torch.max(a)0.4729</code></pre><blockquote><blockquote><p>torch.max(input, dim, max=None, max_indices=None) -&gt; (Tensor, LongTensor)</p></blockquote></blockquote><p>返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引。</p><p>输出形状中，将dim维设定为1，其它与输入形状保持一致。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 指定的维度</li><li>max (Tensor, optional) – 结果张量，包含给定维度上的最大值</li><li>max_indices (LongTensor, optional) – 结果张量，包含给定维度上每个最大值的位置索引</li></ul><p>例子：</p><pre><code>&gt;&gt; a = torch.randn(4, 4)&gt;&gt; a0.0692  0.3142  1.2513 -0.54280.9288  0.8552 -0.2073  0.64091.0695 -0.0101 -2.4507 -1.22300.7426 -0.7666  0.4862 -0.6628torch.FloatTensor of size 4x4]&gt;&gt;&gt; torch.max(a, 1)(1.25130.92881.06950.7426[torch.FloatTensor of size 4x1],2000[torch.LongTensor of size 4x1])</code></pre><blockquote><p>torch.max(input, other, out=None) → Tensor</p></blockquote><p>返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引。 即，outi=max(inputi,otheri)<br>输出形状中，将dim维设定为1，其它与输入形状保持一致。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>other (Tensor) – 输出张量</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.38690.3912-0.8634-0.5468[torch.FloatTensor of size 4]&gt;&gt;&gt; b = torch.randn(4)&gt;&gt;&gt; b1.0067-0.80100.62580.3627[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.max(a, b)1.38690.39120.62580.3627[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.min</p></blockquote><p>torch.min(input) → float<br>返回输入张量所有元素的最小值。</p><p>参数: </p><ul><li>input (Tensor) – 输入张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; a0.4729 -0.2266 -0.2085[torch.FloatTensor of size 1x3]&gt;&gt;&gt; torch.min(a)-0.22663167119026184</code></pre><blockquote><blockquote><p>torch.min(input, dim, min=None, min_indices=None) -&gt; (Tensor, LongTensor)</p></blockquote></blockquote><p>返回输入张量给定维度上每行的最小值，并同时返回每个最小值的位置索引。</p><p>输出形状中，将dim维设定为1，其它与输入形状保持一致。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 指定的维度</li><li>min (Tensor, optional) – 结果张量，包含给定维度上的最小值</li><li>min_indices (LongTensor, optional) – 结果张量，包含给定维度上每个最小值的位置索引</li></ul><p>例子：</p><pre><code>&gt;&gt; a = torch.randn(4, 4)&gt;&gt; a0.0692  0.3142  1.2513 -0.54280.9288  0.8552 -0.2073  0.64091.0695 -0.0101 -2.4507 -1.22300.7426 -0.7666  0.4862 -0.6628torch.FloatTensor of size 4x4]&gt;&gt; torch.min(a, 1)0.54280.20732.45070.7666torch.FloatTensor of size 4x1]3221torch.LongTensor of size 4x1]</code></pre><blockquote><blockquote><p>torch.min(input, other, out=None) → Tensor</p></blockquote></blockquote><p>input中逐元素与other相应位置的元素对比，返回最小值到输出张量。即，outi=min(tensori,otheri)<br>两张量形状不需匹配，但元素数须相同。</p><font color="#ff0000" face="黑体">注意：当形状不匹配时，input的形状作为返回张量的形状。<br></font><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>other (Tensor) – 第二个输入张量</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4)&gt;&gt;&gt; a1.38690.3912-0.8634-0.5468[torch.FloatTensor of size 4]&gt;&gt;&gt; b = torch.randn(4)&gt;&gt;&gt; b1.0067-0.80100.62580.3627[torch.FloatTensor of size 4]&gt;&gt;&gt; torch.min(a, b)1.0067-0.8010-0.8634-0.5468[torch.FloatTensor of size 4]</code></pre><blockquote><p>torch.ne</p></blockquote><p>torch.ne(input, other, out=None) → Tensor<br>逐元素比较input和other ， 即是否 input!=other。 第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p><p>参数:</p><ul><li>input (Tensor) – 待对比的张量</li><li>other (Tensor or float) – 对比的张量或float值</li><li>out (Tensor, optional) – 输出张量。必须为ByteTensor或者与input相同类型。</li></ul><p>返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果 (如果 tensor != other 为True ，返回1)。</p><p>返回类型： Tensor</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.ne(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))0  11  0[torch.ByteTensor of size 2x2]</code></pre><blockquote><p>torch.sort</p></blockquote><p>torch.sort(input, dim=None, descending=False, out=None) -&gt; (Tensor, LongTensor)<br>对输入张量input沿着指定维按升序排序。如果不给定dim，则默认为输入的最后一维。如果指定参数descending为True，则按降序排序</p><p>返回元组 (sorted_tensor, sorted_indices) ， sorted_indices 为原始输入中的下标。</p><p>参数:</p><ul><li>input (Tensor) – 要对比的张量</li><li>dim (int, optional) – 沿着此维排序</li><li>descending (bool, optional) – 布尔值，控制升降排序</li><li>out (tuple, optional) – 输出张量。必须为ByteTensor或者与第一个参数tensor相同类型。</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)&gt;&gt;&gt; sorted, indices = torch.sort(x)&gt;&gt;&gt; sorted-1.6747  0.0610  0.1190  1.4137-1.4782  0.7159  1.0341  1.3678-0.3324 -0.0782  0.3518  0.4763[torch.FloatTensor of size 3x4]&gt;&gt;&gt; indices0  1  3  22  1  0  33  1  0  2[torch.LongTensor of size 3x4]&gt;&gt;&gt; sorted, indices = torch.sort(x, 0)&gt;&gt;&gt; sorted-1.6747 -0.0782 -1.4782 -0.33240.3518  0.0610  0.4763  0.11901.0341  0.7159  1.4137  1.3678[torch.FloatTensor of size 3x4]&gt;&gt;&gt; indices0  2  1  22  0  2  01  1  0  1[torch.LongTensor of size 3x4]</code></pre><blockquote><p>torch.topk</p></blockquote><p>torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor)<br>沿给定dim维度返回输入张量input中 k 个最大值。 如果不指定dim，则默认为input的最后一维。 如果为largest为 False ，则返回最小的 k 个值。</p><p>返回一个元组 (values,indices)，其中indices是原始输入张量input中测元素下标。 如果设定布尔值sorted 为<em>True</em>，将会确保返回的 k 个值被排序。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>k (int) – “top-k”中的k</li><li>dim (int, optional) – 排序的维</li><li>largest (bool, optional) – 布尔值，控制返回最大或最小值</li><li>sorted (bool, optional) – 布尔值，控制返回值是否排序</li><li>out (tuple, optional) – 可选输出张量 (Tensor, LongTensor) output buffers</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.arange(1, 6)&gt;&gt;&gt; x12345[torch.FloatTensor of size 5]&gt;&gt;&gt; torch.topk(x, 3)(543[torch.FloatTensor of size 3],432[torch.LongTensor of size 3])&gt;&gt;&gt; torch.topk(x, 3, 0, largest=False)(123[torch.FloatTensor of size 3],012[torch.LongTensor of size 3])</code></pre><h4 id="其它操作-Other-Operations"><a href="#其它操作-Other-Operations" class="headerlink" title="其它操作 Other Operations"></a>其它操作 Other Operations</h4><hr><blockquote><p>torch.cross</p></blockquote><p>torch.cross(input, other, dim=-1, out=None) → Tensor<br>返回沿着维度dim上，两个张量input和other的向量积（叉积）。 input和other 必须有相同的形状，且指定的dim维上size必须为3。</p><p>如果不指定dim，则默认为第一个尺度为3的维。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>other (Tensor) – 第二个输入张量</li><li>dim (int, optional) – 沿着此维进行叉积操作</li><li>out (Tensor,optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(4, 3)&gt;&gt;&gt; a-0.6652 -1.0116 -0.68570.2286  0.4446 -0.52720.0476  0.2321  1.99910.6199  1.1924 -0.9397[torch.FloatTensor of size 4x3]&gt;&gt;&gt; b = torch.randn(4, 3)&gt;&gt;&gt; b-0.1042 -1.1156  0.19470.9947  0.1149  0.4701-1.0108  0.8319 -0.07500.9045 -1.3754  1.0976[torch.FloatTensor of size 4x3]&gt;&gt;&gt; torch.cross(a, b, dim=1)-0.9619  0.2009  0.63670.2696 -0.6318 -0.4160-1.6805 -2.0171  0.27410.0163 -1.5304 -1.9311[torch.FloatTensor of size 4x3]&gt;&gt;&gt; torch.cross(a, b)-0.9619  0.2009  0.63670.2696 -0.6318 -0.4160-1.6805 -2.0171  0.27410.0163 -1.5304 -1.9311[torch.FloatTensor of size 4x3]</code></pre><ul><li>torch.diag</li></ul><p>torch.diag(input, diagonal=0, out=None) → Tensor<br>如果输入是一个向量(1D 张量)，则返回一个以input为对角线元素的2D方阵<br>如果输入是一个矩阵(2D 张量)，则返回一个包含input对角线元素的1D张量<br>参数diagonal指定对角线:</p><p>diagonal = 0, 主对角线<br>diagonal &gt; 0, 主对角线之上<br>diagonal &lt; 0, 主对角线之下  </p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>diagonal (int, optional) – 指定对角线</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><p>取得以input为对角线的方阵：</p><pre><code>&gt;&gt;&gt; a = torch.randn(3)&gt;&gt;&gt; a1.0480-2.3405-1.1138[torch.FloatTensor of size 3]&gt;&gt;&gt; torch.diag(a)1.0480  0.0000  0.00000.0000 -2.3405  0.00000.0000  0.0000 -1.1138[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.diag(a, 1)0.0000  1.0480  0.0000  0.00000.0000  0.0000 -2.3405  0.00000.0000  0.0000  0.0000 -1.11380.0000  0.0000  0.0000  0.0000[torch.FloatTensor of size 4x4]</code></pre><p>取得给定矩阵第k个对角线:</p><pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)&gt;&gt;&gt; a-1.5328 -1.3210 -1.52040.8596  0.0471 -0.2239-0.6617  0.0146 -1.0817[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.diag(a, 0)-1.53280.0471-1.0817[torch.FloatTensor of size 3]&gt;&gt;&gt; torch.diag(a, 1)-1.3210-0.2239[torch.FloatTensor of size 2]</code></pre><blockquote><p>torch.histc</p></blockquote><p>torch.histc(input, bins=100, min=0, max=0, out=None) → Tensor<br>计算输入张量的直方图。以min和max为range边界，将其均分成bins个直条，然后将排序好的数据划分到各个直条(bins)中。如果min和max都为0, 则利用数据中的最大最小值作为边界。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>bins (int) – 直方图 bins(直条)的个数(默认100个)</li><li>min (int) – range的下边界(包含)</li><li>max (int) – range的上边界(包含)</li><li>out (Tensor, optional) – 结果张量</li></ul><p>返回： 直方图 </p><p>返回类型：张量</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.histc(torch.FloatTensor([1, 2, 1]), bins=4, min=0, max=3)FloatTensor([0, 2, 1, 0])</code></pre><blockquote><p>torch.renorm</p></blockquote><p>torch.renorm(input, p, dim, maxnorm, out=None) → Tensor<br>返回一个张量，包含规范化后的各个子张量，使得沿着dim维划分的各子张量的p范数小于maxnorm。</p><font color="#ff0000" face="黑体">注意 如果p范数的值小于maxnorm，则当前子张量不需要修改。<br></font><font color="#ff0000" face="黑体">注意: 更详细解释参考torch7 以及Hinton et al. 2012, p. 2<br></font><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>p (float) – 范数的p</li><li>dim (int) – 沿着此维切片，得到张量子集</li><li>maxnorm (float) – 每个子张量的范数的最大值</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.ones(3, 3)&gt;&gt;&gt; x[1].fill_(2)&gt;&gt;&gt; x[2].fill_(3)&gt;&gt;&gt; x1  1  12  2  23  3  3[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.renorm(x, 1, 0, 5)1.0000  1.0000  1.00001.6667  1.6667  1.66671.6667  1.6667  1.6667[torch.FloatTensor of size 3x3]</code></pre><blockquote><p>torch.trace</p></blockquote><p>torch.trace(input) → float<br>返回输入2维矩阵对角线元素的和(迹)</p><p>例子：</p><pre><code>&gt;&gt;&gt; x = torch.arange(1, 10).view(3, 3)&gt;&gt;&gt; x1  2  34  5  67  8  9[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.trace(x)15.0</code></pre><blockquote><p>torch.tril</p></blockquote><p>torch.tril(input, k=0, out=None) → Tensor<br>返回一个张量out，包含输入矩阵(2D张量)的下三角部分，out其余部分被设为0。这里所说的下三角部分为矩阵指定对角线diagonal之上的元素。</p><p>参数k控制对角线:</p><ul><li>k = 0, 主对角线</li><li>k &gt; 0, 主对角线之上</li><li>k &lt; 0, 主对角线之下</li></ul><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>k (int, optional) – 指定对角线</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(3,3)&gt;&gt;&gt; a1.3225  1.7304  1.4573-0.3052 -0.3111 -0.18091.2469  0.0064 -1.6250[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.tril(a)1.3225  0.0000  0.0000-0.3052 -0.3111  0.00001.2469  0.0064 -1.6250[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.tril(a, k=1)1.3225  1.7304  0.0000-0.3052 -0.3111 -0.18091.2469  0.0064 -1.6250[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.tril(a, k=-1)0.0000  0.0000  0.0000-0.3052  0.0000  0.00001.2469  0.0064  0.0000[torch.FloatTensor of size 3x3]</code></pre><blockquote><p>torch.triu</p></blockquote><p>torch.triu(input, k=0, out=None) → Tensor<br>返回一个张量，包含输入矩阵(2D张量)的上三角部分，其余部分被设为0。这里所说的上三角部分为矩阵指定对角线diagonal之上的元素。</p><p>参数k控制对角线:</p><ul><li>k = 0, 主对角线</li><li>k &gt; 0, 主对角线之上</li><li>k &lt; 0, 主对角线之下</li></ul><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>k (int, optional) – 指定对角线</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.randn(3,3)&gt;&gt;&gt; a1.3225  1.7304  1.4573-0.3052 -0.3111 -0.18091.2469  0.0064 -1.6250[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.triu(a)1.3225  1.7304  1.45730.0000 -0.3111 -0.18090.0000  0.0000 -1.6250[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.triu(a, k=1)0.0000  1.7304  1.45730.0000  0.0000 -0.18090.0000  0.0000  0.0000[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.triu(a, k=-1)1.3225  1.7304  1.4573-0.3052 -0.3111 -0.18090.0000  0.0064 -1.6250[torch.FloatTensor of size 3x3]</code></pre><h4 id="BLAS-and-LAPACK-Operations"><a href="#BLAS-and-LAPACK-Operations" class="headerlink" title="BLAS and LAPACK Operations"></a>BLAS and LAPACK Operations</h4><blockquote><p>torch.addbmm</p></blockquote><p>torch.addbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) → Tensor</p><p>对两个批batch1和batch2内存储的矩阵进行批矩阵乘操作，附带reduced add 步骤( 所有矩阵乘结果沿着第一维相加)。矩阵mat加到最终结果。 batch1和 batch2都为包含相同数量矩阵的3维张量。 如果batch1是形为b×n×m的张量，batch1是形为b×m×p的张量，则out和mat的形状都是n×p，即 res=(beta∗M)+(alpha∗sum(batch1i@batch2i,i=0,b))<br>对类型为 FloatTensor 或 DoubleTensor 的输入，alphaand beta必须为实数，否则两个参数须为整数。</p><p>参数：</p><ul><li>beta (Number, optional) – 用于mat的乘子</li><li>mat (Tensor) – 相加矩阵</li><li>alpha (Number, optional) – 用于batch1@batch2的乘子</li><li>batch1 (Tensor) – 第一批相乘矩阵</li><li>batch2 (Tensor) – 第二批相乘矩阵</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; M = torch.randn(3, 5)&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)&gt;&gt;&gt; torch.addbmm(M, batch1, batch2)-3.1162  11.0071   7.3102   0.1824  -7.68921.8265   6.0739   0.4589  -0.5641  -5.4283-9.3387  -0.1794  -1.2318  -6.8841  -4.7239[torch.FloatTensor of size 3x5]</code></pre><blockquote><p>torch.addmm</p></blockquote><p>torch.addmm(beta=1, mat, alpha=1, mat1, mat2, out=None) → Tensor</p><p>对矩阵mat1和mat2进行矩阵乘操作。矩阵mat加到最终结果。如果mat1 是一个 n×m张量，mat2 是一个 m×p张量，那么out和mat的形状为n×p。 alpha 和 beta 分别是两个矩阵 mat1@mat2和mat的比例因子，即， out=(beta∗M)+(alpha∗mat1@mat2)</p><p>对类型为 FloatTensor 或 DoubleTensor 的输入，betaand alpha必须为实数，否则两个参数须为整数。</p><p>参数 ：</p><ul><li>beta (Number, optional) – 用于mat的乘子</li><li>mat (Tensor) – 相加矩阵</li><li>alpha (Number, optional) – 用于mat1@mat2的乘子</li><li>mat1 (Tensor) – 第一个相乘矩阵</li><li>mat2 (Tensor) – 第二个相乘矩阵</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; M = torch.randn(2, 3)&gt;&gt;&gt; mat1 = torch.randn(2, 3)&gt;&gt;&gt; mat2 = torch.randn(3, 3)&gt;&gt;&gt; torch.addmm(M, mat1, mat2)-0.4095 -1.9703  1.35615.7674 -4.9760  2.7378[torch.FloatTensor of size 2x3]</code></pre><blockquote><p>torch.addmv</p></blockquote><p>torch.addmv(beta=1, tensor, alpha=1, mat, vec, out=None) → Tensor</p><p>对矩阵mat和向量vec对进行相乘操作。向量tensor加到最终结果。如果mat 是一个 n×m维矩阵，vec 是一个 m维向量，那么out和mat的为n元向量。 可选参数<em>alpha</em> 和 beta 分别是 mat∗vec和mat的比例因子，即， out=(beta∗tensor)+(alpha∗(mat@vec))</p><p>对类型为<em>FloatTensor</em>或<em>DoubleTensor</em>的输入，alphaand beta必须为实数，否则两个参数须为整数。</p><p>参数 ：</p><ul><li>beta (Number, optional) – 用于mat的乘子</li><li>mat (Tensor) – 相加矩阵</li><li>alpha (Number, optional) – 用于mat1@vec的乘子</li><li>mat (Tensor) – 相乘矩阵</li><li>vec (Tensor) – 相乘向量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; M = torch.randn(2)&gt;&gt;&gt; mat = torch.randn(2, 3)&gt;&gt;&gt; vec = torch.randn(3)&gt;&gt;&gt; torch.addmv(M, mat, vec)-2.0939-2.2950[torch.FloatTensor of size 2]</code></pre><blockquote><p>torch.addr</p></blockquote><p>torch.addr(beta=1, mat, alpha=1, vec1, vec2, out=None) → Tensor<br>对向量vec1和vec2对进行张量积操作。矩阵mat加到最终结果。如果vec1 是一个 n维向量，vec2 是一个 m维向量，那么矩阵mat的形状须为n×m。 可选参数<em>beta</em> 和 alpha 分别是两个矩阵 mat和 vec1@vec2的比例因子，即，resi=(beta∗Mi)+(alpha∗batch1i×batch2i)</p><p>对类型为<em>FloatTensor</em>或<em>DoubleTensor</em>的输入，alphaand beta必须为实数，否则两个参数须为整数。</p><p>参数 ：</p><ul><li>beta (Number, optional) – 用于mat的乘子</li><li>mat (Tensor) – 相加矩阵</li><li>alpha (Number, optional) – 用于两向量vec1，vec2外积的乘子</li><li>vec1 (Tensor) – 第一个相乘向量</li><li>vec2 (Tensor) – 第二个相乘向量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; vec1 = torch.arange(1, 4)&gt;&gt;&gt; vec2 = torch.arange(1, 3)&gt;&gt;&gt; M = torch.zeros(3, 2)&gt;&gt;&gt; torch.addr(M, vec1, vec2)1  22  43  6[torch.FloatTensor of size 3x2]</code></pre><blockquote><p>torch.baddbmm</p></blockquote><p>torch.baddbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) → Tensor</p><p>对两个批batch1和batch2内存储的矩阵进行批矩阵乘操作，矩阵mat加到最终结果。 batch1和  batch2都为包含相同数量矩阵的3维张量。 如果batch1是形为b×n×m的张量，batch1是形为b×m×p的张量，则out和mat的形状都是n×p，即 resi=(beta∗Mi)+(alpha∗batch1i×batch2i)<br>对类型为<em>FloatTensor</em>或<em>DoubleTensor</em>的输入，alphaand beta必须为实数，否则两个参数须为整数。</p><p>参数：</p><ul><li>beta (Number, optional) – 用于mat的乘子</li><li>mat (Tensor) – 相加矩阵</li><li>alpha (Number, optional) – 用于batch1@batch2的乘子</li><li>batch1 (Tensor) – 第一批相乘矩阵</li><li>batch2 (Tensor) – 第二批相乘矩阵</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; M = torch.randn(10, 3, 5)&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)&gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size()torch.Size([10, 3, 5])</code></pre><blockquote><p>torch.bmm</p></blockquote><p>torch.bmm(batch1, batch2, out=None) → Tensor<br>对存储在两个批batch1和batch2内的矩阵进行批矩阵乘操作。batch1和 batch2都为包含相同数量矩阵的3维张量。 如果batch1是形为b×n×m的张量，batch1是形为b×m×p的张量，则out和mat的形状都是n×p，即 res=(beta∗M)+(alpha∗sum(batch1i@batch2i,i=0,b))<br>对类型为 FloatTensor 或 DoubleTensor 的输入，alphaand beta必须为实数，否则两个参数须为整数。</p><p>参数：</p><ul><li>batch1 (Tensor) – 第一批相乘矩阵</li><li>batch2 (Tensor) – 第二批相乘矩阵</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)&gt;&gt;&gt; res = torch.bmm(batch1, batch2)&gt;&gt;&gt; res.size()torch.Size([10, 3, 5])</code></pre><blockquote><p>torch.btrifact</p></blockquote><p>torch.btrifact(A, info=None) → Tensor, IntTensor<br>返回一个元组，包含LU 分解和pivots 。 可选参数info决定是否对每个minibatch样本进行分解。info are from dgetrf and a non-zero value indicates an error occurred. 如果用CUDA的话，这个值来自于CUBLAS，否则来自LAPACK。</p><p>参数： </p><ul><li>A (Tensor) – 待分解张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)&gt;&gt;&gt; A_LU = A.btrifact()</code></pre><blockquote><p>torch.btrisolve</p></blockquote><p>torch.btrisolve(b, LU_data, LU_pivots) → Tensor<br>返回线性方程组Ax=b的LU解。</p><p>参数：</p><ul><li>b (Tensor) – RHS 张量.</li><li>LU_data (Tensor) – Pivoted LU factorization of A from btrifact.</li><li>LU_pivots (IntTensor) – LU 分解的Pivots.</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)&gt;&gt;&gt; b = torch.randn(2, 3)&gt;&gt;&gt; A_LU = torch.btrifact(A)&gt;&gt;&gt; x = b.btrisolve(*A_LU)&gt;&gt;&gt; torch.norm(A.bmm(x.unsqueeze(2)) - b)6.664001874625056e-08</code></pre><blockquote><p>torch.dot</p></blockquote><p>torch.dot(tensor1, tensor2) → float<br>计算两个张量的点乘(内乘),两个张量都为1-D 向量.</p><p>例子：</p><pre><code>&gt;&gt;&gt; torch.dot(torch.Tensor([2, 3]), torch.Tensor([2, 1]))7.0</code></pre><blockquote><p>torch.eig</p></blockquote><p>torch.eig(a, eigenvectors=False, out=None) -&gt; (Tensor, Tensor)<br>计算实方阵a 的特征值和特征向量</p><p>参数：</p><ul><li>a (Tensor) – 方阵，待计算其特征值和特征向量</li><li>eigenvectors (bool) – 布尔值，如果True，则同时计算特征值和特征向量，否则只计算特征值。</li><li>out (tuple, optional) – 输出元组</li></ul><p>返回值：<br>元组，包括：</p><ul><li>e (Tensor): a 的右特征向量</li><li>v (Tensor): 如果eigenvectors为True，则为包含特征向量的张量; 否则为空张量</li></ul><p>返回值类型： (Tensor, Tensor)</p><blockquote><p>torch.gels</p></blockquote><p>torch.gels(B, A, out=None) → Tensor<br>对形如m×n的满秩矩阵a计算其最小二乘和最小范数问题的解。 如果m&gt;=n,gels对最小二乘问题进行求解，即：<br>minimize∥AX−B∥F<br>如果m&lt;n,gels求解最小范数问题，即：<br>minimize∥X∥Fsubject toabAX=B<br>返回矩阵X的前n 行包含解。余下的行包含以下残差信息: 相应列从第n 行开始计算的每列的欧式距离。</p><font color="#ff0000" face="黑体">注意： 返回矩阵总是被转置，无论输入矩阵的原始布局如何，总会被转置；即，总是有 stride (1, m) 而不是 (m, 1).<br></font><p>参数：</p><ul><li>B (Tensor) – 矩阵B</li><li>A (Tensor) – m×n矩阵</li><li>out (tuple, optional) – 输出元组</li></ul><p>返回值： 元组，包括：</p><ul><li>X (Tensor): 最小二乘解</li><li>qr (Tensor): QR 分解的细节</li></ul><p>返回值类型： (Tensor, Tensor)</p><p>例子：</p><pre><code>&gt;&gt;&gt; A = torch.Tensor([[1, 1, 1],...                   [2, 3, 4],...                   [3, 5, 2],...                   [4, 2, 5],...                   [5, 4, 3]])&gt;&gt;&gt; B = torch.Tensor([[-10, -3],                    [ 12, 14],                    [ 14, 12],                    [ 16, 16],                    [ 18, 16]])&gt;&gt;&gt; X, _ = torch.gels(B, A)&gt;&gt;&gt; X2.0000  1.00001.0000  1.00001.0000  2.0000[torch.FloatTensor of size 3x2]</code></pre><blockquote><p>torch.geqrf</p></blockquote><p>torch.geqrf(input, out=None) -&gt; (Tensor, Tensor)</p><p>这是一个直接调用LAPACK的底层函数。 一般使用torch.qr()</p><p>计算输入的QR 分解，但是并不会分别创建Q,R两个矩阵，而是直接调用LAPACK 函数 Rather, this directly calls the underlying LAPACK function ?geqrf which produces a sequence of ‘elementary reflectors’.</p><p>参考 LAPACK文档获取更详细信息。</p><p>参数:</p><ul><li>input (Tensor) – 输入矩阵</li><li>out (tuple, optional) – 元组，包含输出张量 (Tensor, Tensor)</li></ul><blockquote><p>torch.ger</p></blockquote><p>torch.ger(vec1, vec2, out=None) → Tensor<br>计算两向量vec1,vec2的张量积。如果vec1的长度为n,vec2长度为m，则输出out应为形如n x m的矩阵。</p><p>参数:</p><ul><li>vec1 (Tensor) – 1D 输入向量</li><li>vec2 (Tensor) – 1D 输入向量</li><li>out (tuple, optional) – 输出张量</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; v1 = torch.arange(1, 5)&gt;&gt;&gt; v2 = torch.arange(1, 4)&gt;&gt;&gt; torch.ger(v1, v2)1   2   32   4   63   6   94   8  12[torch.FloatTensor of size 4x3]</code></pre><blockquote><p>torch.gesv</p></blockquote><p>torch.gesv(B, A, out=None) -&gt; (Tensor, Tensor)</p><p>X,LU=torch.gesv(B,A)，返回线性方程组AX=B的解。</p><p>LU 包含两个矩阵L，U。A须为非奇异方阵，如果A是一个m×m矩阵，B 是m×k矩阵，则LU 是m×m矩阵， X为m×k矩阵</p><p>参数：</p><ul><li>B (Tensor) – m×k矩阵</li><li>A (Tensor) – m×m矩阵</li><li>out (Tensor, optional) – 可选地输出矩阵X</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; A = torch.Tensor([[6.80, -2.11,  5.66,  5.97,  8.23],...                   [-6.05, -3.30,  5.36, -4.44,  1.08],...                   [-0.45,  2.58, -2.70,  0.27,  9.04],...                   [8.32,  2.71,  4.35,  -7.17,  2.14],...                   [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()&gt;&gt;&gt; B = torch.Tensor([[4.02,  6.19, -8.22, -7.57, -3.03],...                   [-1.56,  4.00, -8.67,  1.75,  2.86],...                   [9.81, -4.09, -4.57, -8.61,  8.99]]).t()&gt;&gt;&gt; X, LU = torch.gesv(B, A)&gt;&gt;&gt; torch.dist(B, torch.mm(A, X))9.250057093890353e-06</code></pre><blockquote><p>torch.inverse</p></blockquote><p>torch.inverse(input, out=None) → Tensor</p><p>对方阵输入input 取逆。</p><font color="#ff0000" face="黑体">注意： 返回矩阵总是被转置，无论输入矩阵的原始布局如何，总会被转置；即，总是有 stride (1, m) 而不是 (m, 1).<br></font><p>参数 ：</p><ul><li>input (Tensor) – 输入2维张量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; x = torch.rand(10, 10)&gt;&gt;&gt; x0.7800  0.2267  0.7855  0.9479  0.5914  0.7119  0.4437  0.9131  0.1289  0.19820.0045  0.0425  0.2229  0.4626  0.6210  0.0207  0.6338  0.7067  0.6381  0.81960.8350  0.7810  0.8526  0.9364  0.7504  0.2737  0.0694  0.5899  0.8516  0.38830.6280  0.6016  0.5357  0.2936  0.7827  0.2772  0.0744  0.2627  0.6326  0.91530.7897  0.0226  0.3102  0.0198  0.9415  0.9896  0.3528  0.9397  0.2074  0.69800.5235  0.6119  0.6522  0.3399  0.3205  0.5555  0.8454  0.3792  0.4927  0.60860.1048  0.0328  0.5734  0.6318  0.9802  0.4458  0.0979  0.3320  0.3701  0.09090.2616  0.3485  0.4370  0.5620  0.5291  0.8295  0.7693  0.1807  0.0650  0.84970.1655  0.2192  0.6913  0.0093  0.0178  0.3064  0.6715  0.5101  0.2561  0.33960.4370  0.4695  0.8333  0.1180  0.4266  0.4161  0.0699  0.4263  0.8865  0.2578[torch.FloatTensor of size 10x10]&gt;&gt;&gt; x = torch.rand(10, 10)&gt;&gt;&gt; y = torch.inverse(x)&gt;&gt;&gt; z = torch.mm(x, y)&gt;&gt;&gt; z1.0000  0.0000  0.0000 -0.0000  0.0000  0.0000  0.0000  0.0000 -0.0000 -0.00000.0000  1.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000 -0.0000 -0.0000 -0.00000.0000  0.0000  1.0000 -0.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000 -0.00000.0000  0.0000  0.0000  1.0000  0.0000  0.0000  0.0000 -0.0000 -0.0000  0.00000.0000  0.0000 -0.0000 -0.0000  1.0000  0.0000  0.0000 -0.0000 -0.0000 -0.00000.0000  0.0000  0.0000 -0.0000  0.0000  1.0000 -0.0000 -0.0000 -0.0000 -0.00000.0000  0.0000  0.0000 -0.0000  0.0000  0.0000  1.0000  0.0000 -0.0000  0.00000.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000  1.0000 -0.0000  0.0000-0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000 -0.0000  1.0000 -0.0000-0.0000  0.0000 -0.0000 -0.0000 -0.0000  0.0000 -0.0000 -0.0000  0.0000  1.0000[torch.FloatTensor of size 10x10]&gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(10))) # Max nonzero5.096662789583206e-07</code></pre><blockquote><p>torch.mm</p></blockquote><p>torch.mm(mat1, mat2, out=None) → Tensor</p><p>对矩阵mat1和mat2进行相乘。 如果mat1 是一个n×m 张量，mat2 是一个 m×p 张量，将会输出一个 n×p 张量out。</p><p>参数 ：</p><ul><li>mat1 (Tensor) – 第一个相乘矩阵</li><li>mat2 (Tensor) – 第二个相乘矩阵</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; mat1 = torch.randn(2, 3)&gt;&gt;&gt; mat2 = torch.randn(3, 3)&gt;&gt;&gt; torch.mm(mat1, mat2)0.0519 -0.3304  1.22324.3910 -5.1498  2.7571[torch.FloatTensor of size 2x3]</code></pre><blockquote><p>torch.mv</p></blockquote><p>torch.mv(mat, vec, out=None) → Tensor</p><p>对矩阵mat和向量vec进行相乘。 如果mat 是一个n×m张量，vec 是一个m元 1维张量，将会输出一个n 元 1维张量。</p><p>参数 ：</p><ul><li>mat (Tensor) – 相乘矩阵</li><li>vec (Tensor) – 相乘向量</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; mat = torch.randn(2, 3)&gt;&gt;&gt; vec = torch.randn(3)&gt;&gt;&gt; torch.mv(mat, vec)-2.0939-2.2950[torch.FloatTensor of size 2]</code></pre><hr><pre><code>torch.orgqrtorch.orgqr()torch.ormqrtorch.ormqr()torch.potrftorch.potrf()torch.potritorch.potri()torch.potrstorch.potrs()torch.pstrftorch.pstrf()</code></pre><hr><blockquote><p>torch.qr</p></blockquote><p>torch.qr(input, out=None) -&gt; (Tensor, Tensor)</p><p>计算输入矩阵的QR分解：返回两个矩阵q ,r， 使得 x=q∗r ，这里q 是一个半正交矩阵与 r 是一个上三角矩阵</p><p>本函数返回一个thin(reduced)QR分解。</p><font color="#ff0000" face="黑体">注意 如果输入很大，可能可能会丢失精度。<br></font><font color="#ff0000" face="黑体">注意 本函数依赖于你的LAPACK实现，虽然总能返回一个合法的分解，但不同平台可能得到不同的结果。<br></font><p>参数：</p><ul><li>input (Tensor) – 输入的2维张量</li><li>out (tuple, optional) – 输出元组tuple，包含Q和R</li></ul><p>例子:</p><pre><code>&gt;&gt;&gt; a = torch.Tensor([[12, -51, 4], [6, 167, -68], [-4, 24, -41]])&gt;&gt;&gt; q, r = torch.qr(a)&gt;&gt;&gt; q-0.8571  0.3943  0.3314-0.4286 -0.9029 -0.03430.2857 -0.1714  0.9429[torch.FloatTensor of size 3x3]&gt;&gt;&gt; r-14.0000  -21.0000   14.00000.0000 -175.0000   70.00000.0000    0.0000  -35.0000[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.mm(q, r).round()12  -51    46  167  -68-4   24  -41[torch.FloatTensor of size 3x3]&gt;&gt;&gt; torch.mm(q.t(), q).round()1 -0  0-0  1  00  0  1[torch.FloatTensor of size 3x3]</code></pre><blockquote><p>torch.svd</p></blockquote><p>torch.svd(input, some=True, out=None) -&gt; (Tensor, Tensor, Tensor)</p><p>U,S,V=torch.svd(A)。 返回对形如 n×m的实矩阵 A 进行奇异值分解的结果，使得 A=USV’∗。 U 形状为 n×n，S 形状为 n×m ，V 形状为 m×m</p><p>some 代表了需要计算的奇异值数目。如果 some=True, it computes some and some=False computes all.</p><p>Irrespective of the original strides, the returned matrix U will be transposed, i.e. with strides (1, n) instead of (n, 1).</p><p>参数：</p><ul><li>input (Tensor) – 输入的2维张量</li><li>some (bool, optional) – 布尔值，控制需计算的奇异值数目</li><li>out (tuple, optional) – 结果tuple</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.Tensor([[8.79,  6.11, -9.15,  9.57, -3.49,  9.84],...                   [9.93,  6.91, -7.93,  1.64,  4.02,  0.15],...                   [9.83,  5.04,  4.86,  8.83,  9.80, -8.99],...                   [5.45, -0.27,  4.85,  0.74, 10.00, -6.02],...                   [3.16,  7.98,  3.01,  5.80,  4.27, -5.31]]).t()&gt;&gt;&gt; a8.7900   9.9300   9.8300   5.4500   3.16006.1100   6.9100   5.0400  -0.2700   7.9800-9.1500  -7.9300   4.8600   4.8500   3.01009.5700   1.6400   8.8300   0.7400   5.8000-3.4900   4.0200   9.8000  10.0000   4.27009.8400   0.1500  -8.9900  -6.0200  -5.3100[torch.FloatTensor of size 6x5]&gt;&gt;&gt; u, s, v = torch.svd(a)&gt;&gt;&gt; u-0.5911  0.2632  0.3554  0.3143  0.2299-0.3976  0.2438 -0.2224 -0.7535 -0.3636-0.0335 -0.6003 -0.4508  0.2334 -0.3055-0.4297  0.2362 -0.6859  0.3319  0.1649-0.4697 -0.3509  0.3874  0.1587 -0.51830.2934  0.5763 -0.0209  0.3791 -0.6526[torch.FloatTensor of size 6x5]&gt;&gt;&gt; s27.468722.64328.55845.98572.0149[torch.FloatTensor of size 5]&gt;&gt;&gt; v-0.2514  0.8148 -0.2606  0.3967 -0.2180-0.3968  0.3587  0.7008 -0.4507  0.1402-0.6922 -0.2489 -0.2208  0.2513  0.5891-0.3662 -0.3686  0.3859  0.4342 -0.6265-0.4076 -0.0980 -0.4932 -0.6227 -0.4396[torch.FloatTensor of size 5x5]&gt;&gt;&gt; torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))8.934150226306685e-06</code></pre><blockquote><p>torch.symeig</p></blockquote><p>torch.symeig(input, eigenvectors=False, upper=True, out=None) -&gt; (Tensor, Tensor)<br>e,V=torch.symeig(input) 返回实对称矩阵input的特征值和特征向量。</p><p>input 和 V 为 m×m 矩阵，e 是一个m 维向量。 此函数计算intput的所有特征值(和特征向量)，使得 input=Vdiag(e)V′<br>布尔值参数eigenvectors 规定是否只计算特征向量。如果为False，则只计算特征值；若设为True，则两者都会计算。 因为输入矩阵 input 是对称的，所以默认只需要上三角矩阵。如果参数upper为 False，下三角矩阵部分也被利用。</p><font color="#ff0000" face="黑体">注意： 返回矩阵总是被转置，无论输入矩阵的原始布局如何，总会被转置；即，总是有 stride (1, m) 而不是 (m, 1).<br></font><p>参数：</p><ul><li>input (Tensor) – 输入对称矩阵</li><li>eigenvectors (boolean, optional) – 布尔值（可选），控制是否计算特征向量</li><li>upper (boolean, optional) – 布尔值（可选），控制是否考虑上三角或下三角区域</li><li>out (tuple, optional) – 输出元组(Tensor, Tensor)</li></ul><p>例子：</p><pre><code>&gt;&gt;&gt; a = torch.Tensor([[ 1.96,  0.00,  0.00,  0.00,  0.00],...                   [-6.49,  3.80,  0.00,  0.00,  0.00],...                   [-0.47, -6.39,  4.17,  0.00,  0.00],...                   [-7.20,  1.50, -1.51,  5.70,  0.00],...                   [-0.65, -6.34,  2.67,  1.80, -7.10]]).t()&gt;&gt;&gt; e, v = torch.symeig(a, eigenvectors=True)&gt;&gt;&gt; e-11.0656-6.22870.86408.865516.0948[torch.FloatTensor of size 5]&gt;&gt;&gt; v-0.2981 -0.6075  0.4026 -0.3745  0.4896-0.5078 -0.2880 -0.4066 -0.3572 -0.6053-0.0816 -0.3843 -0.6600  0.5008  0.3991-0.0036 -0.4467  0.4553  0.6204 -0.4564-0.8041  0.4480  0.1725  0.3108  0.1622[torch.FloatTensor of size 5x5]</code></pre><p>torch.trtrs<br> torch.trtrs()</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h3&gt;&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;font color=&quot;#00dddd&quot; size=&quot;4&quot;&gt;张量&lt;/font&gt;&lt;br&gt;  &lt;/li&gt;
      
    
    </summary>
    
      <category term="Pytorch框架" scheme="http://yoursite.com/categories/Pytorch%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
</feed>
